{"pages":[{"url":"http://www.instantinate.com/visual_analysis/sat_scores_visual_analysis.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. Conduct visual analysis and complete the prompts below. This project was completed for the General Assembly Data Science Immersive course. The purpose was to build and refine our foundation of python and visually show interesting findings from the data set. Side Notes: The data was sparse, it only showed data for the 50 states, distric of columbia. Methodology: Accomplish the python prompts with as few lines of code as possible. Check the data for any cleanliness issues. Visualize the different distributions. Visualize any relationships in the data and identify outliers. Plot the state data as a USA heatmap. PROMPT 1: Open the sat_scores.csv file. Investigate the data, and answer the questions below. import pandas as pd #Data was imported using the pandas library for quick analysis. data = pd . read_csv ( '../data/sat_scores.csv' ) df = pd . DataFrame ( data ) df . head () State Rate Verbal Math 0 CT 82 509 510 1 NJ 81 499 513 2 MA 79 511 515 3 NY 77 495 505 4 NH 72 520 516 #After seeing state data there should be 50 rows. Check the shape to see if there is 50 rows. df . shape (52, 4) #Ok.... there is 52... cool. We'll come back to that. PROMPT 2. What does the data describe? #The average SAT section score for each state. And the participation rate of that state in the SAT progam. PROMPT 3. Does the data look complete? Are there any obvious issues with the observations? #I printed the value counts and sorted them multiple times to check for data cleanliness. #NOTE: I am only showing the top 5 of each here because I completed the analysis. print \"--------------\" print \"STATE VALUE COUNTS\" print df [ 'State' ] . value_counts ()[: 5 ] print \"--------------\" print \"RATE VALUE COUNTS\" print df [ 'Rate' ] . value_counts ()[: 5 ] print \"--------------\" print \"VERBAL VALUE COUNTS\" print df [ 'Verbal' ] . value_counts ()[: 5 ] print \"--------------\" print \"MATH VALUE COUNTS\" print df [ 'Math' ] . value_counts ()[: 5 ] -------------- STATE VALUE COUNTS SD 1 OR 1 CA 1 VA 1 NV 1 Name: State, dtype: int64 -------------- RATE VALUE COUNTS 9 3 4 3 8 3 5 2 65 2 Name: Rate, dtype: int64 -------------- VERBAL VALUE COUNTS 577 3 562 3 511 2 501 2 527 2 Name: Verbal, dtype: int64 -------------- MATH VALUE COUNTS 499 6 510 3 515 3 542 3 550 2 Name: Math, dtype: int64 # Data issues to note from earier. There since 50 rows were expected and 52 were found we looked to see what they were. # 1: Is the district of columbia and should be included. # 2: Is called \"All\" which needs further investigation below. #Check what the values of the All row is. df . iloc [ 51 ] State All Rate 45 Verbal 506 Math 514 Name: 51, dtype: object #This might be the sum total average for all the other rows in the dataframe. The below code attempts to confirm that theory. #Look at the average score of the other states not including \"All\" and comparing it to the score above. df [ df [ 'State' ] != 'All' ] . Rate . mean () 37.0 #CONCLUSION: The above number is nowhere near the mean of the rate column. #So this data does not represent the average of the column. #However we will leave it in for now and if it proves to be an outlier we'll consider removing it. PROMPT 4. Create a data dictionary for the dataset. Column Description State The state the data was taken from Rate Participation rate in taking the SAT Verbal Average verbal score for that state Math Average math score for that state PROMPT 5. Load the data into a list of lists import csv #This is easier to accomplish with csv reader rather than Pandas so we'll reimport the data here. with open ( '../data/sat_scores.csv' , 'rU' ) as f : data = [] reader = csv . reader ( f ) for row in reader : data . append ( row ) PROMPT 6. Print the data print data [['State', 'Rate', 'Verbal', 'Math'], ['CT', '82', '509', '510'], ['NJ', '81', '499', '513'], ['MA', '79', '511', '515'], ['NY', '77', '495', '505'], ['NH', '72', '520', '516'], ['RI', '71', '501', '499'], ['PA', '71', '500', '499'], ['VT', '69', '511', '506'], ['ME', '69', '506', '500'], ['VA', '68', '510', '501'], ['DE', '67', '501', '499'], ['MD', '65', '508', '510'], ['NC', '65', '493', '499'], ['GA', '63', '491', '489'], ['IN', '60', '499', '501'], ['SC', '57', '486', '488'], ['DC', '56', '482', '474'], ['OR', '55', '526', '526'], ['FL', '54', '498', '499'], ['WA', '53', '527', '527'], ['TX', '53', '493', '499'], ['HI', '52', '485', '515'], ['AK', '51', '514', '510'], ['CA', '51', '498', '517'], ['AZ', '34', '523', '525'], ['NV', '33', '509', '515'], ['CO', '31', '539', '542'], ['OH', '26', '534', '439'], ['MT', '23', '539', '539'], ['WV', '18', '527', '512'], ['ID', '17', '543', '542'], ['TN', '13', '562', '553'], ['NM', '13', '551', '542'], ['IL', '12', '576', '589'], ['KY', '12', '550', '550'], ['WY', '11', '547', '545'], ['MI', '11', '561', '572'], ['MN', '9', '580', '589'], ['KS', '9', '577', '580'], ['AL', '9', '559', '554'], ['NE', '8', '562', '568'], ['OK', '8', '567', '561'], ['MO', '8', '577', '577'], ['LA', '7', '564', '562'], ['WI', '6', '584', '596'], ['AR', '6', '562', '550'], ['UT', '5', '575', '570'], ['IA', '5', '593', '603'], ['SD', '4', '577', '582'], ['ND', '4', '592', '599'], ['MS', '4', '566', '551'], ['All', '45', '506', '514']] PROMPT 7. Extract a list of the labels from the data, and remove them from the data. #Setting the first row as a columns variable. columns = data [ 0 ][:] #Removing the first row(a.k.a the first list object) from the original data list. data . pop ( 0 ) ['State', 'Rate', 'Verbal', 'Math'] PROMPT 8. Create a list of State names extracted from the data. (Hint: use the list of labels to index on the State column) #Use list comprehension for extracting the first element of each list into a new list. states_list = [ row [ 0 ] for row in data ] print states_list ['CT', 'NJ', 'MA', 'NY', 'NH', 'RI', 'PA', 'VT', 'ME', 'VA', 'DE', 'MD', 'NC', 'GA', 'IN', 'SC', 'DC', 'OR', 'FL', 'WA', 'TX', 'HI', 'AK', 'CA', 'AZ', 'NV', 'CO', 'OH', 'MT', 'WV', 'ID', 'TN', 'NM', 'IL', 'KY', 'WY', 'MI', 'MN', 'KS', 'AL', 'NE', 'OK', 'MO', 'LA', 'WI', 'AR', 'UT', 'IA', 'SD', 'ND', 'MS', 'All'] PROMPT 9. Print the types of each column df . dtypes State object Rate int64 Verbal int64 Math int64 dtype: object PROMPT 10. Do any types need to be reassigned? If so, go ahead and do it. #None that I can see. I should be able to work wiht floats just fine for what I need. PROMPT 11. Create a dictionary for each column mapping the State to its respective value for that column. #Using data coprehension to set the first element of each list item as a #key and then use the rest of the list as the value. sat_dict = { d [ 0 ]: d [ 1 :] for d in data } print sat_dict {'WA': ['53', '527', '527'], 'DE': ['67', '501', '499'], 'DC': ['56', '482', '474'], 'WI': ['6', '584', '596'], 'WV': ['18', '527', '512'], 'HI': ['52', '485', '515'], 'FL': ['54', '498', '499'], 'WY': ['11', '547', '545'], 'NH': ['72', '520', '516'], 'NJ': ['81', '499', '513'], 'NM': ['13', '551', '542'], 'TX': ['53', '493', '499'], 'LA': ['7', '564', '562'], 'NC': ['65', '493', '499'], 'ND': ['4', '592', '599'], 'NE': ['8', '562', '568'], 'TN': ['13', '562', '553'], 'NY': ['77', '495', '505'], 'PA': ['71', '500', '499'], 'RI': ['71', '501', '499'], 'NV': ['33', '509', '515'], 'VA': ['68', '510', '501'], 'CO': ['31', '539', '542'], 'AK': ['51', '514', '510'], 'AL': ['9', '559', '554'], 'AR': ['6', '562', '550'], 'VT': ['69', '511', '506'], 'IL': ['12', '576', '589'], 'GA': ['63', '491', '489'], 'IN': ['60', '499', '501'], 'IA': ['5', '593', '603'], 'OK': ['8', '567', '561'], 'AZ': ['34', '523', '525'], 'CA': ['51', '498', '517'], 'ID': ['17', '543', '542'], 'CT': ['82', '509', '510'], 'ME': ['69', '506', '500'], 'MD': ['65', '508', '510'], 'All': ['45', '506', '514'], 'MA': ['79', '511', '515'], 'OH': ['26', '534', '439'], 'UT': ['5', '575', '570'], 'MO': ['8', '577', '577'], 'MN': ['9', '580', '589'], 'MI': ['11', '561', '572'], 'KS': ['9', '577', '580'], 'MT': ['23', '539', '539'], 'MS': ['4', '566', '551'], 'SC': ['57', '486', '488'], 'KY': ['12', '550', '550'], 'OR': ['55', '526', '526'], 'SD': ['4', '577', '582']} PROMPT 12. Create a dictionary with the values for each of the numeric columns #To accomplish this prompt we need a new list of columns that does not have the state column. numeric_columns = columns [ 0 :][ 1 : 4 ] numeric_columns ['Rate', 'Verbal', 'Math'] #Dictionary comprehension with enumerate and an embedded list comprehension. #NOTE: The start argument for the enumerate function is ignoring the state data in each list #and returning only the numeric values to be mapped to our new numeric columns variable. numeric_data = { column_name : [ row [ i ] for row in data ] for i , column_name in enumerate ( numeric_columns , start = 1 )} print numeric_data {'Rate': ['82', '81', '79', '77', '72', '71', '71', '69', '69', '68', '67', '65', '65', '63', '60', '57', '56', '55', '54', '53', '53', '52', '51', '51', '34', '33', '31', '26', '23', '18', '17', '13', '13', '12', '12', '11', '11', '9', '9', '9', '8', '8', '8', '7', '6', '6', '5', '5', '4', '4', '4', '45'], 'Math': ['510', '513', '515', '505', '516', '499', '499', '506', '500', '501', '499', '510', '499', '489', '501', '488', '474', '526', '499', '527', '499', '515', '510', '517', '525', '515', '542', '439', '539', '512', '542', '553', '542', '589', '550', '545', '572', '589', '580', '554', '568', '561', '577', '562', '596', '550', '570', '603', '582', '599', '551', '514'], 'Verbal': ['509', '499', '511', '495', '520', '501', '500', '511', '506', '510', '501', '508', '493', '491', '499', '486', '482', '526', '498', '527', '493', '485', '514', '498', '523', '509', '539', '534', '539', '527', '543', '562', '551', '576', '550', '547', '561', '580', '577', '559', '562', '567', '577', '564', '584', '562', '575', '593', '577', '592', '566', '506']} PROMPT 13. Print the min and max of each column print \"RATE MINIMUM VALUE = \" , df [ 'Rate' ] . min () print \"RATE MAXIMUM VALUE = \" , df [ 'Rate' ] . max () print \"VERBAL MINIMUM VALUE = \" , df [ 'Verbal' ] . min () print \"VERBAL MAXIMUM VALUE = \" , df [ 'Verbal' ] . max () print \"MATH MINIMUM VALUE = \" , df [ 'Math' ] . min () print \"MATH MAXIMUM VALUE = \" , df [ 'Math' ] . max () RATE MINIMUM VALUE = 4 RATE MAXIMUM VALUE = 82 VERBAL MINIMUM VALUE = 482 VERBAL MAXIMUM VALUE = 593 MATH MINIMUM VALUE = 439 MATH MAXIMUM VALUE = 603 PROMPT 14. Write a function using only list comprehensions, no loops, to compute Standard Deviation. Print the Standard Deviation of each numeric column. #This is a simple formula to accomplish the prompt. There is a better way to do it in numpy so please do not #use the example below as the best way. import math import numpy as np def std ( col ): std = math . sqrt ( sum (( df [ col ] - np . mean ( df [ col ])) ** 2 ) / ( len ( df ) - 1 )) return std print ( 'Standard Deviation for Rate: ' + str ( std ( 'Rate' ))) print ( 'Standard Deviation for Verbal: ' + str ( std ( 'Verbal' ))) print ( 'Standard Deviation for Math: ' + str ( std ( 'Math' ))) Standard Deviation for Rate: 27.3017880729 Standard Deviation for Verbal: 33.2362254438 Standard Deviation for Math: 36.0149750989 PROMPT 15. Using MatPlotLib and PyPlot, plot the distribution of the Rate using histograms. #Using seaborn, matplot lib, and plotly to accomplish the below tasks. import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline #Rugplots are in general more informative than histograms so I default to using them. plt . hist ( df . Rate , alpha =. 3 ) sns . rugplot ( df . Rate ); #average rate by state: df [ 'Rate' ] . mean () 37.15384615384615 PROMPT 16. Plot the Math distribution plt . hist ( df . Math , alpha =. 3 ) sns . rugplot ( df . Math ); #Average math score by state. df [ 'Math' ] . mean () 531.5 PROMPT 17. Plot the Verbal distribution plt . hist ( df . Verbal , alpha =. 3 ) sns . rugplot ( df . Verbal ); #Average Verbal score by state. df [ 'Verbal' ] . mean () 532.0192307692307 PROMPT 18. What is the typical assumption for data distribution? #One typical assumption is that the data follows some kind of normal distribution. #NORMAL CURVE: In this case the data does not seem to follow a true normal distribution. #LOW SPREAD: Not a large spread with outliers of data. #CENTERED: Data does not seem to have a good set of large points around the mean except in the case of the math column. PROMPT 19. Does that distribution hold true for our data? #Perhaps for the math data but not the other two. #However we can still plot some interesting relationships below. PROMPT 20. Plot some scatterplots. BONUS : Use a PyPlot figure to present multiple plots at once. #VERBAL vs. MATH data points. This is not colored by state because there would be too many colors to track #and draw any meaningful conclusions. sns . lmplot ( 'Verbal' , 'Math' , data = df , fit_reg = False , aspect = 2 , size = 6 ) <seaborn.axisgrid.FacetGrid at 0x11d763350> #All in all, the above plot is showing a pretty good relationship between math and verbal scores however #There is one outlier to note that we will investigate further. #Same scatter plot but with a hue for Rate. The reason being you can kind of see relationships between #where the colors cluster but at the same time it also demonstrates why you don't color by large lists of points. sns . lmplot ( 'Verbal' , 'Math' , data = df , hue = 'Rate' , fit_reg = False , aspect = 2 , size = 6 ) <seaborn.axisgrid.FacetGrid at 0x11d13a150> PROMPT 21. Are there any interesting relationships to note? #Calling a correlation plotting function that allows me to see relationships with more information than the standard #pairplot. def corr_pairs ( df_input , coef_percentile ): c = df_input . corr () s = c . unstack () so = s . sort_values ( kind = \"quicksort\" ) df_output = pd . DataFrame ( so . abs (), columns = [ 'coef' ]) df_output = df_output . reset_index () df_output . drop_duplicates ( 'coef' , inplace = True ) df_output . dropna ( inplace = True ) df_output = df_output [( df_output [ 'coef' ] < 1 ) & ( df_output . coef > np . percentile ( df_output [ 'coef' ], coef_percentile ))] #& (df_output.mse < np.percentile(df_output['mse'],mse_percentile))] #Plot the best pairs. for i in range ( len ( df_output . iloc [:, 0 : 2 ])): colors = [ 'r' , 'b' ] plt . scatter ( df_output . iloc [ i , 0 ], df_output . iloc [ i , 1 ], data = df_input , c = colors ) plt . xlabel ( df_output . iloc [ i , 0 ]) plt . ylabel ( df_output . iloc [ i , 1 ]) plt . legend () plt . show () return df_output #First looking at numbers to see if anything is interesting here. df . corr () Rate Verbal Math Rate 1.000000 -0.886432 -0.773746 Verbal -0.886432 1.000000 0.899871 Math -0.773746 0.899871 1.000000 corr_pairs ( df , 0 ) level_0 level_1 coef 0 Rate Verbal 0.886432 4 Verbal Math 0.899871 ### Conclusion: #There is a strong correlation between doing well on Verbal and well on Math unsuprisingly. #But another interesting takeaway is having a good participation rate is MUCH more strongly correlated #to a good verbal score rather than a good Math score. Which suggests that the more people participated #in a states SAT test the higher the Verbal score but the not as high of a math score. #Another interesting note is that Ohio is the outlier in the Math to Verbal correlation where #a Math score of only 439 was achieved but a Verbal score of 534 was achieved. PROMPT 22. Create box plots for each variable. #Done using seaborn plt . figure ( figsize = ( 10 , 10 )) sns . boxplot ( data = df [[ 'Verbal' , 'Math' ]]) <matplotlib.axes._subplots.AxesSubplot at 0x11c76fcd0> #Rate is on a different scale so needed to be plotted separately. plt . figure ( figsize = ( 5 , 10 )) sns . boxplot ( data = df [ 'Rate' ]) <matplotlib.axes._subplots.AxesSubplot at 0x11cf570d0> BONUS: Using Tableau, create a heat map for each variable using a map of the US. #So I didn't use tableau as I wanted to try my hand at using plotly for the first time. I don't know much #about what is being done below but by using some of their out of the box features you get the benefit #of having some interactive charts inside your jupyter notebook. #Click on the jupyter notebook. import plotly.plotly as py from plotly.graph_objs import * import plotly.tools as tls scl = [[ 0.0 , 'rgb(242,240,247)' ],[ 0.2 , 'rgb(218,218,235)' ],[ 0.4 , 'rgb(188,189,220)' ], \\ [ 0.6 , 'rgb(158,154,200)' ],[ 0.8 , 'rgb(117,107,177)' ],[ 1.0 , 'rgb(84,39,143)' ]] df [ 'text' ] = df [ 'State' ] data = [ dict ( type = 'choropleth' , colorscale = scl , autocolorscale = False , locations = df [ 'State' ], z = df [ 'Rate' ] . astype ( float ), locationmode = 'USA-states' , text = df [ 'text' ], hoverinfo = 'location+z' , marker = dict ( line = dict ( color = 'rgb(255,255,255)' , width = 2 ) ), colorbar = dict ( title = \"Rates\" ) )] layout = dict ( title = 'SAT Participation Rates<br>(Hover for breakdown)' , geo = dict ( scope = 'usa' , projection = dict ( type = 'albers usa' ), showlakes = True , lakecolor = 'rgb(255, 255, 255)' ) ) fig = dict ( data = data , layout = layout ) py . iplot ( fig , validate = False , filename = 'd3-electoral-map' ) High five! You successfuly sent some data to your account on plotly. View your plot in your browser at https://plot.ly/~natejhall89/0 or inside your plot.ly account where it is named 'd3-electoral-map' scl = [[ 0.0 , 'rgb(242,240,247)' ],[ 0.2 , 'rgb(218,218,235)' ],[ 0.4 , 'rgb(188,189,220)' ], \\ [ 0.6 , 'rgb(158,154,200)' ],[ 0.8 , 'rgb(117,107,177)' ],[ 1.0 , 'rgb(84,39,143)' ]] df [ 'text' ] = df [ 'State' ] data = [ dict ( type = 'choropleth' , colorscale = scl , autocolorscale = False , locations = df [ 'State' ], z = df [ 'Math' ] . astype ( float ), locationmode = 'USA-states' , text = df [ 'text' ], hoverinfo = 'location+z' , marker = dict ( line = dict ( color = 'rgb(255,255,255)' , width = 2 ) ), colorbar = dict ( title = \"Rates\" ) )] layout = dict ( title = 'SAT Participation Rates<br>(Hover for breakdown)' , geo = dict ( scope = 'usa' , projection = dict ( type = 'albers usa' ), showlakes = True , lakecolor = 'rgb(255, 255, 255)' ) ) fig = dict ( data = data , layout = layout ) py . iplot ( fig , validate = False , filename = 'd3-electoral-map' ) High five! You successfuly sent some data to your account on plotly. View your plot in your browser at https://plot.ly/~natejhall89/0 or inside your plot.ly account where it is named 'd3-electoral-map' scl = [[ 0.0 , 'rgb(242,240,247)' ],[ 0.2 , 'rgb(218,218,235)' ],[ 0.4 , 'rgb(188,189,220)' ], \\ [ 0.6 , 'rgb(158,154,200)' ],[ 0.8 , 'rgb(117,107,177)' ],[ 1.0 , 'rgb(84,39,143)' ]] df [ 'text' ] = df [ 'State' ] data = [ dict ( type = 'choropleth' , colorscale = scl , autocolorscale = False , locations = df [ 'State' ], z = df [ 'Verbal' ] . astype ( float ), locationmode = 'USA-states' , text = df [ 'text' ], hoverinfo = 'location+z' , marker = dict ( line = dict ( color = 'rgb(255,255,255)' , width = 2 ) ), colorbar = dict ( title = \"Rates\" ) )] layout = dict ( title = 'SAT Participation Rates<br>(Hover for breakdown)' , geo = dict ( scope = 'usa' , projection = dict ( type = 'albers usa' ), showlakes = True , lakecolor = 'rgb(255, 255, 255)' ) ) fig = dict ( data = data , layout = layout ) py . iplot ( fig , validate = False , filename = 'd3-electoral-map' ) High five! You successfuly sent some data to your account on plotly. View your plot in your browser at https://plot.ly/~natejhall89/0 or inside your plot.ly account where it is named 'd3-electoral-map'","tags":"visual_analysis","loc":"http://www.instantinate.com/visual_analysis/sat_scores_visual_analysis.html","title":"SAT Scores Visual Analysis"},{"url":"http://www.instantinate.com/how_to/edit_column_names.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. In this example we'll be working with the billboard hot 100 dataset. The overall structure of this dataset could be setup better for what we need to do with some interesting correlations so we'll be using it for a lot of dataset manipulation examples. One method for organizing a data table is for each row to represent the \"lowest level\" of data available. Here we have each row representing a song, but if you notice the column names there is one for each week. And if you look at the shape of our data we have 83 columns total. Can you guess what the lowest level of data really is for this dataset then? import pandas as pd data = pd . read_csv ( '../data/billboard.csv' ) df = pd . DataFrame ( data ) df . head ( 1 ) year artist.inverted track time genre date.entered date.peaked x1st.week x2nd.week x3rd.week ... x67th.week x68th.week x69th.week x70th.week x71st.week x72nd.week x73rd.week x74th.week x75th.week x76th.week 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Lowest level of data as rows. If you guessed the lowest level would be each a pair combo of each week for each song then you'd be correct. Goal: To get each row to represent the week performance of a song. 1. Get the week performance column names to be just numbers This will allow us to convert them into integers and sort them later. Now its as easy as looking for the things that are in those week columns that are not numbers and removing them. We will take all our edits and apply them to the \"df.columns\" variable. This will overwrite the column names in our dataframe. df . columns = Since df.columns is a list of column names we have to pass it a list back so we start our function with brackets. df . columns = [ ] Remove values with .replace Next we get to say what we want to happen to the list we are going to pass back to df.column to replace its current list of column names. One way to remove values and get down to numbers is to replace the values with just blanks and thereby remove the values. So we use the \"col.replace\" method to define what values in the column header list we're going to replace. df . columns = [ col . replace ( ) Now we get to tell df.columns what we want its new names to be. So what do you think is the first thing we can remove? It looks like \".week\" is on every column name so lets take that out and see what we have left. NOTE: We are using a list comprehension so the values at the beginning of the list are what we will get back. In this case it will be all the columns in df.columns but with the \".week\" removed(replaced with blanks). df . columns = [ col . replace ( '.week' , '' ) for col in df . columns ] Cool! That's gone now what? What's the next thing to be removed? Looks, like they all have \"x\" in front of them (for whatever reason...). Lets replace that with blanks. df . columns = [ col . replace ( '.week' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'x' , '' ) for col in df . columns ] Now we're getting somewhere. We are just left with the values that come after the 1, 2, 3, and 4. This part might be tedious but since from 4 onward every value ends with a \"th\" it's not too bad. So we run the list comprehension another four times to replace these values with blanks. df . columns = [ col . replace ( '.week' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'x' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'st' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'nd' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'rd' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'th' , '' ) for col in df . columns ] Check your work to make sure it worked as expected. You only need to load one row since we are looking at columns. df . head ( 1 ) year arti.inverted track time genre date.entered date.peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Success!!!! Or... so it seems... Can you spot the one thing we inadvertedly did by focusing on the number values and running our replace method across the entire column header list? It looks like the \"artist.inverted\" column name had the \"st\" removed when we applied our replace function to remove the value after the 1. Luckily this is easily fixable and introduces us to another useful column editing method called \".rename\". 2. Fix the \"artist.inverted\" column name This is powerful function that we will use again in other contexts. But for columns it is very useful. Remember that dictionaries contain a key and a value. For this application we are telling pandas to go through all our columns and find the ones that match the \"key\" in our dictionary and when it does to replace it with the \"value\" in our dictionary. So this is less flexible than other dictionaries but very useful for our applications. The .rename method can be applied to both rows and columns so it actually goes with the entire datafarme instead of just the columns like the .replace function we used earlier. It also is different because it doesn't have to be used with a list comprehension it takes arguments that can be found in the pandas documentation here. So we start by applying the .rename method to the dataframe and add ( ) at the end because it takes arguments. df . rename () What do you think we should pass to it as an argument? If you look at the pandas documentation you'll see all kinds of things you can make the .rename() method do. For our application we're going to pass it a dictionary like list of column names. So we use the columns= argument. df . rename ( columns = ) Remember we're passing it a dictionary like list. So we don't use brackets [ ] like normal lists. It has to be the dictionary brackets { }. df . rename ( columns = { }) Now we tell the rename method which columns to look for by giving it \"keys\". df . rename ( columns = { 'arti.inverted' : }) Then all we have to do is say what we want to change the column name to. To make the nameing scheme simpler we'll make it be just \"artist\". #The [:1] is slicing this data to only load one row since we're just interested in columns right now. #Notice it gives the same result as the df.head(1) we used earlier. df . rename ( columns = { 'arti.inverted' : 'artist' })[: 1 ] year artist track time genre date.entered date.peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Check your work to make sure it applied like you expect. df . head ( 1 ) year arti.inverted track time genre date.entered date.peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Huh? What happened? Why isn't it changed? Like all debugging, lets backtrack. What happens when you run the code without the df.head() after it. df . rename ( columns = { 'arti.inverted' : 'artist' })[: 1 ] year artist track time genre date.entered date.peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Ok, it works then, what's going on? Pandas is a very smart and powerful tool. Notice when we first used the .replace function we were applying it to the df.columns variable. This was physically changing the list that was already at that variable and writing over it with a new list that we gave it. Do you see any of that type of thing happening with the .rename() method? Are we writing over something as explicitly as redeclaring a variable? Because pandas does not see you declaring something so explicitly it assumes(BY DEFAULT) that you are not intending to apply the change to the original dataframe and are just interested in running the function in a sort of alternate dimension dataframe. Wait... I thought you said it was smart? That doesn't sound very smart... What would happen if pandas over wrote everything you did and you wanted to do something either as an experiment or as a step in a process? You would have to COPY the entire dataset A LOT. And some of these things can get really big. So instead it creates this one alternate universe result of the method and you get to decide if you want to use it to overwrite the current dataframe or use it as an experiment or step in process with a no harm no foul safety net. Neat huh? Use the inplace argument To get every method that can be applied to a dataframe object to write over the existing dataframe you have to say that you want inplace=True (it defaults to False). This will write over your original dataframe in the place where you apply a method and argument to it. 2a. Fix the arti.inverted column with inplace=True df . rename ( columns = { 'arti.inverted' : 'artist' }, inplace = True ) Now, check your work again. df . head ( 1 ) year artist track time genre date.entered date.peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Yay! Success!!! BONUS: You can rename more than one column at a time. Note that we removed the \".\" from \"artist.inverted\" when we renamed it. However, \"date.entered\" and \"date.peaked\" still have them. From what we know about dictionaries that have have many keys and values so why not simply add a comma after the first key value pair and then type in another one? df . rename ( columns = { 'date.entered' : 'entered' , 'date.peaked' : 'peaked' }, inplace = True ) Check your work and see what happens. df . head ( 1 ) year artist track time genre entered peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Success!! Way to go! To see the next step click here to go to unpivoting columns.","tags":"how_to","loc":"http://www.instantinate.com/how_to/edit_column_names.html","title":"How To: Edit Column Names"},{"url":"http://www.instantinate.com/wrangling_data/editing_column_names.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. In this example we'll be working with the billboard hot 100 dataset. The overall structure of this dataset could be setup better for what we need to do with some interesting correlations so we'll be using it for a lot of dataset manipulation examples. One method for organizing a data table is for each row to represent the \"lowest level\" of data available. Here we have each row representing a song, but if you notice the column names there is one for each week. And if you look at the shape of our data we have 83 columns total. Can you guess what the lowest level of data really is for this dataset then? import pandas as pd data = pd . read_csv ( '../data/billboard.csv' ) df = pd . DataFrame ( data ) df . head ( 1 ) year artist.inverted track time genre date.entered date.peaked x1st.week x2nd.week x3rd.week ... x67th.week x68th.week x69th.week x70th.week x71st.week x72nd.week x73rd.week x74th.week x75th.week x76th.week 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Lowest level of data as rows. If you guessed the lowest level would be each a pair combo of each week for each song then you'd be correct. Goal: To get each row to represent the week performance of a song. 1. Get the week performance column names to be just numbers This will allow us to convert them into integers and sort them later. Now its as easy as looking for the things that are in those week columns that are not numbers and removing them. We will take all our edits and apply them to the \"df.columns\" variable. This will overwrite the column names in our dataframe. df . columns = Since df.columns is a list of column names we have to pass it a list back so we start our function with brackets. df . columns = [ ] Remove values with .replace Next we get to say what we want to happen to the list we are going to pass back to df.column to replace its current list of column names. One way to remove values and get down to numbers is to replace the values with just blanks and thereby remove the values. So we use the \"col.replace\" method to define what values in the column header list we're going to replace. df . columns = [ col . replace ( ) Now we get to tell df.columns what we want its new names to be. So what do you think is the first thing we can remove? It looks like \".week\" is on every column name so lets take that out and see what we have left. NOTE: We are using a list comprehension so the values at the beginning of the list are what we will get back. In this case it will be all the columns in df.columns but with the \".week\" removed(replaced with blanks). df . columns = [ col . replace ( '.week' , '' ) for col in df . columns ] Cool! That's gone now what? What's the next thing to be removed? Looks, like they all have \"x\" in front of them (for whatever reason...). Lets replace that with blanks. df . columns = [ col . replace ( '.week' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'x' , '' ) for col in df . columns ] Now we're getting somewhere. We are just left with the values that come after the 1, 2, 3, and 4. This part might be tedious but since from 4 onward every value ends with a \"th\" it's not too bad. So we run the list comprehension another four times to replace these values with blanks. df . columns = [ col . replace ( '.week' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'x' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'st' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'nd' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'rd' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'th' , '' ) for col in df . columns ] Check your work to make sure it worked as expected. You only need to load one row since we are looking at columns. df . head ( 1 ) year arti.inverted track time genre date.entered date.peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Success!!!! Or... so it seems... Can you spot the one thing we inadvertedly did by focusing on the number values and running our replace method across the entire column header list? It looks like the \"artist.inverted\" column name had the \"st\" removed when we applied our replace function to remove the value after the 1. Luckily this is easily fixable and introduces us to another useful column editing method called \".rename\". 2. Fix the \"artist.inverted\" column name This is powerful function that we will use again in other contexts. But for columns it is very useful. Remember that dictionaries contain a key and a value. For this application we are telling pandas to go through all our columns and find the ones that match the \"key\" in our dictionary and when it does to replace it with the \"value\" in our dictionary. So this is less flexible than other dictionaries but very useful for our applications. The .rename method can be applied to both rows and columns so it actually goes with the entire datafarme instead of just the columns like the .replace function we used earlier. It also is different because it doesn't have to be used with a list comprehension it takes arguments that can be found in the pandas documentation here. So we start by applying the .rename method to the dataframe and add ( ) at the end because it takes arguments. df . rename () What do you think we should pass to it as an argument? If you look at the pandas documentation you'll see all kinds of things you can make the .rename() method do. For our application we're going to pass it a dictionary like list of column names. So we use the columns= argument. df . rename ( columns = ) Remember we're passing it a dictionary like list. So we don't use brackets [ ] like normal lists. It has to be the dictionary brackets { }. df . rename ( columns = { }) Now we tell the rename method which columns to look for by giving it \"keys\". df . rename ( columns = { 'arti.inverted' : }) Then all we have to do is say what we want to change the column name to. To make the nameing scheme simpler we'll make it be just \"artist\". #The [:1] is slicing this data to only load one row since we're just interested in columns right now. #Notice it gives the same result as the df.head(1) we used earlier. df . rename ( columns = { 'arti.inverted' : 'artist' })[: 1 ] year artist track time genre date.entered date.peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Check your work to make sure it applied like you expect. df . head ( 1 ) year arti.inverted track time genre date.entered date.peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Huh? What happened? Why isn't it changed? Like all debugging, lets backtrack. What happens when you run the code without the df.head() after it. df . rename ( columns = { 'arti.inverted' : 'artist' })[: 1 ] year artist track time genre date.entered date.peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Ok, it works then, what's going on? Pandas is a very smart and powerful tool. Notice when we first used the .replace function we were applying it to the df.columns variable. This was physically changing the list that was already at that variable and writing over it with a new list that we gave it. Do you see any of that type of thing happening with the .rename() method? Are we writing over something as explicitly as redeclaring a variable? Because pandas does not see you declaring something so explicitly it assumes(BY DEFAULT) that you are not intending to apply the change to the original dataframe and are just interested in running the function in a sort of alternate dimension dataframe. Wait... I thought you said it was smart? That doesn't sound very smart... What would happen if pandas over wrote everything you did and you wanted to do something either as an experiment or as a step in a process? You would have to COPY the entire dataset A LOT. And some of these things can get really big. So instead it creates this one alternate universe result of the method and you get to decide if you want to use it to overwrite the current dataframe or use it as an experiment or step in process with a no harm no foul safety net. Neat huh? Use the inplace argument To get every method that can be applied to a dataframe object to write over the existing dataframe you have to say that you want inplace=True (it defaults to False). This will write over your original dataframe in the place where you apply a method and argument to it. 2a. Fix the arti.inverted column with inplace=True df . rename ( columns = { 'arti.inverted' : 'artist' }, inplace = True ) Now, check your work again. df . head ( 1 ) year artist track time genre date.entered date.peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Yay! Success!!! BONUS: You can rename more than one column at a time. Note that we removed the \".\" from \"artist.inverted\" when we renamed it. However, \"date.entered\" and \"date.peaked\" still have them. From what we know about dictionaries that have have many keys and values so why not simply add a comma after the first key value pair and then type in another one? df . rename ( columns = { 'date.entered' : 'entered' , 'date.peaked' : 'peaked' }, inplace = True ) Check your work and see what happens. df . head ( 1 ) year artist track time genre entered peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Success!! Way to go! To see the next step click here to go to unpivoting columns.","tags":"wrangling_data","loc":"http://www.instantinate.com/wrangling_data/editing_column_names.html","title":"Editing Column Names"},{"url":"http://www.instantinate.com/how_to/filter_columns.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. This is a slightly more complicated application of filtering out columns than the standard df.drop method you may have seen. But if you stick with each element I promise it will begin to make sense. import pandas as pd data = pd . read_csv ( '../data/billboard.csv' ) df = pd . DataFrame ( data ) df . head ( 1 ) year artist.inverted track time genre date.entered date.peaked x1st.week x2nd.week x3rd.week ... x67th.week x68th.week x69th.week x70th.week x71st.week x72nd.week x73rd.week x74th.week x75th.week x76th.week 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Filter out columns that contain a keyword Remember what method we use to drop columns or rows in a dataset? We will be using the same one again here. So we begin our function by saying... df . drop ( ) Now remember in the documentation what we can pass this method to drop? It says it can take on a \"list-like\" set of items to be dropped. Conceptualize what to filter Conceptually lets lay out what we're trying to accomplish. We want the .drop method to get automatically populated with a list of columns that match some type of filter criteria. To get a list of columns that match a criteria we would use a for loop. However, that is too cumbersome to try to embed in the .drop method which is why list comprehensions are very useful to be placed in arguments. Use list comprehension to return the columns to be dropped Remember that list comprehensions you start by declaring what variable you will be getting and then go on to declare the function setup. For our example we're going to say give me all the columns that are in our dataframe if they have the number \"6\" in them. (No real reason behind 6 this is purely for example purposes). We put our comprehension in brackets since its returning a list and declare the first variable we're getting back \"col\". [ col ] Next we need to give it the for loop (what we want it to look through). In this case we want it to give all the \"col\" values that match our filter so we say \"for col in df.columns\" since we're looking through all the columns. [ col for col in df . columns ] Lastly, we get to delcare our filtering criteria, which we said was columns that have the number \"6\" in them. Note, that all the columns are strings so we need to put the 6 in quotations when declaring what to filter. At any rate, we are looking at all the \"col\" so we want all the ones if \"6\" is in them. [ col for col in df . columns if \"6\" in col ] And that's it we have our filter setup with a easy logic filtering test to apply to all the columns. We can place this straight into the df.drop method and add the other arguments for the column axis and inplace being set to true so we apply this to our dataframe. df . drop ([ col for col in df . columns if \"6\" in col ], axis = 1 , inplace = True ) #Check your work df . head ( 1 ) year artist.inverted track time genre date.entered date.peaked x1st.week x2nd.week x3rd.week ... x55th.week x57th.week x58th.week x59th.week x70th.week x71st.week x72nd.week x73rd.week x74th.week x75th.week 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 66 columns Cool! We see when we scroll over that all the columns from the sixties are removed and the ones like 56. Everything with a 6 is gone you powerful data scientist you. Filter out columns that start with \"__\" Because we did a logical argument with the list comprehension the sky is the limit for what we want it to return. In the below example I am asking it to take it one step further and look at the first letter of all the columns, if the letter is an \"x\" then it should drop that column. df . drop ([ col for col in df . columns if col [: 1 ] == 'x' ], axis = 1 , inplace = True ) #Check your work df . head ( 1 ) year artist.inverted track time genre date.entered date.peaked 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 Bam! You can now conquer any mangy dataset columns you come across.","tags":"how_to","loc":"http://www.instantinate.com/how_to/filter_columns.html","title":"How To: Filter Columns"},{"url":"http://www.instantinate.com/wrangling_data/filter_out_columns.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. This is a slightly more complicated application of filtering out columns than the standard df.drop method you may have seen. But if you stick with each element I promise it will begin to make sense. import pandas as pd data = pd . read_csv ( '../data/billboard.csv' ) df = pd . DataFrame ( data ) df . head ( 1 ) year artist.inverted track time genre date.entered date.peaked x1st.week x2nd.week x3rd.week ... x67th.week x68th.week x69th.week x70th.week x71st.week x72nd.week x73rd.week x74th.week x75th.week x76th.week 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Filter out columns that contain a keyword Remember what method we use to drop columns or rows in a dataset? We will be using the same one again here. So we begin our function by saying... df . drop ( ) Now remember in the documentation what we can pass this method to drop? It says it can take on a \"list-like\" set of items to be dropped. Conceptualize what to filter Conceptually lets lay out what we're trying to accomplish. We want the .drop method to get automatically populated with a list of columns that match some type of filter criteria. To get a list of columns that match a criteria we would use a for loop. However, that is too cumbersome to try to embed in the .drop method which is why list comprehensions are very useful to be placed in arguments. Use list comprehension to return the columns to be dropped Remember that list comprehensions you start by declaring what variable you will be getting and then go on to declare the function setup. For our example we're going to say give me all the columns that are in our dataframe if they have the number \"6\" in them. (No real reason behind 6 this is purely for example purposes). We put our comprehension in brackets since its returning a list and declare the first variable we're getting back \"col\". [ col ] Next we need to give it the for loop (what we want it to look through). In this case we want it to give all the \"col\" values that match our filter so we say \"for col in df.columns\" since we're looking through all the columns. [ col for col in df . columns ] Lastly, we get to delcare our filtering criteria, which we said was columns that have the number \"6\" in them. Note, that all the columns are strings so we need to put the 6 in quotations when declaring what to filter. At any rate, we are looking at all the \"col\" so we want all the ones if \"6\" is in them. [ col for col in df . columns if \"6\" in col ] And that's it we have our filter setup with a easy logic filtering test to apply to all the columns. We can place this straight into the df.drop method and add the other arguments for the column axis and inplace being set to true so we apply this to our dataframe. df . drop ([ col for col in df . columns if \"6\" in col ], axis = 1 , inplace = True ) #Check your work df . head ( 1 ) year artist.inverted track time genre date.entered date.peaked x1st.week x2nd.week x3rd.week ... x55th.week x57th.week x58th.week x59th.week x70th.week x71st.week x72nd.week x73rd.week x74th.week x75th.week 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 66 columns Cool! We see when we scroll over that all the columns from the sixties are removed and the ones like 56. Everything with a 6 is gone you powerful data scientist you. Filter out columns that start with \"__\" Because we did a logical argument with the list comprehension the sky is the limit for what we want it to return. In the below example I am asking it to take it one step further and look at the first letter of all the columns, if the letter is an \"x\" then it should drop that column. df . drop ([ col for col in df . columns if col [: 1 ] == 'x' ], axis = 1 , inplace = True ) #Check your work df . head ( 1 ) year artist.inverted track time genre date.entered date.peaked 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 Bam! You can now conquer any mangy dataset columns you come across.","tags":"wrangling_data","loc":"http://www.instantinate.com/wrangling_data/filter_out_columns.html","title":"Filter Out Columns"},{"url":"http://www.instantinate.com/how_to/remove_columns.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. This is a quick introduction to some basic methods for removing columns from a dataset. We will use the billboard hot 100 dataset as an example. import pandas as pd data = pd . read_csv ( '../data/billboard_weeks_edited.csv' ) df = pd . DataFrame ( data ) df . head ( 1 ) year artist track time genre entered peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Drop a column The simplest way to remove a column is with the \"df.drop\" method from pandas. See more info at the documentation here. We specify what dataframe to apply the .drop method. Then we tell it if we're dropping a column or row with the \"axis\" argument. And finally we use the \"inplace\" argument to either create the results in an alternate universe or apply it back to our current dataframe. #Can you guess why I am dropping this column? Its not just for an example. See if you can figure it out. df . drop ( 'year' , axis = 1 , inplace = True ) df . head ( 1 ) artist track time genre entered peaked 1 2 3 4 ... 67 68 69 70 71 72 73 74 75 76 0 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 33.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 82 columns Drop multiple columns Notice that for the \"labels\" argument in the documentation it says that the elements to drop can be \"list-like\". Which means that if we want to drop multiple columns we can give it a list of column names since that is how all the column headers are stored in pandas anyways. So the function would be... #This time we are doing it for example purposes only so note that inplace is set to False to make this abundantly clear. #It defaults to False automatically but this makes it clear we are not applying it to the original dataframe. df . drop ([ 'entered' , 'peaked' ], axis = 1 , inplace = False )[: 1 ] #note we aren't using df.head to see how it works. Because df.head shows the original dataframe values. #since we are not applying this to the original dataframe we can slice the results to show the first row #only and that acts the same as using df.head(1) artist track time genre 1 2 3 4 5 6 ... 67 68 69 70 71 72 73 74 75 76 0 Destiny's Child Independent Women Part I 3:38 Rock 78 63.0 49.0 33.0 23.0 15.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 80 columns","tags":"how_to","loc":"http://www.instantinate.com/how_to/remove_columns.html","title":"How To: Remove Columns"},{"url":"http://www.instantinate.com/wrangling_data/removing_columns.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. This is a quick introduction to some basic methods for removing columns from a dataset. We will use the billboard hot 100 dataset as an example. import pandas as pd data = pd . read_csv ( '../data/billboard_weeks_edited.csv' ) df = pd . DataFrame ( data ) df . head ( 1 ) year artist track time genre entered peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Drop a column The simplest way to remove a column is with the \"df.drop\" method from pandas. See more info at the documentation here. We specify what dataframe to apply the .drop method. Then we tell it if we're dropping a column or row with the \"axis\" argument. And finally we use the \"inplace\" argument to either create the results in an alternate universe or apply it back to our current dataframe. #Can you guess why I am dropping this column? Its not just for an example. See if you can figure it out. df . drop ( 'year' , axis = 1 , inplace = True ) df . head ( 1 ) artist track time genre entered peaked 1 2 3 4 ... 67 68 69 70 71 72 73 74 75 76 0 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 33.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 82 columns Drop multiple columns Notice that for the \"labels\" argument in the documentation it says that the elements to drop can be \"list-like\". Which means that if we want to drop multiple columns we can give it a list of column names since that is how all the column headers are stored in pandas anyways. So the function would be... #This time we are doing it for example purposes only so note that inplace is set to False to make this abundantly clear. #It defaults to False automatically but this makes it clear we are not applying it to the original dataframe. df . drop ([ 'entered' , 'peaked' ], axis = 1 , inplace = False )[: 1 ] #note we aren't using df.head to see how it works. Because df.head shows the original dataframe values. #since we are not applying this to the original dataframe we can slice the results to show the first row #only and that acts the same as using df.head(1) artist track time genre 1 2 3 4 5 6 ... 67 68 69 70 71 72 73 74 75 76 0 Destiny's Child Independent Women Part I 3:38 Rock 78 63.0 49.0 33.0 23.0 15.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 80 columns","tags":"wrangling_data","loc":"http://www.instantinate.com/wrangling_data/removing_columns.html","title":"Removing Columns"},{"url":"http://www.instantinate.com/how_to/unpivot_columns.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. We'll be using the billboard hot 100 dataset as the example. We wanted to get the dataset to have rows representing the lowest level of data possible. We cleaned up the column names in this article. Now to finish the the final step to accomplish the goal below. Goal: To get each row to represent the week performance of a song. import pandas as pd data = pd . read_csv ( '../data/billboard_weeks_edited.csv' ) df = pd . DataFrame ( data ) df . head ( 1 ) year artist track time genre entered peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns #Check the shape of the dataset we're starting with. df . shape (317, 83) We know that we have 317 rows and 83 columns. If we are \"unpivoting\" this to be fewer columns we can expect the rows to increase substantially. Remember this for when we check the shape again later. Melt its face off! We will use the \".melt\" function to accomplish what we need for this dataset. More information on it can be found in the documentation here. Notes on pd.melt First off we see that it is not applied to a dataframe, so we don't do \"df.melt\" it actually gets passed the entire dataframe object as an argument and is called by starting with \"pd.\" pd . melt ( df ) Create a new dataframe Simply for the ease of still being able to track back to our original csv in case something doesn't work. We will then declare a new dataframe that will take on the results of the pd.melt( ) function. df2 = pd . melt ( df ) id_vars=[ ] We already passed the dataframe. The id_vars argument will be the columns that will be retained in the dataframe so we'll declare that by listing the columns we want to keep. Note that this argument is taking on a list so we can pass the columns to it with all the columns list methods that we know. For this example i'll just list out the columns since there are so few. df2 = pd . melt ( df , id_vars = [ 'year' , 'artist' , 'track' , 'time' , 'genre' , 'entered' , 'peaked' ]) value_vars=[ ] This argument says it is for stating all the columns to \"unpivot\". Phew, we have a lot of weeks... do we have to list them all? That's the beauty of this function. If we simply leave this blank it will default to unpivot all of the columns that are not listed in the \"id_vars=[ ]\" argument. SWEET!!! var_name=[ ] So we have this tricky thing to do with unpivoting... we'll have all the week columns get turned into row cells and they will need a column header. To do this we declare the \"var_name=[ ]\" argument with the column header we want. df2 = pd . melt ( df , id_vars = [ 'year' , 'artist' , 'track' , 'time' , 'genre' , 'entered' , 'peaked' ], var_name = 'week' ) value_name[ ] Now what about all the values that are listed in each week column right now? Where are those supposed to go? Glad you asked that. These are the values that we are \"unpivoting\" but they to will need a column header since we just took the week columns away. We know that those values represent the weekly ranking of a song during that week so we'll \"unpivot\" those values to a new column called \"rank\" df2 = pd . melt ( df , id_vars = [ 'year' , 'artist' , 'track' , 'time' , 'genre' , 'entered' , 'peaked' ], var_name = 'week' , value_name = 'rank' ) Check your work. df2 . head () year artist track time genre entered peaked week rank 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 1 78.0 1 2000 Santana Maria, Maria 4:18 Rock 2000-02-12 2000-04-08 1 15.0 2 2000 Savage Garden I Knew I Loved You 4:07 Rock 1999-10-23 2000-01-29 1 71.0 3 2000 Madonna Music 3:45 Rock 2000-08-12 2000-09-16 1 41.0 4 2000 Aguilera, Christina Come On Over Baby (All I Want Is You) 3:38 Rock 2000-08-05 2000-10-14 1 57.0 Cool! It worked! We now have the rows representing the lowest level of data in our dataset. What did this do to the shape of our table? Remember what it was at the beginning? Lets check it again to see what happened. df2 . shape (24092, 9) Woah. That is MUCH longer. Nothing to worry about though. You'll find that pandas can handle this length of a data table easily. Most importantly we have our dataset setup in a way where we can work with it much better.","tags":"how_to","loc":"http://www.instantinate.com/how_to/unpivot_columns.html","title":"How To: Unpivot Columns"},{"url":"http://www.instantinate.com/wrangling_data/unpivoting_columns.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. We'll be using the billboard hot 100 dataset as the example. We wanted to get the dataset to have rows representing the lowest level of data possible. We cleaned up the column names in this article. Now to finish the the final step to accomplish the goal below. Goal: To get each row to represent the week performance of a song. import pandas as pd data = pd . read_csv ( '../data/billboard_weeks_edited.csv' ) df = pd . DataFrame ( data ) df . head ( 1 ) year artist track time genre entered peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns #Check the shape of the dataset we're starting with. df . shape (317, 83) We know that we have 317 rows and 83 columns. If we are \"unpivoting\" this to be fewer columns we can expect the rows to increase substantially. Remember this for when we check the shape again later. Melt its face off! We will use the \".melt\" function to accomplish what we need for this dataset. More information on it can be found in the documentation here. Notes on pd.melt First off we see that it is not applied to a dataframe, so we don't do \"df.melt\" it actually gets passed the entire dataframe object as an argument and is called by starting with \"pd.\" pd . melt ( df ) Create a new dataframe Simply for the ease of still being able to track back to our original csv in case something doesn't work. We will then declare a new dataframe that will take on the results of the pd.melt( ) function. df2 = pd . melt ( df ) id_vars=[ ] We already passed the dataframe. The id_vars argument will be the columns that will be retained in the dataframe so we'll declare that by listing the columns we want to keep. Note that this argument is taking on a list so we can pass the columns to it with all the columns list methods that we know. For this example i'll just list out the columns since there are so few. df2 = pd . melt ( df , id_vars = [ 'year' , 'artist' , 'track' , 'time' , 'genre' , 'entered' , 'peaked' ]) value_vars=[ ] This argument says it is for stating all the columns to \"unpivot\". Phew, we have a lot of weeks... do we have to list them all? That's the beauty of this function. If we simply leave this blank it will default to unpivot all of the columns that are not listed in the \"id_vars=[ ]\" argument. SWEET!!! var_name=[ ] So we have this tricky thing to do with unpivoting... we'll have all the week columns get turned into row cells and they will need a column header. To do this we declare the \"var_name=[ ]\" argument with the column header we want. df2 = pd . melt ( df , id_vars = [ 'year' , 'artist' , 'track' , 'time' , 'genre' , 'entered' , 'peaked' ], var_name = 'week' ) value_name[ ] Now what about all the values that are listed in each week column right now? Where are those supposed to go? Glad you asked that. These are the values that we are \"unpivoting\" but they to will need a column header since we just took the week columns away. We know that those values represent the weekly ranking of a song during that week so we'll \"unpivot\" those values to a new column called \"rank\" df2 = pd . melt ( df , id_vars = [ 'year' , 'artist' , 'track' , 'time' , 'genre' , 'entered' , 'peaked' ], var_name = 'week' , value_name = 'rank' ) Check your work. df2 . head () year artist track time genre entered peaked week rank 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 1 78.0 1 2000 Santana Maria, Maria 4:18 Rock 2000-02-12 2000-04-08 1 15.0 2 2000 Savage Garden I Knew I Loved You 4:07 Rock 1999-10-23 2000-01-29 1 71.0 3 2000 Madonna Music 3:45 Rock 2000-08-12 2000-09-16 1 41.0 4 2000 Aguilera, Christina Come On Over Baby (All I Want Is You) 3:38 Rock 2000-08-05 2000-10-14 1 57.0 Cool! It worked! We now have the rows representing the lowest level of data in our dataset. What did this do to the shape of our table? Remember what it was at the beginning? Lets check it again to see what happened. df2 . shape (24092, 9) Woah. That is MUCH longer. Nothing to worry about though. You'll find that pandas can handle this length of a data table easily. Most importantly we have our dataset setup in a way where we can work with it much better.","tags":"wrangling_data","loc":"http://www.instantinate.com/wrangling_data/unpivoting_columns.html","title":"Unpivoting Columns"},{"url":"http://www.instantinate.com/python/dictionary_comprehension_for_list_of_lists.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. Turn a list of lists into a dictionary with one line of code... the magical comprehension. Click here for a refresher on how to read in data with the csv.reader import csv with open ( '../data/sales_data.csv' , 'rU' ) as f : data = [] reader = csv . reader ( f ) for row in reader : data . append ( row ) print data [['volume_sold', '2015_margin', '2015_q1_sales', '2016_q1_sales'], ['18.4207604861', '93.8022814583', '337166.53', '337804.05'], ['4.77650991918', '21.0824246877', '22351.86', '21736.63'], ['16.6024006077', '93.6124943024', '277764.46', '306942.27'], ['4.29611149826', '16.8247038328', '16805.11', '9307.75'], ['8.15602328201', '35.0114570034', '54411.42', '58939.9'], ['5.00512242518', '31.8774372328', '255939.81', '332979.03'], ['14.60675', '76.5189730216', '319020.69', '302592.88'], ['4.45646649485', '19.3373453608', '45340.33', '55315.23'], ['5.04752965097', '26.142470349', '57849.23', '42398.57'], ['5.38807023767', '22.4270237673', '51031.04', '56241.57'], ['9.34734863474', '41.892132964', '68657.91', '3536.14'], ['10.9303977273', '66.4030492424', '4151.93', '137416.93'], ['6.27020860495', '47.8693242069', '121837.56', '158476.55'], ['12.3959191176', '86.7601504011', '146725.31', '125731.51'], ['4.55771189614', '22.9481762576', '119287.76', '21834.49'], ['4.20012242627', '18.7060545353', '20335.03', '39609.55'], ['10.2528698945', '44.0411766297', '110552.94', '204038.87'], ['12.0767847594', '62.1990040107', '204237.78', '15689.8'], ['3.7250952381', '14.2518095238', '16939.15', '48545.69'], ['3.21072662722', '16.0432686391', '55498.12', '16320.74'], ['6.29097142857', '25.1911714286', '15983.98', '53182.55'], ['7.43482131661', '31.7530658307', '71758.66', '30402.43'], ['4.37622478386', '23.1614514016', '62988.17', '47217.4'], ['12.9889127838', '48.8207407407', '29637.75', '6367.76'], ['11.6974557522', '73.2315044248', '48759.71', '329252.09'], ['5.96517512509', '23.4503335716', '89736.7', '332976.05'], ['3.94522273425', '14.1447926267', '5577.61', '234926.02'], ['7.36958530901', '36.4085284899', '310035.66', '151934.45'], ['7.34350882699', '36.1718619066', '310718.21', '314068.92'], ['12.3500273544', '59.8934779211', '258284.84', '61847.52'], ['8.41791967737', '37.1085548647', '150049.71', '203644.27'], ['10.2608361718', '52.4055916932', '309568.16', '123632.78'], ['7.82435369972', '30.681099171', '66468.32', '1050711.75'], ['10.3314300532', '48.1333683392', '321983.24', '149791.31'], ['12.5284878049', '47.7406803594', '115531.13', '61560.7'], ['18.7447505256', '97.2243398739', '926706.76', '260966.41'], ['6.65773264189', '31.2923926822', '157981.2', '160278.07'], ['10.6321289355', '35.27017991', '51078.36', '78108.56'], ['6.92770422965', '31.9091555963', '272703.74', '253886.88'], ['6.61817422161', '29.1482532051', '180760.93', '173395.13'], ['7.12444444444', '32.6235916667', '72185.66', '91524.76'], ['9.84966032435', '47.9893704508', '263161.4', '247802.36'], ['11.5058377559', '55.5221865049', '164809.62', '115591.86'], ['6.30981315215', '31.941637952', '60986.22', '233350.35'], ['10.1866219839', '49.3420628537', '210690.45', '84940.69'], ['10.1221793301', '42.8693852124', '139068.33', '115993.15'], ['10.8003469032', '53.1849073341', '209403.19', '125207.35'], ['7.26782845188', '25.4050062762', '75110.03', '161300.14'], ['10.6737166742', '43.9390962494', '123291.57', '164939.85'], ['9.15026865672', '44.5348318408', '157591.18', '147632.84'], ['8.12418187744', '39.530065189', '163684.93', '168603.49'], ['6.27579970306', '31.5106033203', '146850.63', '342772.02'], ['10.6772953319', '50.1331972789', '143950.01', '102829.3'], ['5.88898828541', '28.7115801384', '136167.32', '216415.08'], ['10.6401714545', '52.4235630748', '327172.32', '174492.82'], ['4.75559643255', '24.0028010033', '105869.94', '41213.34'], ['10.246884068', '47.3184344342', '176266.68', '91213.98'], ['10.29268081', '49.1944300868', '176720.07', '256707.1'], ['4.41819548872', '19.9170827068', '44836.7', '75866.02'], ['7.10134734573', '32.71362436', '65830.25', '50434.54'], ['8.00611901938', '36.2773863187', '228680.53', '141712.31'], ['7.79050337838', '35.2223614865', '73810.68', '91203.94'], ['11.1293822598', '40.0093030623', '54655.98', '156730.02'], ['9.34847653987', '45.5890982815', '156056.28', '24206.67'], ['6.31088643791', '28.2592197712', '80923.05', '218335.21'], ['11.6256060606', '55.1925378788', '167606.75', '325127.17'], ['6.65440717629', '29.106349454', '25575.88', '94229.41'], ['7.93041476808', '39.1116473887', '211902.95', '117784.92'], ['9.003562316', '46.7565544652', '312502.67', '166847.54'], ['14.4394353772', '57.2886237318', '85217.68', '73117.79'], ['11.0115404852', '46.2992604502', '89525.66', '100545.74'], ['5.72389564186', '25.5950192707', '176252.03', '68966.63'], ['7.77732012195', '36.7157591463', '68258.49', '96059.65'], ['4.75918372602', '20.0006463242', '88008.31', '211600.93'], ['7.78586691659', '29.0692439863', '67219.54', '234774.87'], ['5.03499140759', '22.7097016091', '100722.77', '268226.7'], ['11.6845098446', '58.9404393782', '147242.83', '58122.85'], ['5.14772910448', '25.7666753731', '227702.34', '106662.37'], ['10.0860303763', '49.1620319871', '286763.37', '147781.41'], ['7.94465682362', '33.9956304868', '59531.82', '67514.95'], ['5.29439978635', '24.4766550941', '122811.86', '170466.11'], ['11.8265363003', '52.3184580805', '156513.07', '94158.15'], ['6.300925868', '30.5095964967', '55538.11', '139676.38'], ['8.64269487751', '36.2843302577', '143498.37', '167692.5'], ['6.04822838631', '30.1712350678', '84998.58', '180371.58'], ['9.47492913676', '41.0171194552', '138010.02', '298889.32'], ['3.99185767098', '20.0513661569', '146424.29', '133868.58'], ['8.59207381371', '45.5986531395', '175475.49', '39138.81'], ['7.21148957299', '35.8870443261', '247716.78', '343447.68'], ['8.19108557079', '37.9469113386', '130512.99', '191091.14'], ['7.69531160115', '29.3906663471', '52054.1', '184729.87'], ['13.6351500118', '64.7439062131', '332692.67', '37179.58'], ['6.96681681682', '35.0125088725', '135418.28', '40160.26'], ['11.2323237697', '52.2156089683', '163104.8', '40970.96'], ['5.09546375267', '22.4304264392', '45612.45', '135871.39'], ['11.9368836649', '49.1816335142', '56995.82', '92843.04'], ['5.90627376426', '25.0323193916', '39867.38', '295483.47'], ['9.12709469154', '45.8722118604', '133081.37', '130377.49'], ['7.7544572697', '34.7604217536', '108362.38', '192747.83'], ['7.58599675412', '39.9868794518', '271021.94', '103958.63'], ['7.43725207583', '34.6033448222', '134589.33', '106452.13'], ['9.8798900768', '41.6839918687', '216171.01', '12380.85'], ['9.30319388778', '40.8443061122', '104354.49', '94010.77'], ['9.21365050557', '44.7063598652', '105046.24', '51773.27'], ['5.18177205308', '21.1562997658', '13677.13', '242057.36'], ['8.55507774799', '41.547077748', '89053.08', '392226.28'], ['5.78126590331', '24.6607061069', '51427.05', '369424.59'], ['8.0710230118', '40.1006629307', '264408.55', '73141.35'], ['10.1250032713', '50.3450808487', '294990.21', '292690.86'], ['11.0196516733', '51.4318879891', '358098.75', '172189.31'], ['8.17666774927', '32.3875251705', '68077.3', '211032.4'], ['9.42171292239', '53.4813935145', '313345.86', '171119.44'], ['4.85870921867', '24.5552275389', '163324.44', '201433.01'], ['9.31378525791', '43.8089558984', '203099.8', '200599.95'], ['8.30018036767', '43.4441935484', '104044.33', '476439.16'], ['6.50688776965', '33.4178523235', '202835.96', '215106.77'], ['9.5852099677', '44.3454176281', '192934.41', '204808.54'], ['12.45572275', '65.5244131168', '447305.5', '37858.67'], ['8.09288312646', '39.7529703822', '205170.43', '382790.84'], ['8.68651837806', '46.2175087515', '201282.29', '58332.41'], ['8.34731752963', '32.7325389894', '36855.39', '2443253.18'], ['9.29224055739', '49.786953612', '381036.91', '209066.17'], ['9.77711182109', '44.9972396166', '46509.65', '137697.56'], ['29.878030039', '168.245861698', '2337324.42', '129489.89'], ['8.78393692369', '49.313768542', '208389.88', '72810.95'], ['11.9757685161', '56.569739846', '145742.16', '158368.58'], ['11.1401021385', '47.8089690393', '187407.93', '153305.98'], ['7.5605488194', '30.9038417358', '62335.59', '289199.43'], ['7.39098798637', '37.2649829658', '138878.63', '241311.79'], ['6.43360588592', '35.850617415', '169131.81', '343060.63'], ['13.7999774485', '68.1836227448', '280506.28', '128500.31'], ['6.44703955254', '34.2461556133', '223641.14', '79554.12'], ['8.01794477751', '46.0017090264', '361865.13', '191155.87'], ['6.2553554256', '26.9316699155', '134325.62', '111207.97'], ['9.69742181905', '48.3078228694', '145361.36', '83399.61'], ['7.77268351232', '36.216199621', '185580.5', '25290.33'], ['8.75192030725', '37.1569323092', '93901.53', '20962.08'], ['6.79288937945', '33.5548344371', '68438.51', '15307.3'], ['7.68249438202', '33.223011236', '32698.19', '10464.02'], ['4.38545511613', '20.8197112367', '22829.66', '52246.46'], ['3.60671020408', '15.8903673469', '13141.2', '75566.37'], ['8.45364705882', '35.4949411765', '8738.45', '103585.1'], ['5.21488185976', '26.0287842988', '48441.46', '276396.86'], ['8.40056149733', '37.0314331551', '87115.74', '305851.47'], ['6.84136327817', '29.1820981087', '102794.01', '260453.94'], ['12.5099672623', '72.4642572637', '306947.29', '30441.76'], ['9.0148700565', '52.1879774011', '225615.38', '10564.6'], ['7.20036424796', '40.9748793849', '327334.28', '19357.49'], ['5.77809677419', '23.8230645161', '27114.29', '613414.14'], ['4.94129392971', '24.1203833866', '7539.14', '63793.71'], ['6.00045070423', '26.6531361502', '20378.73', '78706.47'], ['11.9971174753', '65.067257829', '501953.12', '22675.76'], ['10.6377691184', '51.4916463712', '66097.53', '4566.74'], ['8.56422809829', '36.8113087607', '89327.88', '14050.14'], ['8.62268641471', '35.087834525', '20534.72', '11883.3'], ['4.83114713217', '17.9033416459', '5742.23', '11597.18'], ['10.2701848998', '38.2349306626', '15987.52', '23801.1'], ['12.5816945607', '41.9222384937', '13518.07', '22824.14'], ['16.0599706745', '50.8491202346', '17574.68', '18957.2'], ['11.8677385892', '36.282033195', '24357.22', '19832.94'], ['10.2945011338', '39.7768253968', '29611.32', '4258.14'], ['4.17606557377', '16.4770435274', '18571.65', '17116.11'], ['9.36189873418', '35.4246202532', '18712.28', '7499.47'], ['11.0917085427', '48.8443718593', '8458.2', '11367.4'], ['5.3244966443', '20.2525251678', '19089.74', '8041.09'], ['6.63090439276', '24.2994315245', '7305.46', '11966.94'], ['8.58392405063', '34.2229535865', '14796.7', '12707.72'], ['5.53106109325', '25.2890353698', '8200.55', '22130.31'], ['6.13912310287', '26.0607419899', '11412.54', '20817.32'], ['8.47737603306', '33.6023140496', '11228.97', '16786.98'], ['8.44393241167', '37.4071121352', '17232.45', '19203.82'], ['5.15196394076', '20.6980424984', '21340.78', '25302.92'], ['6.53706864564', '22.8882189239', '16617.85', '18650.96'], ['8.50044523598', '27.5077292965', '21711.71', '14626.29'], ['3.93154326923', '21.8116586538', '28128.95', '46323.73'], ['6.16368913858', '25.4285205993', '21803.2', '535381.86'], ['4.90444711538', '19.5683173077', '12517.29', '812011.78'], ['7.40241271087', '26.9353354257', '53988.92', '206247.57'], ['47.5032692308', '235.730677515', '555707.4', '6402.78'], ['55.7391800938', '268.869600245', '1082136.01', '100765.67'], ['11.8407803201', '56.4333884407', '192089.46', '20098.61'], ['7.00229357798', '26.4409174312', '5574.99', '68230.36'], ['8.75314206706', '45.2938472174', '80241.27', '596063.0'], ['3.14774130328', '15.5945516903', '27043.54', '87471.43'], ['7.1967787944', '27.2886517761', '61977.54', '431990.7'], ['76.2036918138', '367.225653291', '977772.62', '136717.57'], ['10.8043371086', '42.0311992005', '41905.18', '30008.81'], ['10.705327051', '45.8914772727', '87839.45', '19420.34'], ['51.8006862745', '255.153235294', '445058.32', '274752.6'], ['5.88277871731', '27.5122713672', '127495.18', '10315.35'], ['6.68640645161', '26.1030967742', '23874.67', '45252.42'], ['5.83335488041', '23.7416031028', '21535.87', '433832.28'], ['45.5560956385', '218.008349974', '276096.18', '74215.43'], ['5.17260575296', '21.5487817259', '8506.79', '78137.29'], ['10.118018018', '46.7007087087', '49163.01', '83915.72'], ['51.6755374204', '233.533188694', '434110.57', '372240.72'], ['2.79463149728', '11.9961176992', '73789.38', '148021.9'], ['7.61169779853', '38.8099733155', '88006.84', '31184.18'], ['15.6976512739', '105.035207006', '117958.96', '33324.36'], ['50.2758932156', '225.055138499', '407738.79', '32079.13']] Every dictionary needs some keys. Below we're \"slicing\" the data from the top row of the dataset (which is our headers) and assigning it to the \"header\" variable. Can you see where this is going? header = data [ 0 ][:] Next we're going back to the list of lists to remove that first row so we don't have any conflicts. At the end we'll end up with two lists one that can be used as \"Keys\" (the header row), and one that can be used as \"Values\" (everything else). data . pop ( 0 ) header = data [ 0 ][:] data . pop ( 0 ) ['volume_sold', '2015_margin', '2015_q1_sales', '2016_q1_sales'] Now for that scary comprehension thingy. So we've got two lists, one that has only 4 items and one that has lots and lots of items and each of those items has 4 items embedded(list of lists). Conceptually, we want to take the 1st entry from each of those embedded lists and map it to the first key value. Then we want to get the second and map it to the second value. etc. etc. Here's a refresher on how dictionary comprehensions are laid out. { key_extractor ( obj ): value_extractor ( obj ) for i , obj in enumerate ( objects )} Values to be returned. Remember that comprehensions are shorthand for other types of functions. So it starts with the values that are going to be returned, in this case we're going to extract a key, and a value for each list in the big list. { column_name : [ row [ i ] for row in data ] Values to iterate through. Then we define the variable \"i\" to iterate through each list. Since we are going through two things at a time (key and value) we need to specify the second item to pull when we iterrate through which will be the \"column name\". That will act as our \"key\" in the dictionary. { column_name : [ row [ i ] for row in data ] for i , column_name Enumerate is the key. Next we need to get two values to iterate through. The \"enumerate\" function is the magical key for this. It will return both the index number and the value in a list that can then be passed to the variables to iterate through.... phew that was a mouthful... so if that was difficult to follow unpack each statement and trace it to the full function until you see what is happening. { column_name : [ row [ i ] for row in data ] for i , column_name in enumerate ( header )} Did you notice that other thing in here? Yeah I did see that mean thing in there next to the first \"column name\". Because we're iterating over a list of lists we need to define all of the items to be returned as values with that key. So we put a second simple List Comprehension to go through all the list of lists and return the value that is at each index point that matches the \"i\" variable. [ row [ i ] for row in data ] Now to see how it all goes together and returns a dictionary of values called \"Sales_Data\" Notice that each of the \"Column Headers\" (Dictionary Keys), has a list of \"Dictionary Values\" that is the corresponding list item for each position (The first item in the list goes to the first column, the second item goes to the second column etc.). sales_data = { column_name : [ row [ i ] for row in data ] for i , column_name in enumerate ( header )} print sales_data {'volume_sold': ['18.4207604861', '4.77650991918', '16.6024006077', '4.29611149826', '8.15602328201', '5.00512242518', '14.60675', '4.45646649485', '5.04752965097', '5.38807023767', '9.34734863474', '10.9303977273', '6.27020860495', '12.3959191176', '4.55771189614', '4.20012242627', '10.2528698945', '12.0767847594', '3.7250952381', '3.21072662722', '6.29097142857', '7.43482131661', '4.37622478386', '12.9889127838', '11.6974557522', '5.96517512509', '3.94522273425', '7.36958530901', '7.34350882699', '12.3500273544', '8.41791967737', '10.2608361718', '7.82435369972', '10.3314300532', '12.5284878049', '18.7447505256', '6.65773264189', '10.6321289355', '6.92770422965', '6.61817422161', '7.12444444444', '9.84966032435', '11.5058377559', '6.30981315215', '10.1866219839', '10.1221793301', '10.8003469032', '7.26782845188', '10.6737166742', '9.15026865672', '8.12418187744', '6.27579970306', '10.6772953319', '5.88898828541', '10.6401714545', '4.75559643255', '10.246884068', '10.29268081', '4.41819548872', '7.10134734573', '8.00611901938', '7.79050337838', '11.1293822598', '9.34847653987', '6.31088643791', '11.6256060606', '6.65440717629', '7.93041476808', '9.003562316', '14.4394353772', '11.0115404852', '5.72389564186', '7.77732012195', '4.75918372602', '7.78586691659', '5.03499140759', '11.6845098446', '5.14772910448', '10.0860303763', '7.94465682362', '5.29439978635', '11.8265363003', '6.300925868', '8.64269487751', '6.04822838631', '9.47492913676', '3.99185767098', '8.59207381371', '7.21148957299', '8.19108557079', '7.69531160115', '13.6351500118', '6.96681681682', '11.2323237697', '5.09546375267', '11.9368836649', '5.90627376426', '9.12709469154', '7.7544572697', '7.58599675412', '7.43725207583', '9.8798900768', '9.30319388778', '9.21365050557', '5.18177205308', '8.55507774799', '5.78126590331', '8.0710230118', '10.1250032713', '11.0196516733', '8.17666774927', '9.42171292239', '4.85870921867', '9.31378525791', '8.30018036767', '6.50688776965', '9.5852099677', '12.45572275', '8.09288312646', '8.68651837806', '8.34731752963', '9.29224055739', '9.77711182109', '29.878030039', '8.78393692369', '11.9757685161', '11.1401021385', '7.5605488194', '7.39098798637', '6.43360588592', '13.7999774485', '6.44703955254', '8.01794477751', '6.2553554256', '9.69742181905', '7.77268351232', '8.75192030725', '6.79288937945', '7.68249438202', '4.38545511613', '3.60671020408', '8.45364705882', '5.21488185976', '8.40056149733', '6.84136327817', '12.5099672623', '9.0148700565', '7.20036424796', '5.77809677419', '4.94129392971', '6.00045070423', '11.9971174753', '10.6377691184', '8.56422809829', '8.62268641471', '4.83114713217', '10.2701848998', '12.5816945607', '16.0599706745', '11.8677385892', '10.2945011338', '4.17606557377', '9.36189873418', '11.0917085427', '5.3244966443', '6.63090439276', '8.58392405063', '5.53106109325', '6.13912310287', '8.47737603306', '8.44393241167', '5.15196394076', '6.53706864564', '8.50044523598', '3.93154326923', '6.16368913858', '4.90444711538', '7.40241271087', '47.5032692308', '55.7391800938', '11.8407803201', '7.00229357798', '8.75314206706', '3.14774130328', '7.1967787944', '76.2036918138', '10.8043371086', '10.705327051', '51.8006862745', '5.88277871731', '6.68640645161', '5.83335488041', '45.5560956385', '5.17260575296', '10.118018018', '51.6755374204', '2.79463149728', '7.61169779853', '15.6976512739', '50.2758932156'], '2015_q1_sales': ['337166.53', '22351.86', '277764.46', '16805.11', '54411.42', '255939.81', '319020.69', '45340.33', '57849.23', '51031.04', '68657.91', '4151.93', '121837.56', '146725.31', '119287.76', '20335.03', '110552.94', '204237.78', '16939.15', '55498.12', '15983.98', '71758.66', '62988.17', '29637.75', '48759.71', '89736.7', '5577.61', '310035.66', '310718.21', '258284.84', '150049.71', '309568.16', '66468.32', '321983.24', '115531.13', '926706.76', '157981.2', '51078.36', '272703.74', '180760.93', '72185.66', '263161.4', '164809.62', '60986.22', '210690.45', '139068.33', '209403.19', '75110.03', '123291.57', '157591.18', '163684.93', '146850.63', '143950.01', '136167.32', '327172.32', '105869.94', '176266.68', '176720.07', '44836.7', '65830.25', '228680.53', '73810.68', '54655.98', '156056.28', '80923.05', '167606.75', '25575.88', '211902.95', '312502.67', '85217.68', '89525.66', '176252.03', '68258.49', '88008.31', '67219.54', '100722.77', '147242.83', '227702.34', '286763.37', '59531.82', '122811.86', '156513.07', '55538.11', '143498.37', '84998.58', '138010.02', '146424.29', '175475.49', '247716.78', '130512.99', '52054.1', '332692.67', '135418.28', '163104.8', '45612.45', '56995.82', '39867.38', '133081.37', '108362.38', '271021.94', '134589.33', '216171.01', '104354.49', '105046.24', '13677.13', '89053.08', '51427.05', '264408.55', '294990.21', '358098.75', '68077.3', '313345.86', '163324.44', '203099.8', '104044.33', '202835.96', '192934.41', '447305.5', '205170.43', '201282.29', '36855.39', '381036.91', '46509.65', '2337324.42', '208389.88', '145742.16', '187407.93', '62335.59', '138878.63', '169131.81', '280506.28', '223641.14', '361865.13', '134325.62', '145361.36', '185580.5', '93901.53', '68438.51', '32698.19', '22829.66', '13141.2', '8738.45', '48441.46', '87115.74', '102794.01', '306947.29', '225615.38', '327334.28', '27114.29', '7539.14', '20378.73', '501953.12', '66097.53', '89327.88', '20534.72', '5742.23', '15987.52', '13518.07', '17574.68', '24357.22', '29611.32', '18571.65', '18712.28', '8458.2', '19089.74', '7305.46', '14796.7', '8200.55', '11412.54', '11228.97', '17232.45', '21340.78', '16617.85', '21711.71', '28128.95', '21803.2', '12517.29', '53988.92', '555707.4', '1082136.01', '192089.46', '5574.99', '80241.27', '27043.54', '61977.54', '977772.62', '41905.18', '87839.45', '445058.32', '127495.18', '23874.67', '21535.87', '276096.18', '8506.79', '49163.01', '434110.57', '73789.38', '88006.84', '117958.96', '407738.79'], '2016_q1_sales': ['337804.05', '21736.63', '306942.27', '9307.75', '58939.9', '332979.03', '302592.88', '55315.23', '42398.57', '56241.57', '3536.14', '137416.93', '158476.55', '125731.51', '21834.49', '39609.55', '204038.87', '15689.8', '48545.69', '16320.74', '53182.55', '30402.43', '47217.4', '6367.76', '329252.09', '332976.05', '234926.02', '151934.45', '314068.92', '61847.52', '203644.27', '123632.78', '1050711.75', '149791.31', '61560.7', '260966.41', '160278.07', '78108.56', '253886.88', '173395.13', '91524.76', '247802.36', '115591.86', '233350.35', '84940.69', '115993.15', '125207.35', '161300.14', '164939.85', '147632.84', '168603.49', '342772.02', '102829.3', '216415.08', '174492.82', '41213.34', '91213.98', '256707.1', '75866.02', '50434.54', '141712.31', '91203.94', '156730.02', '24206.67', '218335.21', '325127.17', '94229.41', '117784.92', '166847.54', '73117.79', '100545.74', '68966.63', '96059.65', '211600.93', '234774.87', '268226.7', '58122.85', '106662.37', '147781.41', '67514.95', '170466.11', '94158.15', '139676.38', '167692.5', '180371.58', '298889.32', '133868.58', '39138.81', '343447.68', '191091.14', '184729.87', '37179.58', '40160.26', '40970.96', '135871.39', '92843.04', '295483.47', '130377.49', '192747.83', '103958.63', '106452.13', '12380.85', '94010.77', '51773.27', '242057.36', '392226.28', '369424.59', '73141.35', '292690.86', '172189.31', '211032.4', '171119.44', '201433.01', '200599.95', '476439.16', '215106.77', '204808.54', '37858.67', '382790.84', '58332.41', '2443253.18', '209066.17', '137697.56', '129489.89', '72810.95', '158368.58', '153305.98', '289199.43', '241311.79', '343060.63', '128500.31', '79554.12', '191155.87', '111207.97', '83399.61', '25290.33', '20962.08', '15307.3', '10464.02', '52246.46', '75566.37', '103585.1', '276396.86', '305851.47', '260453.94', '30441.76', '10564.6', '19357.49', '613414.14', '63793.71', '78706.47', '22675.76', '4566.74', '14050.14', '11883.3', '11597.18', '23801.1', '22824.14', '18957.2', '19832.94', '4258.14', '17116.11', '7499.47', '11367.4', '8041.09', '11966.94', '12707.72', '22130.31', '20817.32', '16786.98', '19203.82', '25302.92', '18650.96', '14626.29', '46323.73', '535381.86', '812011.78', '206247.57', '6402.78', '100765.67', '20098.61', '68230.36', '596063.0', '87471.43', '431990.7', '136717.57', '30008.81', '19420.34', '274752.6', '10315.35', '45252.42', '433832.28', '74215.43', '78137.29', '83915.72', '372240.72', '148021.9', '31184.18', '33324.36', '32079.13'], '2015_margin': ['93.8022814583', '21.0824246877', '93.6124943024', '16.8247038328', '35.0114570034', '31.8774372328', '76.5189730216', '19.3373453608', '26.142470349', '22.4270237673', '41.892132964', '66.4030492424', '47.8693242069', '86.7601504011', '22.9481762576', '18.7060545353', '44.0411766297', '62.1990040107', '14.2518095238', '16.0432686391', '25.1911714286', '31.7530658307', '23.1614514016', '48.8207407407', '73.2315044248', '23.4503335716', '14.1447926267', '36.4085284899', '36.1718619066', '59.8934779211', '37.1085548647', '52.4055916932', '30.681099171', '48.1333683392', '47.7406803594', '97.2243398739', '31.2923926822', '35.27017991', '31.9091555963', '29.1482532051', '32.6235916667', '47.9893704508', '55.5221865049', '31.941637952', '49.3420628537', '42.8693852124', '53.1849073341', '25.4050062762', '43.9390962494', '44.5348318408', '39.530065189', '31.5106033203', '50.1331972789', '28.7115801384', '52.4235630748', '24.0028010033', '47.3184344342', '49.1944300868', '19.9170827068', '32.71362436', '36.2773863187', '35.2223614865', '40.0093030623', '45.5890982815', '28.2592197712', '55.1925378788', '29.106349454', '39.1116473887', '46.7565544652', '57.2886237318', '46.2992604502', '25.5950192707', '36.7157591463', '20.0006463242', '29.0692439863', '22.7097016091', '58.9404393782', '25.7666753731', '49.1620319871', '33.9956304868', '24.4766550941', '52.3184580805', '30.5095964967', '36.2843302577', '30.1712350678', '41.0171194552', '20.0513661569', '45.5986531395', '35.8870443261', '37.9469113386', '29.3906663471', '64.7439062131', '35.0125088725', '52.2156089683', '22.4304264392', '49.1816335142', '25.0323193916', '45.8722118604', '34.7604217536', '39.9868794518', '34.6033448222', '41.6839918687', '40.8443061122', '44.7063598652', '21.1562997658', '41.547077748', '24.6607061069', '40.1006629307', '50.3450808487', '51.4318879891', '32.3875251705', '53.4813935145', '24.5552275389', '43.8089558984', '43.4441935484', '33.4178523235', '44.3454176281', '65.5244131168', '39.7529703822', '46.2175087515', '32.7325389894', '49.786953612', '44.9972396166', '168.245861698', '49.313768542', '56.569739846', '47.8089690393', '30.9038417358', '37.2649829658', '35.850617415', '68.1836227448', '34.2461556133', '46.0017090264', '26.9316699155', '48.3078228694', '36.216199621', '37.1569323092', '33.5548344371', '33.223011236', '20.8197112367', '15.8903673469', '35.4949411765', '26.0287842988', '37.0314331551', '29.1820981087', '72.4642572637', '52.1879774011', '40.9748793849', '23.8230645161', '24.1203833866', '26.6531361502', '65.067257829', '51.4916463712', '36.8113087607', '35.087834525', '17.9033416459', '38.2349306626', '41.9222384937', '50.8491202346', '36.282033195', '39.7768253968', '16.4770435274', '35.4246202532', '48.8443718593', '20.2525251678', '24.2994315245', '34.2229535865', '25.2890353698', '26.0607419899', '33.6023140496', '37.4071121352', '20.6980424984', '22.8882189239', '27.5077292965', '21.8116586538', '25.4285205993', '19.5683173077', '26.9353354257', '235.730677515', '268.869600245', '56.4333884407', '26.4409174312', '45.2938472174', '15.5945516903', '27.2886517761', '367.225653291', '42.0311992005', '45.8914772727', '255.153235294', '27.5122713672', '26.1030967742', '23.7416031028', '218.008349974', '21.5487817259', '46.7007087087', '233.533188694', '11.9961176992', '38.8099733155', '105.035207006', '225.055138499']}","tags":"python","loc":"http://www.instantinate.com/python/dictionary_comprehension_for_list_of_lists.html","title":"Dictionary Comprehension for List of Lists"},{"url":"http://www.instantinate.com/how_to/dictionary_comprehensions.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. Turn a list of lists into a dictionary with one line of code... the magical comprehension. Click here for a refresher on how to read in data with the csv.reader import csv with open ( '../data/sales_data.csv' , 'rU' ) as f : data = [] reader = csv . reader ( f ) for row in reader : data . append ( row ) print data [['volume_sold', '2015_margin', '2015_q1_sales', '2016_q1_sales'], ['18.4207604861', '93.8022814583', '337166.53', '337804.05'], ['4.77650991918', '21.0824246877', '22351.86', '21736.63'], ['16.6024006077', '93.6124943024', '277764.46', '306942.27'], ['4.29611149826', '16.8247038328', '16805.11', '9307.75'], ['8.15602328201', '35.0114570034', '54411.42', '58939.9'], ['5.00512242518', '31.8774372328', '255939.81', '332979.03'], ['14.60675', '76.5189730216', '319020.69', '302592.88'], ['4.45646649485', '19.3373453608', '45340.33', '55315.23'], ['5.04752965097', '26.142470349', '57849.23', '42398.57'], ['5.38807023767', '22.4270237673', '51031.04', '56241.57'], ['9.34734863474', '41.892132964', '68657.91', '3536.14'], ['10.9303977273', '66.4030492424', '4151.93', '137416.93'], ['6.27020860495', '47.8693242069', '121837.56', '158476.55'], ['12.3959191176', '86.7601504011', '146725.31', '125731.51'], ['4.55771189614', '22.9481762576', '119287.76', '21834.49'], ['4.20012242627', '18.7060545353', '20335.03', '39609.55'], ['10.2528698945', '44.0411766297', '110552.94', '204038.87'], ['12.0767847594', '62.1990040107', '204237.78', '15689.8'], ['3.7250952381', '14.2518095238', '16939.15', '48545.69'], ['3.21072662722', '16.0432686391', '55498.12', '16320.74'], ['6.29097142857', '25.1911714286', '15983.98', '53182.55'], ['7.43482131661', '31.7530658307', '71758.66', '30402.43'], ['4.37622478386', '23.1614514016', '62988.17', '47217.4'], ['12.9889127838', '48.8207407407', '29637.75', '6367.76'], ['11.6974557522', '73.2315044248', '48759.71', '329252.09'], ['5.96517512509', '23.4503335716', '89736.7', '332976.05'], ['3.94522273425', '14.1447926267', '5577.61', '234926.02'], ['7.36958530901', '36.4085284899', '310035.66', '151934.45'], ['7.34350882699', '36.1718619066', '310718.21', '314068.92'], ['12.3500273544', '59.8934779211', '258284.84', '61847.52'], ['8.41791967737', '37.1085548647', '150049.71', '203644.27'], ['10.2608361718', '52.4055916932', '309568.16', '123632.78'], ['7.82435369972', '30.681099171', '66468.32', '1050711.75'], ['10.3314300532', '48.1333683392', '321983.24', '149791.31'], ['12.5284878049', '47.7406803594', '115531.13', '61560.7'], ['18.7447505256', '97.2243398739', '926706.76', '260966.41'], ['6.65773264189', '31.2923926822', '157981.2', '160278.07'], ['10.6321289355', '35.27017991', '51078.36', '78108.56'], ['6.92770422965', '31.9091555963', '272703.74', '253886.88'], ['6.61817422161', '29.1482532051', '180760.93', '173395.13'], ['7.12444444444', '32.6235916667', '72185.66', '91524.76'], ['9.84966032435', '47.9893704508', '263161.4', '247802.36'], ['11.5058377559', '55.5221865049', '164809.62', '115591.86'], ['6.30981315215', '31.941637952', '60986.22', '233350.35'], ['10.1866219839', '49.3420628537', '210690.45', '84940.69'], ['10.1221793301', '42.8693852124', '139068.33', '115993.15'], ['10.8003469032', '53.1849073341', '209403.19', '125207.35'], ['7.26782845188', '25.4050062762', '75110.03', '161300.14'], ['10.6737166742', '43.9390962494', '123291.57', '164939.85'], ['9.15026865672', '44.5348318408', '157591.18', '147632.84'], ['8.12418187744', '39.530065189', '163684.93', '168603.49'], ['6.27579970306', '31.5106033203', '146850.63', '342772.02'], ['10.6772953319', '50.1331972789', '143950.01', '102829.3'], ['5.88898828541', '28.7115801384', '136167.32', '216415.08'], ['10.6401714545', '52.4235630748', '327172.32', '174492.82'], ['4.75559643255', '24.0028010033', '105869.94', '41213.34'], ['10.246884068', '47.3184344342', '176266.68', '91213.98'], ['10.29268081', '49.1944300868', '176720.07', '256707.1'], ['4.41819548872', '19.9170827068', '44836.7', '75866.02'], ['7.10134734573', '32.71362436', '65830.25', '50434.54'], ['8.00611901938', '36.2773863187', '228680.53', '141712.31'], ['7.79050337838', '35.2223614865', '73810.68', '91203.94'], ['11.1293822598', '40.0093030623', '54655.98', '156730.02'], ['9.34847653987', '45.5890982815', '156056.28', '24206.67'], ['6.31088643791', '28.2592197712', '80923.05', '218335.21'], ['11.6256060606', '55.1925378788', '167606.75', '325127.17'], ['6.65440717629', '29.106349454', '25575.88', '94229.41'], ['7.93041476808', '39.1116473887', '211902.95', '117784.92'], ['9.003562316', '46.7565544652', '312502.67', '166847.54'], ['14.4394353772', '57.2886237318', '85217.68', '73117.79'], ['11.0115404852', '46.2992604502', '89525.66', '100545.74'], ['5.72389564186', '25.5950192707', '176252.03', '68966.63'], ['7.77732012195', '36.7157591463', '68258.49', '96059.65'], ['4.75918372602', '20.0006463242', '88008.31', '211600.93'], ['7.78586691659', '29.0692439863', '67219.54', '234774.87'], ['5.03499140759', '22.7097016091', '100722.77', '268226.7'], ['11.6845098446', '58.9404393782', '147242.83', '58122.85'], ['5.14772910448', '25.7666753731', '227702.34', '106662.37'], ['10.0860303763', '49.1620319871', '286763.37', '147781.41'], ['7.94465682362', '33.9956304868', '59531.82', '67514.95'], ['5.29439978635', '24.4766550941', '122811.86', '170466.11'], ['11.8265363003', '52.3184580805', '156513.07', '94158.15'], ['6.300925868', '30.5095964967', '55538.11', '139676.38'], ['8.64269487751', '36.2843302577', '143498.37', '167692.5'], ['6.04822838631', '30.1712350678', '84998.58', '180371.58'], ['9.47492913676', '41.0171194552', '138010.02', '298889.32'], ['3.99185767098', '20.0513661569', '146424.29', '133868.58'], ['8.59207381371', '45.5986531395', '175475.49', '39138.81'], ['7.21148957299', '35.8870443261', '247716.78', '343447.68'], ['8.19108557079', '37.9469113386', '130512.99', '191091.14'], ['7.69531160115', '29.3906663471', '52054.1', '184729.87'], ['13.6351500118', '64.7439062131', '332692.67', '37179.58'], ['6.96681681682', '35.0125088725', '135418.28', '40160.26'], ['11.2323237697', '52.2156089683', '163104.8', '40970.96'], ['5.09546375267', '22.4304264392', '45612.45', '135871.39'], ['11.9368836649', '49.1816335142', '56995.82', '92843.04'], ['5.90627376426', '25.0323193916', '39867.38', '295483.47'], ['9.12709469154', '45.8722118604', '133081.37', '130377.49'], ['7.7544572697', '34.7604217536', '108362.38', '192747.83'], ['7.58599675412', '39.9868794518', '271021.94', '103958.63'], ['7.43725207583', '34.6033448222', '134589.33', '106452.13'], ['9.8798900768', '41.6839918687', '216171.01', '12380.85'], ['9.30319388778', '40.8443061122', '104354.49', '94010.77'], ['9.21365050557', '44.7063598652', '105046.24', '51773.27'], ['5.18177205308', '21.1562997658', '13677.13', '242057.36'], ['8.55507774799', '41.547077748', '89053.08', '392226.28'], ['5.78126590331', '24.6607061069', '51427.05', '369424.59'], ['8.0710230118', '40.1006629307', '264408.55', '73141.35'], ['10.1250032713', '50.3450808487', '294990.21', '292690.86'], ['11.0196516733', '51.4318879891', '358098.75', '172189.31'], ['8.17666774927', '32.3875251705', '68077.3', '211032.4'], ['9.42171292239', '53.4813935145', '313345.86', '171119.44'], ['4.85870921867', '24.5552275389', '163324.44', '201433.01'], ['9.31378525791', '43.8089558984', '203099.8', '200599.95'], ['8.30018036767', '43.4441935484', '104044.33', '476439.16'], ['6.50688776965', '33.4178523235', '202835.96', '215106.77'], ['9.5852099677', '44.3454176281', '192934.41', '204808.54'], ['12.45572275', '65.5244131168', '447305.5', '37858.67'], ['8.09288312646', '39.7529703822', '205170.43', '382790.84'], ['8.68651837806', '46.2175087515', '201282.29', '58332.41'], ['8.34731752963', '32.7325389894', '36855.39', '2443253.18'], ['9.29224055739', '49.786953612', '381036.91', '209066.17'], ['9.77711182109', '44.9972396166', '46509.65', '137697.56'], ['29.878030039', '168.245861698', '2337324.42', '129489.89'], ['8.78393692369', '49.313768542', '208389.88', '72810.95'], ['11.9757685161', '56.569739846', '145742.16', '158368.58'], ['11.1401021385', '47.8089690393', '187407.93', '153305.98'], ['7.5605488194', '30.9038417358', '62335.59', '289199.43'], ['7.39098798637', '37.2649829658', '138878.63', '241311.79'], ['6.43360588592', '35.850617415', '169131.81', '343060.63'], ['13.7999774485', '68.1836227448', '280506.28', '128500.31'], ['6.44703955254', '34.2461556133', '223641.14', '79554.12'], ['8.01794477751', '46.0017090264', '361865.13', '191155.87'], ['6.2553554256', '26.9316699155', '134325.62', '111207.97'], ['9.69742181905', '48.3078228694', '145361.36', '83399.61'], ['7.77268351232', '36.216199621', '185580.5', '25290.33'], ['8.75192030725', '37.1569323092', '93901.53', '20962.08'], ['6.79288937945', '33.5548344371', '68438.51', '15307.3'], ['7.68249438202', '33.223011236', '32698.19', '10464.02'], ['4.38545511613', '20.8197112367', '22829.66', '52246.46'], ['3.60671020408', '15.8903673469', '13141.2', '75566.37'], ['8.45364705882', '35.4949411765', '8738.45', '103585.1'], ['5.21488185976', '26.0287842988', '48441.46', '276396.86'], ['8.40056149733', '37.0314331551', '87115.74', '305851.47'], ['6.84136327817', '29.1820981087', '102794.01', '260453.94'], ['12.5099672623', '72.4642572637', '306947.29', '30441.76'], ['9.0148700565', '52.1879774011', '225615.38', '10564.6'], ['7.20036424796', '40.9748793849', '327334.28', '19357.49'], ['5.77809677419', '23.8230645161', '27114.29', '613414.14'], ['4.94129392971', '24.1203833866', '7539.14', '63793.71'], ['6.00045070423', '26.6531361502', '20378.73', '78706.47'], ['11.9971174753', '65.067257829', '501953.12', '22675.76'], ['10.6377691184', '51.4916463712', '66097.53', '4566.74'], ['8.56422809829', '36.8113087607', '89327.88', '14050.14'], ['8.62268641471', '35.087834525', '20534.72', '11883.3'], ['4.83114713217', '17.9033416459', '5742.23', '11597.18'], ['10.2701848998', '38.2349306626', '15987.52', '23801.1'], ['12.5816945607', '41.9222384937', '13518.07', '22824.14'], ['16.0599706745', '50.8491202346', '17574.68', '18957.2'], ['11.8677385892', '36.282033195', '24357.22', '19832.94'], ['10.2945011338', '39.7768253968', '29611.32', '4258.14'], ['4.17606557377', '16.4770435274', '18571.65', '17116.11'], ['9.36189873418', '35.4246202532', '18712.28', '7499.47'], ['11.0917085427', '48.8443718593', '8458.2', '11367.4'], ['5.3244966443', '20.2525251678', '19089.74', '8041.09'], ['6.63090439276', '24.2994315245', '7305.46', '11966.94'], ['8.58392405063', '34.2229535865', '14796.7', '12707.72'], ['5.53106109325', '25.2890353698', '8200.55', '22130.31'], ['6.13912310287', '26.0607419899', '11412.54', '20817.32'], ['8.47737603306', '33.6023140496', '11228.97', '16786.98'], ['8.44393241167', '37.4071121352', '17232.45', '19203.82'], ['5.15196394076', '20.6980424984', '21340.78', '25302.92'], ['6.53706864564', '22.8882189239', '16617.85', '18650.96'], ['8.50044523598', '27.5077292965', '21711.71', '14626.29'], ['3.93154326923', '21.8116586538', '28128.95', '46323.73'], ['6.16368913858', '25.4285205993', '21803.2', '535381.86'], ['4.90444711538', '19.5683173077', '12517.29', '812011.78'], ['7.40241271087', '26.9353354257', '53988.92', '206247.57'], ['47.5032692308', '235.730677515', '555707.4', '6402.78'], ['55.7391800938', '268.869600245', '1082136.01', '100765.67'], ['11.8407803201', '56.4333884407', '192089.46', '20098.61'], ['7.00229357798', '26.4409174312', '5574.99', '68230.36'], ['8.75314206706', '45.2938472174', '80241.27', '596063.0'], ['3.14774130328', '15.5945516903', '27043.54', '87471.43'], ['7.1967787944', '27.2886517761', '61977.54', '431990.7'], ['76.2036918138', '367.225653291', '977772.62', '136717.57'], ['10.8043371086', '42.0311992005', '41905.18', '30008.81'], ['10.705327051', '45.8914772727', '87839.45', '19420.34'], ['51.8006862745', '255.153235294', '445058.32', '274752.6'], ['5.88277871731', '27.5122713672', '127495.18', '10315.35'], ['6.68640645161', '26.1030967742', '23874.67', '45252.42'], ['5.83335488041', '23.7416031028', '21535.87', '433832.28'], ['45.5560956385', '218.008349974', '276096.18', '74215.43'], ['5.17260575296', '21.5487817259', '8506.79', '78137.29'], ['10.118018018', '46.7007087087', '49163.01', '83915.72'], ['51.6755374204', '233.533188694', '434110.57', '372240.72'], ['2.79463149728', '11.9961176992', '73789.38', '148021.9'], ['7.61169779853', '38.8099733155', '88006.84', '31184.18'], ['15.6976512739', '105.035207006', '117958.96', '33324.36'], ['50.2758932156', '225.055138499', '407738.79', '32079.13']] Every dictionary needs some keys. Below we're \"slicing\" the data from the top row of the dataset (which is our headers) and assigning it to the \"header\" variable. Can you see where this is going? header = data [ 0 ][:] Next we're going back to the list of lists to remove that first row so we don't have any conflicts. At the end we'll end up with two lists one that can be used as \"Keys\" (the header row), and one that can be used as \"Values\" (everything else). data . pop ( 0 ) header = data [ 0 ][:] data . pop ( 0 ) ['volume_sold', '2015_margin', '2015_q1_sales', '2016_q1_sales'] Now for that scary comprehension thingy. So we've got two lists, one that has only 4 items and one that has lots and lots of items and each of those items has 4 items embedded(list of lists). Conceptually, we want to take the 1st entry from each of those embedded lists and map it to the first key value. Then we want to get the second and map it to the second value. etc. etc. Here's a refresher on how dictionary comprehensions are laid out. { key_extractor ( obj ): value_extractor ( obj ) for i , obj in enumerate ( objects )} Values to be returned. Remember that comprehensions are shorthand for other types of functions. So it starts with the values that are going to be returned, in this case we're going to extract a key, and a value for each list in the big list. { column_name : [ row [ i ] for row in data ] Values to iterate through. Then we define the variable \"i\" to iterate through each list. Since we are going through two things at a time (key and value) we need to specify the second item to pull when we iterrate through which will be the \"column name\". That will act as our \"key\" in the dictionary. { column_name : [ row [ i ] for row in data ] for i , column_name Enumerate is the key. Next we need to get two values to iterate through. The \"enumerate\" function is the magical key for this. It will return both the index number and the value in a list that can then be passed to the variables to iterate through.... phew that was a mouthful... so if that was difficult to follow unpack each statement and trace it to the full function until you see what is happening. { column_name : [ row [ i ] for row in data ] for i , column_name in enumerate ( header )} Did you notice that other thing in here? Yeah I did see that mean thing in there next to the first \"column name\". Because we're iterating over a list of lists we need to define all of the items to be returned as values with that key. So we put a second simple List Comprehension to go through all the list of lists and return the value that is at each index point that matches the \"i\" variable. [ row [ i ] for row in data ] Now to see how it all goes together and returns a dictionary of values called \"Sales_Data\" Notice that each of the \"Column Headers\" (Dictionary Keys), has a list of \"Dictionary Values\" that is the corresponding list item for each position (The first item in the list goes to the first column, the second item goes to the second column etc.). sales_data = { column_name : [ row [ i ] for row in data ] for i , column_name in enumerate ( header )} print sales_data {'volume_sold': ['18.4207604861', '4.77650991918', '16.6024006077', '4.29611149826', '8.15602328201', '5.00512242518', '14.60675', '4.45646649485', '5.04752965097', '5.38807023767', '9.34734863474', '10.9303977273', '6.27020860495', '12.3959191176', '4.55771189614', '4.20012242627', '10.2528698945', '12.0767847594', '3.7250952381', '3.21072662722', '6.29097142857', '7.43482131661', '4.37622478386', '12.9889127838', '11.6974557522', '5.96517512509', '3.94522273425', '7.36958530901', '7.34350882699', '12.3500273544', '8.41791967737', '10.2608361718', '7.82435369972', '10.3314300532', '12.5284878049', '18.7447505256', '6.65773264189', '10.6321289355', '6.92770422965', '6.61817422161', '7.12444444444', '9.84966032435', '11.5058377559', '6.30981315215', '10.1866219839', '10.1221793301', '10.8003469032', '7.26782845188', '10.6737166742', '9.15026865672', '8.12418187744', '6.27579970306', '10.6772953319', '5.88898828541', '10.6401714545', '4.75559643255', '10.246884068', '10.29268081', '4.41819548872', '7.10134734573', '8.00611901938', '7.79050337838', '11.1293822598', '9.34847653987', '6.31088643791', '11.6256060606', '6.65440717629', '7.93041476808', '9.003562316', '14.4394353772', '11.0115404852', '5.72389564186', '7.77732012195', '4.75918372602', '7.78586691659', '5.03499140759', '11.6845098446', '5.14772910448', '10.0860303763', '7.94465682362', '5.29439978635', '11.8265363003', '6.300925868', '8.64269487751', '6.04822838631', '9.47492913676', '3.99185767098', '8.59207381371', '7.21148957299', '8.19108557079', '7.69531160115', '13.6351500118', '6.96681681682', '11.2323237697', '5.09546375267', '11.9368836649', '5.90627376426', '9.12709469154', '7.7544572697', '7.58599675412', '7.43725207583', '9.8798900768', '9.30319388778', '9.21365050557', '5.18177205308', '8.55507774799', '5.78126590331', '8.0710230118', '10.1250032713', '11.0196516733', '8.17666774927', '9.42171292239', '4.85870921867', '9.31378525791', '8.30018036767', '6.50688776965', '9.5852099677', '12.45572275', '8.09288312646', '8.68651837806', '8.34731752963', '9.29224055739', '9.77711182109', '29.878030039', '8.78393692369', '11.9757685161', '11.1401021385', '7.5605488194', '7.39098798637', '6.43360588592', '13.7999774485', '6.44703955254', '8.01794477751', '6.2553554256', '9.69742181905', '7.77268351232', '8.75192030725', '6.79288937945', '7.68249438202', '4.38545511613', '3.60671020408', '8.45364705882', '5.21488185976', '8.40056149733', '6.84136327817', '12.5099672623', '9.0148700565', '7.20036424796', '5.77809677419', '4.94129392971', '6.00045070423', '11.9971174753', '10.6377691184', '8.56422809829', '8.62268641471', '4.83114713217', '10.2701848998', '12.5816945607', '16.0599706745', '11.8677385892', '10.2945011338', '4.17606557377', '9.36189873418', '11.0917085427', '5.3244966443', '6.63090439276', '8.58392405063', '5.53106109325', '6.13912310287', '8.47737603306', '8.44393241167', '5.15196394076', '6.53706864564', '8.50044523598', '3.93154326923', '6.16368913858', '4.90444711538', '7.40241271087', '47.5032692308', '55.7391800938', '11.8407803201', '7.00229357798', '8.75314206706', '3.14774130328', '7.1967787944', '76.2036918138', '10.8043371086', '10.705327051', '51.8006862745', '5.88277871731', '6.68640645161', '5.83335488041', '45.5560956385', '5.17260575296', '10.118018018', '51.6755374204', '2.79463149728', '7.61169779853', '15.6976512739', '50.2758932156'], '2015_q1_sales': ['337166.53', '22351.86', '277764.46', '16805.11', '54411.42', '255939.81', '319020.69', '45340.33', '57849.23', '51031.04', '68657.91', '4151.93', '121837.56', '146725.31', '119287.76', '20335.03', '110552.94', '204237.78', '16939.15', '55498.12', '15983.98', '71758.66', '62988.17', '29637.75', '48759.71', '89736.7', '5577.61', '310035.66', '310718.21', '258284.84', '150049.71', '309568.16', '66468.32', '321983.24', '115531.13', '926706.76', '157981.2', '51078.36', '272703.74', '180760.93', '72185.66', '263161.4', '164809.62', '60986.22', '210690.45', '139068.33', '209403.19', '75110.03', '123291.57', '157591.18', '163684.93', '146850.63', '143950.01', '136167.32', '327172.32', '105869.94', '176266.68', '176720.07', '44836.7', '65830.25', '228680.53', '73810.68', '54655.98', '156056.28', '80923.05', '167606.75', '25575.88', '211902.95', '312502.67', '85217.68', '89525.66', '176252.03', '68258.49', '88008.31', '67219.54', '100722.77', '147242.83', '227702.34', '286763.37', '59531.82', '122811.86', '156513.07', '55538.11', '143498.37', '84998.58', '138010.02', '146424.29', '175475.49', '247716.78', '130512.99', '52054.1', '332692.67', '135418.28', '163104.8', '45612.45', '56995.82', '39867.38', '133081.37', '108362.38', '271021.94', '134589.33', '216171.01', '104354.49', '105046.24', '13677.13', '89053.08', '51427.05', '264408.55', '294990.21', '358098.75', '68077.3', '313345.86', '163324.44', '203099.8', '104044.33', '202835.96', '192934.41', '447305.5', '205170.43', '201282.29', '36855.39', '381036.91', '46509.65', '2337324.42', '208389.88', '145742.16', '187407.93', '62335.59', '138878.63', '169131.81', '280506.28', '223641.14', '361865.13', '134325.62', '145361.36', '185580.5', '93901.53', '68438.51', '32698.19', '22829.66', '13141.2', '8738.45', '48441.46', '87115.74', '102794.01', '306947.29', '225615.38', '327334.28', '27114.29', '7539.14', '20378.73', '501953.12', '66097.53', '89327.88', '20534.72', '5742.23', '15987.52', '13518.07', '17574.68', '24357.22', '29611.32', '18571.65', '18712.28', '8458.2', '19089.74', '7305.46', '14796.7', '8200.55', '11412.54', '11228.97', '17232.45', '21340.78', '16617.85', '21711.71', '28128.95', '21803.2', '12517.29', '53988.92', '555707.4', '1082136.01', '192089.46', '5574.99', '80241.27', '27043.54', '61977.54', '977772.62', '41905.18', '87839.45', '445058.32', '127495.18', '23874.67', '21535.87', '276096.18', '8506.79', '49163.01', '434110.57', '73789.38', '88006.84', '117958.96', '407738.79'], '2016_q1_sales': ['337804.05', '21736.63', '306942.27', '9307.75', '58939.9', '332979.03', '302592.88', '55315.23', '42398.57', '56241.57', '3536.14', '137416.93', '158476.55', '125731.51', '21834.49', '39609.55', '204038.87', '15689.8', '48545.69', '16320.74', '53182.55', '30402.43', '47217.4', '6367.76', '329252.09', '332976.05', '234926.02', '151934.45', '314068.92', '61847.52', '203644.27', '123632.78', '1050711.75', '149791.31', '61560.7', '260966.41', '160278.07', '78108.56', '253886.88', '173395.13', '91524.76', '247802.36', '115591.86', '233350.35', '84940.69', '115993.15', '125207.35', '161300.14', '164939.85', '147632.84', '168603.49', '342772.02', '102829.3', '216415.08', '174492.82', '41213.34', '91213.98', '256707.1', '75866.02', '50434.54', '141712.31', '91203.94', '156730.02', '24206.67', '218335.21', '325127.17', '94229.41', '117784.92', '166847.54', '73117.79', '100545.74', '68966.63', '96059.65', '211600.93', '234774.87', '268226.7', '58122.85', '106662.37', '147781.41', '67514.95', '170466.11', '94158.15', '139676.38', '167692.5', '180371.58', '298889.32', '133868.58', '39138.81', '343447.68', '191091.14', '184729.87', '37179.58', '40160.26', '40970.96', '135871.39', '92843.04', '295483.47', '130377.49', '192747.83', '103958.63', '106452.13', '12380.85', '94010.77', '51773.27', '242057.36', '392226.28', '369424.59', '73141.35', '292690.86', '172189.31', '211032.4', '171119.44', '201433.01', '200599.95', '476439.16', '215106.77', '204808.54', '37858.67', '382790.84', '58332.41', '2443253.18', '209066.17', '137697.56', '129489.89', '72810.95', '158368.58', '153305.98', '289199.43', '241311.79', '343060.63', '128500.31', '79554.12', '191155.87', '111207.97', '83399.61', '25290.33', '20962.08', '15307.3', '10464.02', '52246.46', '75566.37', '103585.1', '276396.86', '305851.47', '260453.94', '30441.76', '10564.6', '19357.49', '613414.14', '63793.71', '78706.47', '22675.76', '4566.74', '14050.14', '11883.3', '11597.18', '23801.1', '22824.14', '18957.2', '19832.94', '4258.14', '17116.11', '7499.47', '11367.4', '8041.09', '11966.94', '12707.72', '22130.31', '20817.32', '16786.98', '19203.82', '25302.92', '18650.96', '14626.29', '46323.73', '535381.86', '812011.78', '206247.57', '6402.78', '100765.67', '20098.61', '68230.36', '596063.0', '87471.43', '431990.7', '136717.57', '30008.81', '19420.34', '274752.6', '10315.35', '45252.42', '433832.28', '74215.43', '78137.29', '83915.72', '372240.72', '148021.9', '31184.18', '33324.36', '32079.13'], '2015_margin': ['93.8022814583', '21.0824246877', '93.6124943024', '16.8247038328', '35.0114570034', '31.8774372328', '76.5189730216', '19.3373453608', '26.142470349', '22.4270237673', '41.892132964', '66.4030492424', '47.8693242069', '86.7601504011', '22.9481762576', '18.7060545353', '44.0411766297', '62.1990040107', '14.2518095238', '16.0432686391', '25.1911714286', '31.7530658307', '23.1614514016', '48.8207407407', '73.2315044248', '23.4503335716', '14.1447926267', '36.4085284899', '36.1718619066', '59.8934779211', '37.1085548647', '52.4055916932', '30.681099171', '48.1333683392', '47.7406803594', '97.2243398739', '31.2923926822', '35.27017991', '31.9091555963', '29.1482532051', '32.6235916667', '47.9893704508', '55.5221865049', '31.941637952', '49.3420628537', '42.8693852124', '53.1849073341', '25.4050062762', '43.9390962494', '44.5348318408', '39.530065189', '31.5106033203', '50.1331972789', '28.7115801384', '52.4235630748', '24.0028010033', '47.3184344342', '49.1944300868', '19.9170827068', '32.71362436', '36.2773863187', '35.2223614865', '40.0093030623', '45.5890982815', '28.2592197712', '55.1925378788', '29.106349454', '39.1116473887', '46.7565544652', '57.2886237318', '46.2992604502', '25.5950192707', '36.7157591463', '20.0006463242', '29.0692439863', '22.7097016091', '58.9404393782', '25.7666753731', '49.1620319871', '33.9956304868', '24.4766550941', '52.3184580805', '30.5095964967', '36.2843302577', '30.1712350678', '41.0171194552', '20.0513661569', '45.5986531395', '35.8870443261', '37.9469113386', '29.3906663471', '64.7439062131', '35.0125088725', '52.2156089683', '22.4304264392', '49.1816335142', '25.0323193916', '45.8722118604', '34.7604217536', '39.9868794518', '34.6033448222', '41.6839918687', '40.8443061122', '44.7063598652', '21.1562997658', '41.547077748', '24.6607061069', '40.1006629307', '50.3450808487', '51.4318879891', '32.3875251705', '53.4813935145', '24.5552275389', '43.8089558984', '43.4441935484', '33.4178523235', '44.3454176281', '65.5244131168', '39.7529703822', '46.2175087515', '32.7325389894', '49.786953612', '44.9972396166', '168.245861698', '49.313768542', '56.569739846', '47.8089690393', '30.9038417358', '37.2649829658', '35.850617415', '68.1836227448', '34.2461556133', '46.0017090264', '26.9316699155', '48.3078228694', '36.216199621', '37.1569323092', '33.5548344371', '33.223011236', '20.8197112367', '15.8903673469', '35.4949411765', '26.0287842988', '37.0314331551', '29.1820981087', '72.4642572637', '52.1879774011', '40.9748793849', '23.8230645161', '24.1203833866', '26.6531361502', '65.067257829', '51.4916463712', '36.8113087607', '35.087834525', '17.9033416459', '38.2349306626', '41.9222384937', '50.8491202346', '36.282033195', '39.7768253968', '16.4770435274', '35.4246202532', '48.8443718593', '20.2525251678', '24.2994315245', '34.2229535865', '25.2890353698', '26.0607419899', '33.6023140496', '37.4071121352', '20.6980424984', '22.8882189239', '27.5077292965', '21.8116586538', '25.4285205993', '19.5683173077', '26.9353354257', '235.730677515', '268.869600245', '56.4333884407', '26.4409174312', '45.2938472174', '15.5945516903', '27.2886517761', '367.225653291', '42.0311992005', '45.8914772727', '255.153235294', '27.5122713672', '26.1030967742', '23.7416031028', '218.008349974', '21.5487817259', '46.7007087087', '233.533188694', '11.9961176992', '38.8099733155', '105.035207006', '225.055138499']}","tags":"how_to","loc":"http://www.instantinate.com/how_to/dictionary_comprehensions.html","title":"How To: Write Dictionary Comprehensions"},{"url":"http://www.instantinate.com/how_to/load_a_dataset_from_a_url.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. We'll bring in data with urllib and then load it into our dataset as a csv. import urllib import pandas as pd 1. Create a variable called \"data_url\" and paste in the URL as a string data_url = \"http://instantinate.com/data/sales_data.html\" 2. Call the urllib function with the urlretrieve method The first argument we pass is the variable with the link to the dataset. And the second argument is the name of the dataset. urllib . urlretrieve ( data_url , \"sales.data\" ) 3. Create the \"data\" variable and use the pandas.read_csv method Pass the 'sales.data' argument to the read_csv method. Specify the delimiter for the dataset which in this case is spaces. data = pd . read_csv ( 'sales.data' , delimiter = '\\s+' ) 4. Create a dataframe using the the .dataframe method Pass the data variable which read all the information from the url. df = pd . DataFrame ( data ) 5. Print out the .head( ) to confirm the dataset was loaded correctly. df . head () Here is the full code. data_url = \"http://instantinate.com/data/sales_data.html\" urllib . urlretrieve ( data_url , \"sales.data\" ) data = pd . read_csv ( 'sales.data' , delimiter = '\\s+' ) df = pd . DataFrame ( data ) df . head () volume_sold 2015_margin 2015_q1_sales 2016_q1_sales 0 18.420760 93.802281 337166.53 337804.05 1 4.776510 21.082425 22351.86 21736.63 2 16.602401 93.612494 277764.46 306942.27 3 4.296111 16.824704 16805.11 9307.75 4 8.156023 35.011457 54411.42 58939.90","tags":"how_to","loc":"http://www.instantinate.com/how_to/load_a_dataset_from_a_url.html","title":"How To: Load A Dataset from A URL"},{"url":"http://www.instantinate.com/python/load_dataset_from_url.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. We'll bring in data with urllib and then load it into our dataset as a csv. import urllib import pandas as pd 1. Create a variable called \"data_url\" and paste in the URL as a string data_url = \"http://instantinate.com/data/sales_data.html\" 2. Call the urllib function with the urlretrieve method The first argument we pass is the variable with the link to the dataset. And the second argument is the name of the dataset. urllib . urlretrieve ( data_url , \"sales.data\" ) 3. Create the \"data\" variable and use the pandas.read_csv method Pass the 'sales.data' argument to the read_csv method. Specify the delimiter for the dataset which in this case is spaces. data = pd . read_csv ( 'sales.data' , delimiter = '\\s+' ) 4. Create a dataframe using the the .dataframe method Pass the data variable which read all the information from the url. df = pd . DataFrame ( data ) 5. Print out the .head( ) to confirm the dataset was loaded correctly. df . head () Here is the full code. data_url = \"http://instantinate.com/data/sales_data.html\" urllib . urlretrieve ( data_url , \"sales.data\" ) data = pd . read_csv ( 'sales.data' , delimiter = '\\s+' ) df = pd . DataFrame ( data ) df . head () volume_sold 2015_margin 2015_q1_sales 2016_q1_sales 0 18.420760 93.802281 337166.53 337804.05 1 4.776510 21.082425 22351.86 21736.63 2 16.602401 93.612494 277764.46 306942.27 3 4.296111 16.824704 16805.11 9307.75 4 8.156023 35.011457 54411.42 58939.90","tags":"python","loc":"http://www.instantinate.com/python/load_dataset_from_url.html","title":"Load Dataset from URL"},{"url":"http://www.instantinate.com/python/a_basic_python_function.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. What do dictionaries and baking cakes have in common? Read below to find out. First we'll load in some data below. This is a list of dictionaries that contains information about different movies. #Loading in a movie ratings data set. movies = [ { \"name\" : \"Usual Suspects\" , \"imdb\" : 7.0 , \"category\" : \"Thriller\" }, { \"name\" : \"Hitman\" , \"imdb\" : 6.3 , \"category\" : \"Action\" }, { \"name\" : \"Dark Knight\" , \"imdb\" : 9.0 , \"category\" : \"Adventure\" }, { \"name\" : \"The Help\" , \"imdb\" : 8.0 , \"category\" : \"Drama\" }, { \"name\" : \"The Choice\" , \"imdb\" : 6.2 , \"category\" : \"Romance\" }, { \"name\" : \"Colonia\" , \"imdb\" : 7.4 , \"category\" : \"Romance\" }, { \"name\" : \"Love\" , \"imdb\" : 6.0 , \"category\" : \"Romance\" }, { \"name\" : \"Bride Wars\" , \"imdb\" : 5.4 , \"category\" : \"Romance\" }, { \"name\" : \"AlphaJet\" , \"imdb\" : 3.2 , \"category\" : \"War\" }, { \"name\" : \"Ringing Crime\" , \"imdb\" : 4.0 , \"category\" : \"Crime\" }, { \"name\" : \"Joking muck\" , \"imdb\" : 7.2 , \"category\" : \"Comedy\" }, { \"name\" : \"What is the name\" , \"imdb\" : 9.2 , \"category\" : \"Suspense\" }, { \"name\" : \"Detective\" , \"imdb\" : 7.0 , \"category\" : \"Suspense\" }, { \"name\" : \"Exam\" , \"imdb\" : 4.2 , \"category\" : \"Thriller\" }, { \"name\" : \"We Two\" , \"imdb\" : 7.2 , \"category\" : \"Romance\" } ] It starts with a def Every function needs a definition. Think of it like a normal dictionary. If you were to go look up a certain word you would need to know the first few letters to find it. But there also would only be one possible definition for that word in the dictionary. Python functions are the same in this regard, they need a unique name so that you can find it later. We're going to definie our function below: def genre_avg ( cat ): Wait a minute?! What's that \"cat\" thing in there?!?! (Your ingredients) That is a scary thing called a \"parameter\". It is basically the ingredients to your recipe. If you want to bake a cake you put ingredients into it and out comes a delicious cake. For parameters you \"pass\" these ingredients and then tell the function what to do with them and hopefully [crossesfingers] we get cake. Create an empty list. (Get out a mixing bowl) The genre_avg list will hold all the results of our function. def genre_avg ( cat ): genre_avg = [] For Loop or not to For Loop (Measure the ingredients) We have the ingredients and the mixing bowl... so now we need to measure the ingredients. The for loop in this function goes through each statement in the dictionary above and measures it against the \"parameter\" we passed it. In this case we're interested only in movies that fall in the Action genre. So we measure out all the action movies from our dataset and then what? Can you see what happens next? def genre_avg ( cat ): genre_avg = [] for x in movies : if cat == x [ \"category\" ]: Append (Mix the ingredients) This takes all the ingredients (parameters) that you just measured and mixes them together. And where do you mix things? In the mixing bowl of course! So now all our ingredients have been measured and are back in our mixing bowl all neat and unbaked... hmm... I wonder what the next step is. def genre_avg ( cat ): genre_avg = [] for x in movies : if cat == x [ \"category\" ]: genre_avg . append ( x [ 'imdb' ]) Return (Bake it!!!!) Now we bake all those nice ingredients to get the most perfect cake we ever could ask for. In this case, a perfect average of score of a particular genre in our movie dataset. def genre_avg ( cat ): genre_avg = [] for x in movies : if cat == x [ \"category\" ]: genre_avg . append ( x [ 'imdb' ]) return sum ( genre_avg ) / len ( genre_avg ) Now for the magic. This is not just any cake baking exercise. This is a magical cake machine that can give you any kind of cake you want so long as you give it the ingredients with the right \"parameter\". That's what is going on in the last statement after the function is defined. We reference that function and give it some ingredients and instantly we have some magic cake! How about that. genre_avg ( \"Action\" ) Here's the whole code. def genre_avg ( cat ): genre_avg = [] for x in movies : if cat == x [ \"category\" ]: genre_avg . append ( x [ 'imdb' ]) return sum ( genre_avg ) / len ( genre_avg ) genre_avg ( \"Action\" ) 6.3","tags":"python","loc":"http://www.instantinate.com/python/a_basic_python_function.html","title":"A Basic Python Function"},{"url":"http://www.instantinate.com/articles/billboard_analysis.html","text":"Here is some open ended analysis using pandas to show the power of this library for handling literally anything that can be thrown at it. import pandas as pd import numpy as np import matplotlib.pyplot as plt % matplotlib inline Pandas to read in data data = pd . read_csv ( '../data/billboard.csv' ) df = pd . DataFrame ( data ) df . head () year artist.inverted track time genre date.entered date.peaked x1st.week x2nd.week x3rd.week ... x67th.week x68th.week x69th.week x70th.week x71st.week x72nd.week x73rd.week x74th.week x75th.week x76th.week 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 2000 Santana Maria, Maria 4:18 Rock 2000-02-12 2000-04-08 15 8.0 6.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2 2000 Savage Garden I Knew I Loved You 4:07 Rock 1999-10-23 2000-01-29 71 48.0 43.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 2000 Madonna Music 3:45 Rock 2000-08-12 2000-09-16 41 23.0 18.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 2000 Aguilera, Christina Come On Over Baby (All I Want Is You) 3:38 Rock 2000-08-05 2000-10-14 57 47.0 45.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 5 rows × 83 columns Renaming column names df . columns = [ col . replace ( '.week' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'x' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'st' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'nd' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'rd' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'th' , '' ) for col in df . columns ] #df.columns = ufo.columns.str.replace(' ', '_') Describe your data: check the value counts + descrisptive stats #We will first drop 'year' since all the songs are from 2000 df . drop ( 'year' , axis = 1 , inplace = True ) #basic describe df . describe () 1 2 3 4 5 6 7 8 9 10 ... 67 68 69 70 71 72 73 74 75 76 count 317.000000 312.000000 307.000000 300.000000 292.000000 280.000000 269.000000 260.000000 253.000000 244.000000 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 mean 79.958991 71.173077 65.045603 59.763333 56.339041 52.360714 49.219331 47.119231 46.343874 45.786885 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN std 14.686865 18.200443 20.752302 22.324619 23.780022 24.473273 25.654279 26.370782 27.136419 28.152357 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN min 15.000000 8.000000 6.000000 5.000000 2.000000 1.000000 1.000000 1.000000 1.000000 1.000000 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 25% 74.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 50% 81.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 75% 91.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN max 100.000000 100.000000 100.000000 100.000000 100.000000 99.000000 100.000000 99.000000 100.000000 100.000000 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 8 rows × 76 columns df2 = pd . melt ( df , id_vars = [ 'arti.inverted' , 'track' , 'time' , 'genre' , 'date.entered' , 'date.peaked' ], var_name = 'week' , value_name = 'rank' ) #checking function that was run print df2 . shape print df2 . columns (24092, 8) Index([u'arti.inverted', u'track', u'time', u'genre', u'date.entered', u'date.peaked', u'week', u'rank'], dtype='object') #We now the data formated to plot ranking over time for each track. #We will use other methods to determine which tracks to plot. df3 = df2 [ df2 [ 'track' ] == 'I Wanna Know' ] plt . plot ( df3 [ 'rank' ]) plt . ylabel ( 'Weekly Rank' ) plt . axis ([ 0 , 18000 , 100 , 0 ]) plt . show () Future Exploratory options... the world is yours with pandas. Look at time it takes to get to the top. (time entered, to time peak) Also add a column called time in top 100. Compare the three columns. That was interesting... now lets wrangle this dataset for some cool correlations. Data wrangling for the most correlated genres #create a new dataframe for manipulation. df7 = df2 Group the data df7 = df7 . groupby ([ 'week' , 'genre' ], as_index = False ) . mean () #check that the group by function worked. df7 . head () week genre rank 0 1 Country 82.405405 1 1 Electronica 84.500000 2 1 Gospel 76.000000 3 1 Jazz 89.000000 4 1 Latin 73.222222 Create a pivot table #Use the pivot table function to get to something that can be correlated. df7 = df7 . pivot ( index = 'week' , columns = 'genre' , values = 'rank' ) #Moving the week column from the index back into a column position on the data table. df7 . reset_index ( inplace = True ) df7 . head ( 5 ) genre week Country Electronica Gospel Jazz Latin Pop R&B Rap Reggae Rock 0 1 82.405405 84.500000 76.0 89.0 73.222222 79.222222 84.086957 85.172414 72.0 76.116788 1 10 52.377049 55.750000 59.0 NaN 43.250000 43.571429 63.866667 53.380952 75.0 35.895238 2 11 51.016949 53.250000 66.0 NaN 49.625000 50.142857 62.538462 52.538462 84.0 36.048077 3 12 50.714286 59.750000 68.0 NaN 35.285714 58.250000 67.000000 50.000000 92.0 33.734694 4 13 52.301887 49.333333 61.0 NaN 39.285714 58.333333 59.666667 53.235294 85.0 34.125000 #Sorting the data inside week after it is converted to a string. df7 [ 'week' ] = df7 . week . astype ( int ) df7 . sort ( 'week' ) df7 . head () /anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:3: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....) app.launch_new_instance() genre week Country Electronica Gospel Jazz Latin Pop R&B Rap Reggae Rock 0 1 82.405405 84.500000 76.0 89.0 73.222222 79.222222 84.086957 85.172414 72.0 76.116788 1 10 52.377049 55.750000 59.0 NaN 43.250000 43.571429 63.866667 53.380952 75.0 35.895238 2 11 51.016949 53.250000 66.0 NaN 49.625000 50.142857 62.538462 52.538462 84.0 36.048077 3 12 50.714286 59.750000 68.0 NaN 35.285714 58.250000 67.000000 50.000000 92.0 33.734694 4 13 52.301887 49.333333 61.0 NaN 39.285714 58.333333 59.666667 53.235294 85.0 34.125000 #Remove the pesky column and index names that will mess up the correlation formula later. df7 . index . name = None df7 . columns . name = None #Remove the week column since we sorted by it already. df7 . drop ( 'week' , inplace = True , axis = 1 ) df7 . head () Country Electronica Gospel Jazz Latin Pop R&B Rap Reggae Rock 0 82.405405 84.500000 76.0 89.0 73.222222 79.222222 84.086957 85.172414 72.0 76.116788 1 52.377049 55.750000 59.0 NaN 43.250000 43.571429 63.866667 53.380952 75.0 35.895238 2 51.016949 53.250000 66.0 NaN 49.625000 50.142857 62.538462 52.538462 84.0 36.048077 3 50.714286 59.750000 68.0 NaN 35.285714 58.250000 67.000000 50.000000 92.0 33.734694 4 52.301887 49.333333 61.0 NaN 39.285714 58.333333 59.666667 53.235294 85.0 34.125000 Data wrangling for the most correlated artist rankings df8 = df2 Group the data df8 = df8 . groupby ([ 'week' , 'arti.inverted' ], as_index = False ) . mean () df8 . shape (17328, 3) df8 = df8 [ np . isfinite ( df8 [ 'rank' ])] df8 . shape (3989, 3) counts = df8 [ 'arti.inverted' ] . value_counts () ##Removing any artists that have less than 15 datapoints on the rankings. df8 = df8 [ df8 [ 'arti.inverted' ] . isin ( counts [ counts > 30 ] . index )] Create a pivot table df8 = df8 . pivot ( index = 'week' , columns = 'arti.inverted' , values = 'rank' ) #Moving the week column from the index back into a column position on the data table. df8 . reset_index ( inplace = True ) #Sorting the data inside week after it is converted to a string. df8 [ 'week' ] = df8 . week . astype ( int ) df8 . sort ( 'week' ) df8 . head () /anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:3: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....) app.launch_new_instance() arti.inverted week 3 Doors Down Aaliyah Anthony, Marc BBMak Braxton, Toni Creed Destiny's Child Hill, Faith Joe Jordan, Montell Lonestar Nelly Pink Savage Garden Vertical Horizon matchbox twenty 0 1 78.5 71.5 79.5 99.0 79.0 82.5 78.333333 82.0 85.5 92.0 82.666667 100.0 55.0 73.0 67.0 60.0 1 10 56.0 23.0 44.5 18.0 21.0 65.5 12.333333 49.0 53.0 24.0 33.333333 36.0 11.5 16.5 22.5 12.0 2 11 56.0 23.0 43.0 19.0 27.0 63.5 10.333333 42.0 52.5 24.0 33.000000 37.0 10.0 19.0 20.0 8.0 3 12 55.0 22.5 43.5 15.0 29.0 68.0 7.666667 46.0 54.5 20.0 31.666667 30.0 11.0 22.5 19.0 6.0 4 13 54.0 22.0 50.0 18.0 32.0 70.5 2.666667 55.5 54.0 19.0 35.000000 23.0 12.0 25.0 19.0 1.0 #Remove the pesky column and index names that will mess up the correlation formula later. df8 . index . name = None df8 . columns . name = None #Remove the week column since we sorted by it already. df8 . drop ( 'week' , inplace = True , axis = 1 ) Data wrangling for the most correlated song rankings How would you do this part? #df9 = df2 Run a correlation function on the dataframes from the wrangling steps. This is a simple formula that I have been working to improve to work on any data set. It is designed to be a useful alternative to the spray and pray sns.pairplot or scatter matrix methods. sns.pairplot (on df7) = 14s corrr_pairs function (on df7) =384ms def corr_pairs ( df_input , coef_percentile ): #,mse_percentile #from sklearn.metrics import mean_squared_error #Get top correlated pairs using Pearson coefficient c = df_input . corr () s = c . unstack () so = s . sort_values ( kind = \"quicksort\" ) df_output = pd . DataFrame ( so . abs (), columns = [ 'coef' ]) df_output = df_output . reset_index () df_output . drop_duplicates ( 'coef' , inplace = True ) df_output . dropna ( inplace = True ) #df_input = df_input.fillna(0.0) #Get mean squared error for better accuracy #mse_l = [] #for i in range(len(df_output.iloc[:,0:2])): #mse_var = mean_squared_error(df_input[df_output.iloc[i,0]], df_input[df_output.iloc[i,1]]) #mse_l.append(mse_var) #df_output['mse'] = mse_l #Filter the results by both Coefficient and MSE for best pairs. df_output = df_output [( df_output [ 'coef' ] < 1 ) & ( df_output . coef > np . percentile ( df_output [ 'coef' ], coef_percentile ))] #& (df_output.mse < np.percentile(df_output['mse'],mse_percentile))] #Plot the best pairs. for i in range ( len ( df_output . iloc [:, 0 : 2 ])): colors = [ 'r' , 'b' ] plt . scatter ( df_output . iloc [ i , 0 ], df_output . iloc [ i , 1 ], data = df_input , c = colors ) plt . xlabel ( df_output . iloc [ i , 0 ]) plt . ylabel ( df_output . iloc [ i , 1 ]) plt . legend () plt . show () return df_output Showing the most correlated genres in the rankings corr_pairs ( df7 , 95 ) level_0 level_1 coef 86 Electronica Country 0.848179 88 Electronica R&B 0.913870 Showing the most correlated artists in the rankings corr_pairs ( df8 , 95 ) level_0 level_1 coef 230 Creed Joe 0.909992 232 Savage Garden Aaliyah 0.949246 234 BBMak matchbox twenty 0.954013 236 matchbox twenty Destiny's Child 0.965130 238 Destiny's Child BBMak 0.982112 Showing the most correlated songs in the rankings How would you do this part?","tags":"Articles","loc":"http://www.instantinate.com/articles/billboard_analysis.html","title":"Billboard Hot 100 Analysis"},{"url":"http://www.instantinate.com/how_to/build_a_basic_python_function.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. What do dictionaries and baking cakes have in common? Read below to find out. First we'll load in some data below. This is a list of dictionaries that contains information about different movies. #Loading in a movie ratings data set. movies = [ { \"name\" : \"Usual Suspects\" , \"imdb\" : 7.0 , \"category\" : \"Thriller\" }, { \"name\" : \"Hitman\" , \"imdb\" : 6.3 , \"category\" : \"Action\" }, { \"name\" : \"Dark Knight\" , \"imdb\" : 9.0 , \"category\" : \"Adventure\" }, { \"name\" : \"The Help\" , \"imdb\" : 8.0 , \"category\" : \"Drama\" }, { \"name\" : \"The Choice\" , \"imdb\" : 6.2 , \"category\" : \"Romance\" }, { \"name\" : \"Colonia\" , \"imdb\" : 7.4 , \"category\" : \"Romance\" }, { \"name\" : \"Love\" , \"imdb\" : 6.0 , \"category\" : \"Romance\" }, { \"name\" : \"Bride Wars\" , \"imdb\" : 5.4 , \"category\" : \"Romance\" }, { \"name\" : \"AlphaJet\" , \"imdb\" : 3.2 , \"category\" : \"War\" }, { \"name\" : \"Ringing Crime\" , \"imdb\" : 4.0 , \"category\" : \"Crime\" }, { \"name\" : \"Joking muck\" , \"imdb\" : 7.2 , \"category\" : \"Comedy\" }, { \"name\" : \"What is the name\" , \"imdb\" : 9.2 , \"category\" : \"Suspense\" }, { \"name\" : \"Detective\" , \"imdb\" : 7.0 , \"category\" : \"Suspense\" }, { \"name\" : \"Exam\" , \"imdb\" : 4.2 , \"category\" : \"Thriller\" }, { \"name\" : \"We Two\" , \"imdb\" : 7.2 , \"category\" : \"Romance\" } ] It starts with a def Every function needs a definition. Think of it like a normal dictionary. If you were to go look up a certain word you would need to know the first few letters to find it. But there also would only be one possible definition for that word in the dictionary. Python functions are the same in this regard, they need a unique name so that you can find it later. We're going to definie our function below: def genre_avg ( cat ): Wait a minute?! What's that \"cat\" thing in there?!?! (Your ingredients) That is a scary thing called a \"parameter\". It is basically the ingredients to your recipe. If you want to bake a cake you put ingredients into it and out comes a delicious cake. For parameters you \"pass\" these ingredients and then tell the function what to do with them and hopefully [crossesfingers] we get cake. Create an empty list. (Get out a mixing bowl) The genre_avg list will hold all the results of our function. def genre_avg ( cat ): genre_avg = [] For Loop or not to For Loop (Measure the ingredients) We have the ingredients and the mixing bowl... so now we need to measure the ingredients. The for loop in this function goes through each statement in the dictionary above and measures it against the \"parameter\" we passed it. In this case we're interested only in movies that fall in the Action genre. So we measure out all the action movies from our dataset and then what? Can you see what happens next? def genre_avg ( cat ): genre_avg = [] for x in movies : if cat == x [ \"category\" ]: Append (Mix the ingredients) This takes all the ingredients (parameters) that you just measured and mixes them together. And where do you mix things? In the mixing bowl of course! So now all our ingredients have been measured and are back in our mixing bowl all neat and unbaked... hmm... I wonder what the next step is. def genre_avg ( cat ): genre_avg = [] for x in movies : if cat == x [ \"category\" ]: genre_avg . append ( x [ 'imdb' ]) Return (Bake it!!!!) Now we bake all those nice ingredients to get the most perfect cake we ever could ask for. In this case, a perfect average of score of a particular genre in our movie dataset. def genre_avg ( cat ): genre_avg = [] for x in movies : if cat == x [ \"category\" ]: genre_avg . append ( x [ 'imdb' ]) return sum ( genre_avg ) / len ( genre_avg ) Now for the magic. This is not just any cake baking exercise. This is a magical cake machine that can give you any kind of cake you want so long as you give it the ingredients with the right \"parameter\". That's what is going on in the last statement after the function is defined. We reference that function and give it some ingredients and instantly we have some magic cake! How about that. genre_avg ( \"Action\" ) Here's the whole code. def genre_avg ( cat ): genre_avg = [] for x in movies : if cat == x [ \"category\" ]: genre_avg . append ( x [ 'imdb' ]) return sum ( genre_avg ) / len ( genre_avg ) genre_avg ( \"Action\" ) 6.3","tags":"how_to","loc":"http://www.instantinate.com/how_to/build_a_basic_python_function.html","title":"How To: Build A Basic Python Function"},{"url":"http://www.instantinate.com/python/load_dataset_with_csv_reader.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. Quick reference for using built in python functionality to import a csv dataset. import csv Brings in the csv library that is core to python. with open('../data/sales/data.csv', 'rU') as f: Declare open to be used. Specify the file location. Select a dialect to use. Assign the results to the variable f. data = [ ] Create an empty list to receive the data from the f variable. reader = csv.reader(f) Create a reader variable for the csv reader to be applied to the f variable. for row in reader: Create a for loop to iterate over each row of the f variable(csv) using the reader variable. data.append(row) Append rows to the data list as they are read with the reader variable print data Print your data set as a list of lists with the first list being the header row of the csv file. import csv with open ( '../data/sales_data.csv' , 'rU' ) as f : data = [] reader = csv . reader ( f ) for row in reader : data . append ( row ) print data #f.close() -- each with statement has a close statement built into the block so this is not needed. [['volume_sold', '2015_margin', '2015_q1_sales', '2016_q1_sales'], ['18.4207604861', '93.8022814583', '337166.53', '337804.05'], ['4.77650991918', '21.0824246877', '22351.86', '21736.63'], ['16.6024006077', '93.6124943024', '277764.46', '306942.27'], ['4.29611149826', '16.8247038328', '16805.11', '9307.75'], ['8.15602328201', '35.0114570034', '54411.42', '58939.9'], ['5.00512242518', '31.8774372328', '255939.81', '332979.03'], ['14.60675', '76.5189730216', '319020.69', '302592.88'], ['4.45646649485', '19.3373453608', '45340.33', '55315.23'], ['5.04752965097', '26.142470349', '57849.23', '42398.57'], ['5.38807023767', '22.4270237673', '51031.04', '56241.57'], ['9.34734863474', '41.892132964', '68657.91', '3536.14'], ['10.9303977273', '66.4030492424', '4151.93', '137416.93'], ['6.27020860495', '47.8693242069', '121837.56', '158476.55'], ['12.3959191176', '86.7601504011', '146725.31', '125731.51'], ['4.55771189614', '22.9481762576', '119287.76', '21834.49'], ['4.20012242627', '18.7060545353', '20335.03', '39609.55'], ['10.2528698945', '44.0411766297', '110552.94', '204038.87'], ['12.0767847594', '62.1990040107', '204237.78', '15689.8'], ['3.7250952381', '14.2518095238', '16939.15', '48545.69'], ['3.21072662722', '16.0432686391', '55498.12', '16320.74'], ['6.29097142857', '25.1911714286', '15983.98', '53182.55'], ['7.43482131661', '31.7530658307', '71758.66', '30402.43'], ['4.37622478386', '23.1614514016', '62988.17', '47217.4'], ['12.9889127838', '48.8207407407', '29637.75', '6367.76'], ['11.6974557522', '73.2315044248', '48759.71', '329252.09'], ['5.96517512509', '23.4503335716', '89736.7', '332976.05'], ['3.94522273425', '14.1447926267', '5577.61', '234926.02'], ['7.36958530901', '36.4085284899', '310035.66', '151934.45'], ['7.34350882699', '36.1718619066', '310718.21', '314068.92'], ['12.3500273544', '59.8934779211', '258284.84', '61847.52'], ['8.41791967737', '37.1085548647', '150049.71', '203644.27'], ['10.2608361718', '52.4055916932', '309568.16', '123632.78'], ['7.82435369972', '30.681099171', '66468.32', '1050711.75'], ['10.3314300532', '48.1333683392', '321983.24', '149791.31'], ['12.5284878049', '47.7406803594', '115531.13', '61560.7'], ['18.7447505256', '97.2243398739', '926706.76', '260966.41'], ['6.65773264189', '31.2923926822', '157981.2', '160278.07'], ['10.6321289355', '35.27017991', '51078.36', '78108.56'], ['6.92770422965', '31.9091555963', '272703.74', '253886.88'], ['6.61817422161', '29.1482532051', '180760.93', '173395.13'], ['7.12444444444', '32.6235916667', '72185.66', '91524.76'], ['9.84966032435', '47.9893704508', '263161.4', '247802.36'], ['11.5058377559', '55.5221865049', '164809.62', '115591.86'], ['6.30981315215', '31.941637952', '60986.22', '233350.35'], ['10.1866219839', '49.3420628537', '210690.45', '84940.69'], ['10.1221793301', '42.8693852124', '139068.33', '115993.15'], ['10.8003469032', '53.1849073341', '209403.19', '125207.35'], ['7.26782845188', '25.4050062762', '75110.03', '161300.14'], ['10.6737166742', '43.9390962494', '123291.57', '164939.85'], ['9.15026865672', '44.5348318408', '157591.18', '147632.84'], ['8.12418187744', '39.530065189', '163684.93', '168603.49'], ['6.27579970306', '31.5106033203', '146850.63', '342772.02'], ['10.6772953319', '50.1331972789', '143950.01', '102829.3'], ['5.88898828541', '28.7115801384', '136167.32', '216415.08'], ['10.6401714545', '52.4235630748', '327172.32', '174492.82'], ['4.75559643255', '24.0028010033', '105869.94', '41213.34'], ['10.246884068', '47.3184344342', '176266.68', '91213.98'], ['10.29268081', '49.1944300868', '176720.07', '256707.1'], ['4.41819548872', '19.9170827068', '44836.7', '75866.02'], ['7.10134734573', '32.71362436', '65830.25', '50434.54'], ['8.00611901938', '36.2773863187', '228680.53', '141712.31'], ['7.79050337838', '35.2223614865', '73810.68', '91203.94'], ['11.1293822598', '40.0093030623', '54655.98', '156730.02'], ['9.34847653987', '45.5890982815', '156056.28', '24206.67'], ['6.31088643791', '28.2592197712', '80923.05', '218335.21'], ['11.6256060606', '55.1925378788', '167606.75', '325127.17'], ['6.65440717629', '29.106349454', '25575.88', '94229.41'], ['7.93041476808', '39.1116473887', '211902.95', '117784.92'], ['9.003562316', '46.7565544652', '312502.67', '166847.54'], ['14.4394353772', '57.2886237318', '85217.68', '73117.79'], ['11.0115404852', '46.2992604502', '89525.66', '100545.74'], ['5.72389564186', '25.5950192707', '176252.03', '68966.63'], ['7.77732012195', '36.7157591463', '68258.49', '96059.65'], ['4.75918372602', '20.0006463242', '88008.31', '211600.93'], ['7.78586691659', '29.0692439863', '67219.54', '234774.87'], ['5.03499140759', '22.7097016091', '100722.77', '268226.7'], ['11.6845098446', '58.9404393782', '147242.83', '58122.85'], ['5.14772910448', '25.7666753731', '227702.34', '106662.37'], ['10.0860303763', '49.1620319871', '286763.37', '147781.41'], ['7.94465682362', '33.9956304868', '59531.82', '67514.95'], ['5.29439978635', '24.4766550941', '122811.86', '170466.11'], ['11.8265363003', '52.3184580805', '156513.07', '94158.15'], ['6.300925868', '30.5095964967', '55538.11', '139676.38'], ['8.64269487751', '36.2843302577', '143498.37', '167692.5'], ['6.04822838631', '30.1712350678', '84998.58', '180371.58'], ['9.47492913676', '41.0171194552', '138010.02', '298889.32'], ['3.99185767098', '20.0513661569', '146424.29', '133868.58'], ['8.59207381371', '45.5986531395', '175475.49', '39138.81'], ['7.21148957299', '35.8870443261', '247716.78', '343447.68'], ['8.19108557079', '37.9469113386', '130512.99', '191091.14'], ['7.69531160115', '29.3906663471', '52054.1', '184729.87'], ['13.6351500118', '64.7439062131', '332692.67', '37179.58'], ['6.96681681682', '35.0125088725', '135418.28', '40160.26'], ['11.2323237697', '52.2156089683', '163104.8', '40970.96'], ['5.09546375267', '22.4304264392', '45612.45', '135871.39'], ['11.9368836649', '49.1816335142', '56995.82', '92843.04'], ['5.90627376426', '25.0323193916', '39867.38', '295483.47'], ['9.12709469154', '45.8722118604', '133081.37', '130377.49'], ['7.7544572697', '34.7604217536', '108362.38', '192747.83'], ['7.58599675412', '39.9868794518', '271021.94', '103958.63'], ['7.43725207583', '34.6033448222', '134589.33', '106452.13'], ['9.8798900768', '41.6839918687', '216171.01', '12380.85'], ['9.30319388778', '40.8443061122', '104354.49', '94010.77'], ['9.21365050557', '44.7063598652', '105046.24', '51773.27'], ['5.18177205308', '21.1562997658', '13677.13', '242057.36'], ['8.55507774799', '41.547077748', '89053.08', '392226.28'], ['5.78126590331', '24.6607061069', '51427.05', '369424.59'], ['8.0710230118', '40.1006629307', '264408.55', '73141.35'], ['10.1250032713', '50.3450808487', '294990.21', '292690.86'], ['11.0196516733', '51.4318879891', '358098.75', '172189.31'], ['8.17666774927', '32.3875251705', '68077.3', '211032.4'], ['9.42171292239', '53.4813935145', '313345.86', '171119.44'], ['4.85870921867', '24.5552275389', '163324.44', '201433.01'], ['9.31378525791', '43.8089558984', '203099.8', '200599.95'], ['8.30018036767', '43.4441935484', '104044.33', '476439.16'], ['6.50688776965', '33.4178523235', '202835.96', '215106.77'], ['9.5852099677', '44.3454176281', '192934.41', '204808.54'], ['12.45572275', '65.5244131168', '447305.5', '37858.67'], ['8.09288312646', '39.7529703822', '205170.43', '382790.84'], ['8.68651837806', '46.2175087515', '201282.29', '58332.41'], ['8.34731752963', '32.7325389894', '36855.39', '2443253.18'], ['9.29224055739', '49.786953612', '381036.91', '209066.17'], ['9.77711182109', '44.9972396166', '46509.65', '137697.56'], ['29.878030039', '168.245861698', '2337324.42', '129489.89'], ['8.78393692369', '49.313768542', '208389.88', '72810.95'], ['11.9757685161', '56.569739846', '145742.16', '158368.58'], ['11.1401021385', '47.8089690393', '187407.93', '153305.98'], ['7.5605488194', '30.9038417358', '62335.59', '289199.43'], ['7.39098798637', '37.2649829658', '138878.63', '241311.79'], ['6.43360588592', '35.850617415', '169131.81', '343060.63'], ['13.7999774485', '68.1836227448', '280506.28', '128500.31'], ['6.44703955254', '34.2461556133', '223641.14', '79554.12'], ['8.01794477751', '46.0017090264', '361865.13', '191155.87'], ['6.2553554256', '26.9316699155', '134325.62', '111207.97'], ['9.69742181905', '48.3078228694', '145361.36', '83399.61'], ['7.77268351232', '36.216199621', '185580.5', '25290.33'], ['8.75192030725', '37.1569323092', '93901.53', '20962.08'], ['6.79288937945', '33.5548344371', '68438.51', '15307.3'], ['7.68249438202', '33.223011236', '32698.19', '10464.02'], ['4.38545511613', '20.8197112367', '22829.66', '52246.46'], ['3.60671020408', '15.8903673469', '13141.2', '75566.37'], ['8.45364705882', '35.4949411765', '8738.45', '103585.1'], ['5.21488185976', '26.0287842988', '48441.46', '276396.86'], ['8.40056149733', '37.0314331551', '87115.74', '305851.47'], ['6.84136327817', '29.1820981087', '102794.01', '260453.94'], ['12.5099672623', '72.4642572637', '306947.29', '30441.76'], ['9.0148700565', '52.1879774011', '225615.38', '10564.6'], ['7.20036424796', '40.9748793849', '327334.28', '19357.49'], ['5.77809677419', '23.8230645161', '27114.29', '613414.14'], ['4.94129392971', '24.1203833866', '7539.14', '63793.71'], ['6.00045070423', '26.6531361502', '20378.73', '78706.47'], ['11.9971174753', '65.067257829', '501953.12', '22675.76'], ['10.6377691184', '51.4916463712', '66097.53', '4566.74'], ['8.56422809829', '36.8113087607', '89327.88', '14050.14'], ['8.62268641471', '35.087834525', '20534.72', '11883.3'], ['4.83114713217', '17.9033416459', '5742.23', '11597.18'], ['10.2701848998', '38.2349306626', '15987.52', '23801.1'], ['12.5816945607', '41.9222384937', '13518.07', '22824.14'], ['16.0599706745', '50.8491202346', '17574.68', '18957.2'], ['11.8677385892', '36.282033195', '24357.22', '19832.94'], ['10.2945011338', '39.7768253968', '29611.32', '4258.14'], ['4.17606557377', '16.4770435274', '18571.65', '17116.11'], ['9.36189873418', '35.4246202532', '18712.28', '7499.47'], ['11.0917085427', '48.8443718593', '8458.2', '11367.4'], ['5.3244966443', '20.2525251678', '19089.74', '8041.09'], ['6.63090439276', '24.2994315245', '7305.46', '11966.94'], ['8.58392405063', '34.2229535865', '14796.7', '12707.72'], ['5.53106109325', '25.2890353698', '8200.55', '22130.31'], ['6.13912310287', '26.0607419899', '11412.54', '20817.32'], ['8.47737603306', '33.6023140496', '11228.97', '16786.98'], ['8.44393241167', '37.4071121352', '17232.45', '19203.82'], ['5.15196394076', '20.6980424984', '21340.78', '25302.92'], ['6.53706864564', '22.8882189239', '16617.85', '18650.96'], ['8.50044523598', '27.5077292965', '21711.71', '14626.29'], ['3.93154326923', '21.8116586538', '28128.95', '46323.73'], ['6.16368913858', '25.4285205993', '21803.2', '535381.86'], ['4.90444711538', '19.5683173077', '12517.29', '812011.78'], ['7.40241271087', '26.9353354257', '53988.92', '206247.57'], ['47.5032692308', '235.730677515', '555707.4', '6402.78'], ['55.7391800938', '268.869600245', '1082136.01', '100765.67'], ['11.8407803201', '56.4333884407', '192089.46', '20098.61'], ['7.00229357798', '26.4409174312', '5574.99', '68230.36'], ['8.75314206706', '45.2938472174', '80241.27', '596063.0'], ['3.14774130328', '15.5945516903', '27043.54', '87471.43'], ['7.1967787944', '27.2886517761', '61977.54', '431990.7'], ['76.2036918138', '367.225653291', '977772.62', '136717.57'], ['10.8043371086', '42.0311992005', '41905.18', '30008.81'], ['10.705327051', '45.8914772727', '87839.45', '19420.34'], ['51.8006862745', '255.153235294', '445058.32', '274752.6'], ['5.88277871731', '27.5122713672', '127495.18', '10315.35'], ['6.68640645161', '26.1030967742', '23874.67', '45252.42'], ['5.83335488041', '23.7416031028', '21535.87', '433832.28'], ['45.5560956385', '218.008349974', '276096.18', '74215.43'], ['5.17260575296', '21.5487817259', '8506.79', '78137.29'], ['10.118018018', '46.7007087087', '49163.01', '83915.72'], ['51.6755374204', '233.533188694', '434110.57', '372240.72'], ['2.79463149728', '11.9961176992', '73789.38', '148021.9'], ['7.61169779853', '38.8099733155', '88006.84', '31184.18'], ['15.6976512739', '105.035207006', '117958.96', '33324.36'], ['50.2758932156', '225.055138499', '407738.79', '32079.13']] NOTE: At first I had noted that an f.close() statement was needed at the end of this function. But it was rightly pointed out to me that this is a built in functionality for the with statement code block and is therefore not needed.","tags":"python","loc":"http://www.instantinate.com/python/load_dataset_with_csv_reader.html","title":"Load Dataset with CSV.reader"}]}