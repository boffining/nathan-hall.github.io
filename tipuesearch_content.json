{"pages":[{"url":"http://www.instantinate.com/articles/analyzing_iowa_liqour_sales_project.html","text":"Scenario 2: Market research for new store locations A liquor store owner in Iowa is looking to expand to new locations and has hired you to investigate the market data for potential new locations. The business owner is interested in the details of the best model you can fit to the data so that his team can evaluate potential locations for a new storefront. Goal for Scenario #2: Your task is to: Build models of total sales based on location, price per bottle, total bottles sold. You may find it useful to build models for each county, zip code, or city. Provide a table of the best performing stores by location type of your choice (city, county, or zip code) and the predictions of your model(s). Based on your models and the table of data, recommend some general locations to the business owner, taking into account model performance. Validate your model's performance and ability to predict future sales using cross-validation. 1. Define Success Definition of \"BEST LOCATION\": The best location would consider all of the following 1. The most demand. 2. The strongest demand. 3. The most opportunity. 2. Data Wrangling: Import Data import pandas as pd import numpy as np ## Load the data into a DataFrame data = pd . read_csv ( '../../../../../project2_data/Iowa_Liquor_Sales_reduced.csv' ) df = pd . DataFrame ( data ) / Users / nathanhall / anaconda / lib / python2 . 7 / site - packages / IPython / core / interactiveshell . py : 2717 : DtypeWarning : Columns ( 3 ) have mixed types . Specify dtype option on import or set low_memory = False . interactivity = interactivity , compiler = compiler , result = result ) #See what the data looks like df . head () Date Store Number City Zip Code County Number County Category Category Name Vendor Number Item Number Item Description Bottle Volume (ml) State Bottle Cost State Bottle Retail Bottles Sold Sale (Dollars) Volume Sold (Liters) Volume Sold (Gallons) 0 03/31/2016 5029 DAVENPORT 52806 82.0 Scott 1022100.0 TEQUILA 370 87152 Avion Silver 375 $9.99 $14.99 12 $179.88 4.5 1.19 1 03/31/2016 5029 DAVENPORT 52806 82.0 Scott 1022100.0 TEQUILA 395 89197 Jose Cuervo Especial Reposado Tequila 1000 $12.50 $18.75 2 $37.50 2.0 0.53 2 03/31/2016 4959 CEDAR FALLS 50613 7.0 Black Hawk 1071100.0 AMERICAN COCKTAILS 380 63959 Uv Blue Raspberry Lemonade Pet 1750 $5.97 $8.96 6 $53.76 10.5 2.77 3 03/31/2016 2190 DES MOINES 50314 77.0 Polk 1031200.0 VODKA FLAVORED 205 40597 New Amsterdam Red Berry 200 $2.24 $3.36 48 $161.28 9.6 2.54 4 03/31/2016 5240 WEST BRANCH 52358 NaN NaN 1081200.0 CREAM LIQUEURS 305 73055 Rumchata 750 $12.50 $18.75 6 $112.50 4.5 1.19 #Size and shape of data set. df . shape (2709552, 18) #Check data types. df . dtypes #We have several object columns which can store multiple types of values in them. We'll need to keep an eye on these #for the multiple data-type warning received with importing the csv file. Date object Store Number int64 City object Zip Code object County Number float64 County object Category float64 Category Name object Vendor Number int64 Item Number int64 Item Description object Bottle Volume (ml) int64 State Bottle Cost object State Bottle Retail object Bottles Sold int64 Sale (Dollars) object Volume Sold (Liters) float64 Volume Sold (Gallons) float64 dtype: object #Check for missing values. df . isnull () . sum () Date 0 Store Number 0 City 0 Zip Code 0 County Number 10913 County 10913 Category 779 Category Name 6109 Vendor Number 0 Item Number 0 Item Description 0 Bottle Volume (ml) 0 State Bottle Cost 0 State Bottle Retail 0 Bottles Sold 0 Sale (Dollars) 0 Volume Sold (Liters) 0 Volume Sold (Gallons) 0 dtype: int64 #Check for duplicate rows. df . drop_duplicates () df . shape (2709552, 18) #We're goign to drop the following columns as they are all redundant with another column. df . drop ([ 'County Number' , 'Volume Sold (Gallons)' ], axis = 1 , inplace = True ) 2. Data Wrangling: Resolve Missing Data \"Counties\" Column REFERENCE: Make Dictionary to Fix the Missing Counties Below is the code used for fixing the data type and testing the mapping function #Put County and Zip Code into their own dataframe. df_test = df [[ 'County' , 'Zip Code' ]] #Convert df_fill to string data type (otherwise this will duplicate dictionary values for the ones that are strings and the ones that are integers) df_test = df_test . astype ( str ) #Check shape to confirm we're ok. df_test . shape I had used the below code to make a dictionary off these columns. Then I filled in the dozen remaining missing counties for zip codes by looking them up online. After that I was left with a dictionary by which I could replace the missing values. #Drop duplicates on the zipcode column. df_test = df_test . drop_duplicates ([ 'Zip Code' ]) #Check shape to confirm we're ok. df_test . shape #Turn dataframe into dictionary. dict_test = df_test . set_index ( 'Zip Code' ) . to_dict ()[ 'County' ] #Check dictionary to confimr it worked as expected. dict_test #The dictionary created in the above step was saved as in a .py file and is being imported here. from missing_data_dictionaries import zip_counties_map #We have mixed data types in the zip code column so we'll set those to all be strings. df [ 'Zip Code' ] = df [ 'Zip Code' ] . astype ( str ) #Create a new column that uses the zip code column and zip code dictionary to return a complete county column. #This is MUCH faster than using \"fillna\" or \"replace\" to impute values into a column from a dictionary. df [ 'County' ] = df [ 'Zip Code' ] . map ( zip_counties_map ) #Checking results #print df.County.value_counts() #Check for missing values. df . isnull () . sum () #All the missing values have been correctly mapped and resolved. Date 0 Store Number 0 City 0 Zip Code 0 County 0 Category 779 Category Name 6109 Vendor Number 0 Item Number 0 Item Description 0 Bottle Volume (ml) 0 State Bottle Cost 0 State Bottle Retail 0 Bottles Sold 0 Sale (Dollars) 0 Volume Sold (Liters) 0 dtype: int64 \"Categories Name\" Column #Put Category and item description into a data frame. df_category_name = df [[ 'Category Name' , 'Item Description' ]] df_category_name . shape #Drop duplicates on the Item Description column. df_category_name = df_category_name . drop_duplicates ([ 'Item Description' ]) #Convert the nan's to strings. df_category_name = df_category_name . astype ( str ) #Turn dataframe into dictionary. dict_category_name = df_category_name . set_index ( 'Item Description' ) . to_dict ()[ 'Category Name' ] #Split the two dictionaries using dictionary comprehension #Filtering out the dictionary that has nan's to fix by hand. dict_nan = { k : v for k , v in dict_category_name . iteritems () if v == 'nan' } #Retaining the dictionary that has correct values. dict_category = { k : v for k , v in dict_category_name . iteritems () if v != 'nan' } #The dictionary with missing values was exported to a .py file, fixed by hand, and is being imported back here. from missing_data_dictionaries import missing_category_names_map #Concatenate the two dictionaries to create a complete map to fill in missing values. dict_category_name_complete = dict ( dict_category . items () + missing_category_names_map . items ()) #Fill in the missing values using the .map function and check to see that all missing values were resolved. df [ 'Category Name' ] = df [ 'Item Description' ] . map ( dict_category_name_complete ) df . isnull () . sum () Date 0 Store Number 0 City 0 Zip Code 0 County 0 Category 779 Category Name 0 Vendor Number 0 Item Number 0 Item Description 0 Bottle Volume (ml) 0 State Bottle Cost 0 State Bottle Retail 0 Bottles Sold 0 Sale (Dollars) 0 Volume Sold (Liters) 0 dtype: int64 \"Zip Code\" Column #Bringing in data that was scraped from iowa.hometownlocator.com to validate the zip code column. data3 = pd . read_csv ( '../../../../../project2_data/Iowa_Population_Scraped_Data.csv' ) df4 = pd . DataFrame ( data3 ) df4 . drop ([ 'Unnamed: 0' ], axis = 1 , inplace = True ) df4 . head () Average Family Size Average Household Size Diversity Index2 Family Households Population Density1 Population in Families Population in Group Qrtrs Population in Households Total Households Total Population Zips 0 3.0 2.76 8.0 206.0 30.0 651.0 0.0 736.0 267.0 736.0 50001 1 3.0 2.28 7.0 367.0 12.0 1052.0 0.0 1325.0 581.0 1325.0 50002 2 3.0 2.57 11.0 2027.0 63.0 6254.0 119.0 7465.0 2910.0 7584.0 50003 3 3.0 2.46 9.0 222.0 26.0 632.0 0.0 757.0 308.0 757.0 50005 4 3.0 2.45 10.0 501.0 14.0 1479.0 2.0 1798.0 735.0 1800.0 50006 #Convert Zip Code columns to strings so we're comparing apples to apples. df [ 'Zip Code' ] = df [ 'Zip Code' ] . astype ( str ) df4 [ 'Zips' ] = df4 [ 'Zips' ] . astype ( str ) #Check if the \"Zip Code\" values in our data set are also in the \"Zips\" column that was scraped. #Return a true or false and place the values into a \"Zip_Check\" column. df [ 'Zip_Check' ] = np . where ( df [ 'Zip Code' ] . isin ( df4 [ 'Zips' ]), 'True' , 'False' ) #Get a value count of the \"Zip_Check\" column and see where we stand. df [ 'Zip_Check' ] . value_counts () True 2694948 False 14604 Name: Zip_Check, dtype: int64 #Filter out the \"False\" rows into a new dataframe that we can analyze further. df_zips = df [ df [ 'Zip_Check' ] == 'False' ] #Now lets see what city zip codes don't match the list we scraped. df_zips [ 'City' ] . unique () array(['DAVENPORT', 'DUNLAP', 'CEDAR RAPIDS', 'BURLINGTON', 'MANCHESTER', 'DES MOINES'], dtype=object) #Lets check what the given zip codes for these cities are in our original dataframe. df_zips [ 'Zip Code' ] . unique () array(['52084', '712-2', '52303', '56201', '52087', '50300'], dtype=object) #After looking these cities and zips up online its clear these zip codes are mis-entered. We'll fix them below. #Fix DAVENPORT zip code df [ 'Zip Code' ] = df [ 'Zip Code' ] . replace ( to_replace = '52084' , value = '52804' ) #Fix MANCHESTER zip code df [ 'Zip Code' ] = df [ 'Zip Code' ] . replace ( to_replace = '52087' , value = '52057' ) #Fix DUNLAP zip code df [ 'Zip Code' ] = df [ 'Zip Code' ] . replace ( to_replace = '712-2' , value = '51529' ) #Fix DES MOINES zip code df [ 'Zip Code' ] = df [ 'Zip Code' ] . replace ( to_replace = '50300' , value = '50301' ) #Fix BURLINGTON zip code df [ 'Zip Code' ] = df [ 'Zip Code' ] . replace ( to_replace = '56201' , value = '52601' ) #Fix Cedar Rapids zip code df [ 'Zip Code' ] = df [ 'Zip Code' ] . replace ( to_replace = '52303' , value = '52403' ) #Run the check again to make sure all the values in our zip codes column are accurate zip codes. df [ 'Zip_Check2' ] = np . where ( df [ 'Zip Code' ] . isin ( df4 [ 'Zips' ]), 'True' , 'False' ) df [ 'Zip_Check2' ] . value_counts () True 2709552 Name: Zip_Check2, dtype: int64 \"Category\" Column #The code below was used to create a dictionary of category names to category. The category values were then #replaced with higher level generic names for meaningful data analysis. #Put Category and Category name into a data frame. df_cat = df [[ 'Category Name' , 'Category' ]] #Drop duplicates on the Category Name column. df_cat = df_cat . drop_duplicates ([ 'Category Name' ]) #Convert the nan's to strings. df_cat = df_cat . astype ( str ) #Turn dataframe into dictionary. dict_cat = df_cat . set_index ( 'Category Name' ) . to_dict ()[ 'Category' ] #The dictionary was then exported as a .py file and is being imported back into the notebook here. from missing_data_dictionaries import general_map #Use the above dictionary map to relabel the \"Category\" column based on the values in the \"Category Name\" column #Then check for missing values. df [ 'Category' ] = df [ 'Category Name' ] . map ( general_map ) df . isnull () . sum () Date 0 Store Number 0 City 0 Zip Code 0 County 0 Category 0 Category Name 0 Vendor Number 0 Item Number 0 Item Description 0 Bottle Volume (ml) 0 State Bottle Cost 0 State Bottle Retail 0 Bottles Sold 0 Sale (Dollars) 0 Volume Sold (Liters) 0 Zip_Check 0 Zip_Check2 0 dtype: int64 RESULT: Resolving Missing Values Dataframe Shape Before = (2709552, 18) Dataframe Shape After = (2709552, 18) Missing Values Resolved = 17,801 Missing Values Unresolved/Dropped = 0 2. Data Wrangling: Extract Data [\"Date\"] to --> [\"year\"] [\"month\"] [\"day\"] [\"weekday\"] #Convert the date column to string so we know what to expect with each step below. df [ 'Date' ] = df [ 'Date' ] . astype ( str ) #Create a year column with simple slicing. df [ 'year' ] = df [ 'Date' ] . str [ - 4 :] #Create month column with simple slicing. df [ 'month' ] = df [ 'Date' ] . str [: 2 ] #Create a day column with a two step slicing. df [ 'day' ] = df [ 'Date' ] . str [: 5 ] df [ 'day' ] = df [ 'day' ] . str [ - 2 :] #import datetime library to get the weekday for the \"Date\" column. import datetime as dt #Run the most annoying code in the world on the \"Date\" column. df [ 'Date_Change' ] = pd . to_datetime ( df [ 'Date' ]) #Create a weekday column from the new \"Date_Change\" column. df [ 'weekday' ] = df [ 'Date_Change' ] . dt . dayofweek #Convert the weekdays from numbers to their abbreviations. days = { 0 : 'Mon' , 1 : 'Tues' , 2 : 'Weds' , 3 : 'Thurs' , 4 : 'Fri' , 5 : 'Sat' , 6 : 'Sun' } df [ 'weekday' ] = df [ 'weekday' ] . map ( days ) [\"Bottle Volumn (ml)\"] [\"Bottles Sold\"] to --> [\"ml_sold\"] #Create a new column called \"ml_sold\" df [ 'ml_sold' ] = df [ 'Bottle Volume (ml)' ] * df [ 'Bottles Sold' ] df [ 'ml_sold' ] = df [ 'ml_sold' ] . astype ( float ) [\"Sale (Dollars)] and [\"State Bottle Cost\"] formatting #Format the Sale (Dollars) column. df [ 'Sale (Dollars)' ] = df [ 'Sale (Dollars)' ] . str . replace ( '$' , '' ) df [ 'Sale (Dollars)' ] = df [ 'Sale (Dollars)' ] . astype ( float ) #Format the State Cost column. df [ 'State Bottle Cost' ] = df [ 'State Bottle Cost' ] . str . replace ( '$' , '' ) df [ 'State Bottle Cost' ] = df [ 'State Bottle Cost' ] . astype ( float ) [\"Sale (Dollars)\"] [\"ml_sold\"] to --> [\"price_per_ml\"] #Create a new column called \"price_per_ml\" df [ 'price_per_ml' ] = df [ 'Sale (Dollars)' ] / df [ 'ml_sold' ] [\"State Bottle Cost\"] [\"ml_sold\"] to --> [\"cost_per_ml\"] #Create a new column called \"cost_per_ml\" df [ 'cost_per_ml' ] = df [ 'State Bottle Cost' ] / df [ 'ml_sold' ] [\"Category\"] [\"ml_sold\"] to --> [\"BRANDY\"] [\"GIN\"] [\"LIQUEUR\"] [\"MISC\"] [\"MIXED\"] [\"RUM\"] [\"TEQUILA\"] [\"VODKA\"] [\"WHISKEY\"] #Create dummy variables for category so we can compare the different alcohols. cat_dummy = pd . get_dummies ( df [ 'Category' ]) df = pd . concat ([ df , cat_dummy ], axis = 1 ) #Multiply the dummy variables by ml_sold so that we can aggregate a total sold for each category. df [ 'BRANDY' ] = df [ 'BRANDY' ] * df [ 'ml_sold' ] df [ 'GIN' ] = df [ 'GIN' ] * df [ 'ml_sold' ] df [ 'LIQUEUR' ] = df [ 'LIQUEUR' ] * df [ 'ml_sold' ] df [ 'MISC' ] = df [ 'MISC' ] * df [ 'ml_sold' ] df [ 'MIXED' ] = df [ 'MIXED' ] * df [ 'ml_sold' ] df [ 'RUM' ] = df [ 'RUM' ] * df [ 'ml_sold' ] df [ 'TEQUILA' ] = df [ 'TEQUILA' ] * df [ 'ml_sold' ] df [ 'VODKA' ] = df [ 'VODKA' ] * df [ 'ml_sold' ] df [ 'WHISKEY' ] = df [ 'WHISKEY' ] * df [ 'ml_sold' ] 3. Import Outside Data IMPORTING --> [\"Food Service Businesses per Zip Code\"] #Importing population data to accomplish some of these key questions. biz_data = pd . read_csv ( '../data/biz_data.csv' ) biz = pd . DataFrame ( biz_data ) biz [ 'Food Businesses' ] = biz [ 'Food Businesses' ] . astype ( str ) biz [ 'Zip Code' ] = biz [ 'Zip Code' ] . astype ( int ) biz . head () #Data taken from US Census NAICS industry data for zip codes Zip Code Food Businesses 0 50002 5 1 50003 12 2 50006 1 3 50009 41 4 50010 120 #Convert Zip Code column to integer to merge datasets on. df [ 'Zip Code' ] = df [ 'Zip Code' ] . astype ( int ) #Merge the biz dataframe with the main dataframe. df = pd . merge ( df , biz , how = 'left' , on = 'Zip Code' , copy = False , sort = False ) #Fill any \"NaN\" with 0's as that zip code doesn't have any food businesses registered. df [ 'Food Businesses' ] = df [ 'Food Businesses' ] . fillna ( 0 ) #Convert datatype back to int df [ 'Food Businesses' ] = df [ 'Food Businesses' ] . astype ( int ) IMPORTING --> [\"Total Housing Units\"] [\"Median Home Value\"] [\"Average Home Value\"] [\"Median Household Income\"] [\"Average Household Income\"] [\"Per Capita Income\"] [\"Population Density\"] [\"Population in Families\"] [\"Total Population\"] #Bringing in data that was scraped from iowa.hometownlocator.com data2 = pd . read_csv ( '../../../../../project2_data/Iowa_Population_Scraped_Data_Second_Table_Fixed.csv' ) df3 = pd . DataFrame ( data2 ) df3 . drop ([ 'Unnamed: 0' , 'Total Housing Units' , 'Renter Occupied HU_percent' , 'Renter Occupied HU' , 'Vacant Housing Units' , 'Vacant Housing Units _percent' ], axis = 1 , inplace = True ) df3 . head () Median Home Value Average Home Value Median Household Income Average Household Income Per Capita Income Zips 0 214394.0 229626.0 69372.0 82718.0 30008.0 50001 1 96875.0 124448.0 47230.0 58575.0 25684.0 50002 2 187228.0 239472.0 64408.0 84820.0 32778.0 50003 3 130000.0 159231.0 54553.0 69720.0 28367.0 50005 4 87207.0 113404.0 53722.0 72983.0 29811.0 50006 #Bringing in data that was scraped from iowa.hometownlocator.com data3 = pd . read_csv ( '../../../../../project2_data/Iowa_Population_Scraped_Data.csv' ) df4 = pd . DataFrame ( data3 ) df4 . drop ([ 'Unnamed: 0' , 'Average Family Size' , 'Average Household Size' , 'Diversity Index2' , 'Population in Group Qrtrs' , 'Population in Households' , 'Family Households' , 'Total Households' ], axis = 1 , inplace = True ) df4 . head () Population Density1 Population in Families Total Population Zips 0 30.0 651.0 736.0 50001 1 12.0 1052.0 1325.0 50002 2 63.0 6254.0 7584.0 50003 3 26.0 632.0 757.0 50005 4 14.0 1479.0 1800.0 50006 #Rename \"Zips\" column and set to integer so we can merge on it. df4 . rename ( columns = { 'Zips' : 'Zip Code' }, inplace = True ) df4 [ 'Zip Code' ] = df4 [ 'Zip Code' ] . astype ( int ) #Rename \"Zips\" column and set to integer so we can merge on it. df3 . rename ( columns = { 'Zips' : 'Zip Code' }, inplace = True ) df3 [ 'Zip Code' ] = df3 [ 'Zip Code' ] . astype ( int ) #Merge on Zip Code column df = pd . merge ( df , df3 , how = 'left' , on = 'Zip Code' , copy = False , sort = False ) #Merge on Zip Code column df = pd . merge ( df , df4 , how = 'left' , on = 'Zip Code' , copy = False , sort = False ) df . columns Index([u'Store Number', u'City', u'Zip Code', u'County', u'Category', u'Category Name', u'Vendor Number', u'Item Number', u'Item Description', u'Bottle Volume (ml)', u'State Bottle Cost', u'State Bottle Retail', u'Bottles Sold', u'Sale (Dollars)', u'Volume Sold (Liters)', u'year', u'month', u'day', u'weekday', u'Food Businesses', u'Median Home Value', u'Average Home Value', u'Median Household Income', u'Average Household Income', u'Per Capita Income', u'Population Density1', u'Population in Families', u'Total Population'], dtype='object') #Dropping columns that won't get used. df . drop ([ 'Zip_Check' , 'Zip_Check2' , 'Date' , 'Date_Change' ], axis = 1 , inplace = True ) #Creating a new dataframe called \"df_clean\" that we'll used for EDA. df_clean = df df_clean Store Number City Zip Code County Category Category Name Vendor Number Item Number Item Description Bottle Volume (ml) ... weekday Food Businesses Median Home Value Average Home Value Median Household Income Average Household Income Per Capita Income Population Density1 Population in Families Total Population 0 5029 DAVENPORT 52806 Scott TEQUILA TEQUILA 370 87152 Avion Silver 375 ... Thurs 85 144979.0 159648.0 51336.0 64201.0 26216.0 944.0 23161.0 29531.0 1 5029 DAVENPORT 52806 Scott TEQUILA TEQUILA 395 89197 Jose Cuervo Especial Reposado Tequila 1000 ... Thurs 85 144979.0 159648.0 51336.0 64201.0 26216.0 944.0 23161.0 29531.0 2 4959 CEDAR FALLS 50613 Black Hawk MIXED AMERICAN COCKTAILS 380 63959 Uv Blue Raspberry Lemonade Pet 1750 ... Thurs 110 185832.0 213143.0 54779.0 72853.0 30239.0 332.0 26715.0 39926.0 3 2190 DES MOINES 50314 Polk VODKA VODKA FLAVORED 205 40597 New Amsterdam Red Berry 200 ... Thurs 24 87153.0 104188.0 31885.0 43863.0 15501.0 5617.0 9831.0 14112.0 4 5240 WEST BRANCH 52358 Cedar LIQUEUR CREAM LIQUEURS 305 73055 Rumchata 750 ... Thurs 9 164138.0 197958.0 62530.0 73256.0 29832.0 46.0 3035.0 3865.0 5 4988 CEDAR FALLS 50613 Black Hawk WHISKEY STRAIGHT BOURBON WHISKIES 260 17090 Bulleit Bourbon 10YR 750 ... Thurs 110 185832.0 213143.0 54779.0 72853.0 30239.0 332.0 26715.0 39926.0 6 3993 WATERLOO 50701 Black Hawk LIQUEUR IMPORTED SCHNAPPS 421 69636 Dr. Mcgillicuddy's Cherry Schnapps 750 ... Thurs 61 145403.0 165981.0 50060.0 65911.0 28708.0 422.0 22760.0 30930.0 7 5029 DAVENPORT 52806 Scott TEQUILA TEQUILA 410 88294 Patron Silver Tequila 375 ... Thurs 85 144979.0 159648.0 51336.0 64201.0 26216.0 944.0 23161.0 29531.0 8 5144 ANKENY 50021 Polk BRANDY AMERICAN GRAPE BRANDIES 259 52318 Christian Bros Brandy 1750 ... Thurs 74 195948.0 211988.0 79976.0 94705.0 37508.0 985.0 21100.0 25657.0 9 5029 DAVENPORT 52806 Scott BRANDY IMPORTED GRAPE BRANDIES 389 49185 Remy Martin Vsop (flask) 375 ... Thurs 85 144979.0 159648.0 51336.0 64201.0 26216.0 944.0 23161.0 29531.0 10 5029 DAVENPORT 52806 Scott TEQUILA TEQUILA 410 88296 Patron Tequila Silver 750 ... Thurs 85 144979.0 159648.0 51336.0 64201.0 26216.0 944.0 23161.0 29531.0 11 5029 DAVENPORT 52806 Scott BRANDY IMPORTED GRAPE BRANDIES 420 48105 Hennessy VS 375 ... Thurs 85 144979.0 159648.0 51336.0 64201.0 26216.0 944.0 23161.0 29531.0 12 5029 DAVENPORT 52806 Scott BRANDY IMPORTED GRAPE BRANDIES 389 49186 Remy Martin Vsop Cognac 750 ... Thurs 85 144979.0 159648.0 51336.0 64201.0 26216.0 944.0 23161.0 29531.0 13 5029 DAVENPORT 52806 Scott WHISKEY CANADIAN WHISKIES 260 11298 Crown Royal Canadian Whisky 1750 ... Thurs 85 144979.0 159648.0 51336.0 64201.0 26216.0 944.0 23161.0 29531.0 14 5029 DAVENPORT 52806 Scott RUM SPICED RUM 260 43337 Captain Morgan Spiced Rum 1000 ... Thurs 85 144979.0 159648.0 51336.0 64201.0 26216.0 944.0 23161.0 29531.0 15 5029 DAVENPORT 52806 Scott RUM SPICED RUM 260 43244 Captain Morgan 100 Proof Spiced Rum 750 ... Thurs 85 144979.0 159648.0 51336.0 64201.0 26216.0 944.0 23161.0 29531.0 16 4447 URBANDALE 50322 Polk VODKA VODKA 80 PROOF 300 36904 Mccormick Vodka Pet 375 ... Thurs 87 184512.0 196452.0 74240.0 94575.0 39596.0 3019.0 26021.0 33071.0 17 5029 DAVENPORT 52806 Scott LIQUEUR MISC. IMPORTED CORDIALS & LIQUEURS 192 65256 Jagermeister Liqueur 750 ... Thurs 85 144979.0 159648.0 51336.0 64201.0 26216.0 944.0 23161.0 29531.0 18 5029 DAVENPORT 52806 Scott TEQUILA TEQUILA 395 89196 Jose Cuervo Especial Reposado Tequila 750 ... Thurs 85 144979.0 159648.0 51336.0 64201.0 26216.0 944.0 23161.0 29531.0 19 5029 DAVENPORT 52806 Scott WHISKEY STRAIGHT BOURBON WHISKIES 65 19066 Jim Beam 750 ... Thurs 85 144979.0 159648.0 51336.0 64201.0 26216.0 944.0 23161.0 29531.0 20 5029 DAVENPORT 52806 Scott WHISKEY CANADIAN WHISKIES 260 10805 Crown Royal Regal Apple 375 ... Thurs 85 144979.0 159648.0 51336.0 64201.0 26216.0 944.0 23161.0 29531.0 21 5029 DAVENPORT 52806 Scott WHISKEY BLENDED WHISKIES 297 23826 Five Star 750 ... Thurs 85 144979.0 159648.0 51336.0 64201.0 26216.0 944.0 23161.0 29531.0 22 5029 DAVENPORT 52806 Scott WHISKEY CANADIAN WHISKIES 115 11774 Black Velvet 375 ... Thurs 85 144979.0 159648.0 51336.0 64201.0 26216.0 944.0 23161.0 29531.0 23 5034 MASON CITY 50401 Cerro Gordo WHISKEY CANADIAN WHISKIES 55 11936 Canadian Ltd Whisky Convenience Pack 750 ... Thurs 82 121305.0 148989.0 44670.0 60145.0 27169.0 195.0 21502.0 29540.0 24 5029 DAVENPORT 52806 Scott WHISKEY CANADIAN WHISKIES 260 11297 Crown Royal Canadian Whisky 1000 ... Thurs 85 144979.0 159648.0 51336.0 64201.0 26216.0 944.0 23161.0 29531.0 25 5029 DAVENPORT 52806 Scott VODKA IMPORTED VODKA - MISC 260 64511 Ciroc Apple 375 ... Thurs 85 144979.0 159648.0 51336.0 64201.0 26216.0 944.0 23161.0 29531.0 26 5029 DAVENPORT 52806 Scott VODKA VODKA 80 PROOF 380 37346 Phillips Vodka 750 ... Thurs 85 144979.0 159648.0 51336.0 64201.0 26216.0 944.0 23161.0 29531.0 27 5029 DAVENPORT 52806 Scott BRANDY PEACH BRANDIES 115 56195 Paul Masson Peach Grande Amber Brandy 375 ... Thurs 85 144979.0 159648.0 51336.0 64201.0 26216.0 944.0 23161.0 29531.0 28 4802 DES MOINES 50310 Polk WHISKEY CANADIAN WHISKIES 115 11788 Black Velvet 1750 ... Thurs 78 144152.0 151836.0 54430.0 67785.0 29144.0 3926.0 23098.0 31820.0 29 5029 DAVENPORT 52806 Scott BRANDY AMERICAN GRAPE BRANDIES 205 52594 E & J Vs Brandy 375 ... Thurs 85 144979.0 159648.0 51336.0 64201.0 26216.0 944.0 23161.0 29531.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2709522 2527 DES MOINES 50315 Polk LIQUEUR PEPPERMINT SCHNAPPS 322 75213 Kinky Blue 375 ... Mon 59 117251.0 131356.0 46894.0 57460.0 22566.0 3879.0 28913.0 38347.0 2709523 3403 COLUMBUS JUNCTION 52738 Louisa RUM SPICED RUM 260 43337 Captain Morgan Spiced Rum 1000 ... Mon 5 118569.0 151177.0 53253.0 63189.0 23659.0 30.0 2958.0 3609.0 2709524 4267 ANITA 50020 Cass LIQUEUR COFFEE LIQUEURS 65 67556 Kamora Coffee Liqueur 750 ... Mon 2 86962.0 131983.0 46092.0 58449.0 25658.0 16.0 1109.0 1403.0 2709525 4229 HARLAN 51537 Shelby VODKA VODKA 80 PROOF 434 36308 Hawkeye Vodka 1750 ... Mon 17 128089.0 164435.0 49391.0 62325.0 26784.0 40.0 5041.0 6475.0 2709526 4794 BURLINGTON 52601 Des Moines GIN IMPORTED DRY GINS 35 28236 Bombay Sapphire Gin 750 ... Mon 81 97672.0 130929.0 42928.0 55654.0 24203.0 215.0 22636.0 29893.0 2709527 2560 MARION 52302 Linn LIQUEUR CREAM LIQUEURS 260 68022 Bailey's Salted Caramel 750 ... Mon 56 179547.0 215378.0 66825.0 85049.0 33963.0 507.0 32580.0 40249.0 2709528 2558 MOUNT PLEASANT 52641 Henry LIQUEUR IMPORTED SCHNAPPS 421 69611 Dr. Mcgillicuddy's Apple Pie Schnapps 750 ... Mon 37 126127.0 162932.0 47282.0 63247.0 25012.0 60.0 8692.0 12166.0 2709529 2573 MUSCATINE 52761 Muscatine VODKA VODKA FLAVORED 380 40244 Prairie Organic Cucumber 750 ... Mon 73 131936.0 158914.0 51255.0 65218.0 25484.0 181.0 25123.0 31280.0 2709530 4188 HOLY CROSS 52053 Dubuque BRANDY AMERICAN GRAPE BRANDIES 85 52806 Korbel Brandy 750 ... Mon 1 140534.0 192355.0 53391.0 64017.0 23886.0 20.0 1001.0 1131.0 2709531 2599 CORALVILLE 52241 Johnson MIXED AMERICAN COCKTAILS 395 58872 Jose Cuervo Authentic Light Margarita Lime 1750 ... Mon 104 208008.0 252146.0 58212.0 81118.0 34195.0 1512.0 13204.0 19909.0 2709532 3952 BETTENDORF 52722 Scott VODKA IMPORTED VODKA - MISC 370 34029 Absolut Citron (lemon Vodka) 1000 ... Mon 76 220240.0 259427.0 75901.0 103441.0 42042.0 1384.0 31521.0 38041.0 2709533 3631 AUDUBON 50025 Audubon VODKA VODKA 80 PROOF 260 37994 Smirnoff Vodka 80 Prf 375 ... Mon 4 82997.0 119378.0 47561.0 70060.0 30843.0 18.0 2452.0 3094.0 2709534 3773 CEDAR RAPIDS 52401 Linn TEQUILA TEQUILA 410 88296 Patron Tequila Silver 750 ... Mon 39 80690.0 98950.0 23066.0 37809.0 18864.0 1794.0 1081.0 2359.0 2709535 2643 WATERLOO 50701 Black Hawk BRANDY AMERICAN GRAPE BRANDIES 259 52317 Christian Bros Brandy-Square 1000 ... Mon 61 145403.0 165981.0 50060.0 65911.0 28708.0 422.0 22760.0 30930.0 2709536 2633 DES MOINES 50320 Polk LIQUEUR MISC. IMPORTED CORDIALS & LIQUEURS 260 66636 Romana Sambuca Italian Liquore 750 ... Mon 29 165019.0 177346.0 57010.0 71409.0 26095.0 1150.0 18038.0 22180.0 2709537 2648 WEST DES MOINES 50265 Polk WHISKEY SINGLE BARREL BOURBON WHISKIES 65 19235 Knob Creek Single Barrel Reserve 750 ... Mon 78 195826.0 237351.0 70458.0 90579.0 37942.0 1852.0 25769.0 34256.0 2709538 2506 BURLINGTON 52601 Des Moines WHISKEY CANADIAN WHISKIES 65 10628 Canadian Club Whisky 1750 ... Mon 81 97672.0 130929.0 42928.0 55654.0 24203.0 215.0 22636.0 29893.0 2709539 4164 BETTENDORF 52722 Scott WHISKEY TENNESSEE WHISKIES 85 86668 Jack Daniel's Tennessee Honey Mini 500 ... Mon 76 220240.0 259427.0 75901.0 103441.0 42042.0 1384.0 31521.0 38041.0 2709540 4972 MONONA 52159 Clayton VODKA VODKA FLAVORED 380 40625 UV Chocolate Cake 750 ... Mon 4 118807.0 146975.0 50226.0 61501.0 26205.0 33.0 1791.0 2212.0 2709541 3525 WASHINGTON 52353 Washington VODKA IMPORTED VODKA - MISC 260 64727 Ciroc Amaretto 750 ... Mon 22 118875.0 145151.0 50677.0 60831.0 25570.0 55.0 7118.0 9210.0 2709542 4000 WOODBINE 51579 Harrison LIQUEUR COFFEE LIQUEURS 370 67571 Kahlua Mocha Liqueur 750 ... Mon 3 111635.0 148433.0 43525.0 59893.0 24762.0 20.0 1891.0 2370.0 2709543 2590 CEDAR RAPIDS 52402 Linn VODKA VODKA 80 PROOF 297 35913 Five O'clock Vodka 200 ... Mon 148 146730.0 178726.0 58412.0 78027.0 32653.0 3070.0 29625.0 41311.0 2709544 3494 WATERLOO 50702 Black Hawk VODKA VODKA 80 PROOF 55 37938 Skol Vodka 1750 ... Mon 71 108462.0 121424.0 42359.0 52451.0 22515.0 1933.0 15330.0 20750.0 2709545 3907 MUSCATINE 52761 Muscatine RUM FLAVORED RUM 35 43136 Bacardi Limon 750 ... Mon 73 131936.0 158914.0 51255.0 65218.0 25484.0 181.0 25123.0 31280.0 2709546 2448 INDIANOLA 50125 Warren WHISKEY WHISKEY LIQUEUR 85 86887 Southern Comfort 1000 ... Mon 40 177962.0 206910.0 61032.0 79096.0 30345.0 123.0 15267.0 19900.0 2709547 4696 WEST DES MOINES 50266 Polk VODKA VODKA FLAVORED 205 40192 New Amsterdam Pineapple 375 ... Mon 138 241009.0 271953.0 75865.0 101637.0 43971.0 1562.0 23494.0 32129.0 2709548 2590 CEDAR RAPIDS 52402 Linn WHISKEY SCOTCH WHISKIES 260 5346 Johnnie Walker Red 750 ... Mon 148 146730.0 178726.0 58412.0 78027.0 32653.0 3070.0 29625.0 41311.0 2709549 3631 AUDUBON 50025 Audubon LIQUEUR PEACH SCHNAPPS 65 82847 Dekuyper Peachtree 1000 ... Mon 4 82997.0 119378.0 47561.0 70060.0 30843.0 18.0 2452.0 3094.0 2709550 2517 NEWTON 50208 Jasper MIXED AMERICAN COCKTAILS 395 58868 Jose Cuervo Authentic Strawberry Margarita 1750 ... Mon 44 122724.0 148301.0 49278.0 60441.0 25089.0 126.0 14902.0 20484.0 2709551 2643 WATERLOO 50701 Black Hawk WHISKEY SCOTCH WHISKIES 260 5329 Johnnie Walker Blue 750 ... Mon 61 145403.0 165981.0 50060.0 65911.0 28708.0 422.0 22760.0 30930.0 2709552 rows × 28 columns #Sending cleansed dataset to CSV so I don't have to run the cleansing code multiple times during EDA and Modeling. #df.to_csv('../../../../../project2_data/Iowa_Liquor_Cleansed.csv') 4. EDA Key Questions: (Written Before EDA Is Started) Do specialty alcohols sell better in more urban areas? If people drink a lot do they pay less for it?(price per ml). Do certain types of liquor sell better on specific days of the week? Is there a correlation between number of food businesses and sales/consumption? Where is the most demand? Where is the strongest demand? Where is the most opportunity? import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline 4.0 Create Three Dataframes to Answer All The Questions Below. As a result of the merges we have some data duplicated on rows such as total population per zip code. This will need to be aggregated differently than total sales per zip code. Also we have store number duplicated many times for each sale row, this will have to be aggregated separately as well to get an accurate count of stores per zip code. #Create a dataframe grouped by store number to get store number count per City. #To do this we will need to do a groupby mean and then a groupby count. df_store_agg = df . groupby ([ 'City' , 'Store Number' ]) . mean () df_store_agg = df_store_agg . reset_index () df_store_agg = df_store_agg . groupby ([ 'City' ]) . count () df_store_agg = df_store_agg . reset_index () df_store_agg = df_store_agg [[ 'City' , 'Store Number' ]] df_store_agg . head () City Store Number 0 ACKLEY 2 1 ADAIR 2 2 ADEL 3 3 AFTON 1 4 AKRON 1 #Create a dataframe grouped by city with population data for each city. #To do this we will need to do a groupby mean and then a groupby sum. df_city_agg = df . groupby ([ 'City' , 'Zip Code' ]) . mean () df_city_agg = df_city_agg . reset_index () df_city_agg = df_city_agg . groupby ([ 'City' ]) . sum () df_city_agg = df_city_agg . reset_index () df_city_agg = df_city_agg [[ 'City' , 'Zip Code' , 'Food Businesses' , 'Median Home Value' , 'Average Home Value' , 'Median Household Income' , 'Average Household Income' , 'Per Capita Income' , 'Population Density1' , 'Population in Families' , 'Total Population' ]] df_city_agg . head () City Zip Code Food Businesses Median Home Value Average Home Value Median Household Income Average Household Income Per Capita Income Population Density1 Population in Families Total Population 0 ACKLEY 50601 6.0 95066.0 123245.0 49502.0 63798.0 27282.0 18.0 1919.0 2415.0 1 ADAIR 50002 5.0 96875.0 124448.0 47230.0 58575.0 25684.0 12.0 1052.0 1325.0 2 ADEL 50003 12.0 187228.0 239472.0 64408.0 84820.0 32778.0 63.0 6254.0 7584.0 3 AFTON 50830 4.0 98952.0 136323.0 47230.0 55645.0 22663.0 17.0 1320.0 1704.0 4 AKRON 51001 3.0 115609.0 165228.0 53372.0 68927.0 28101.0 17.0 2176.0 2595.0 #Create a dataframe grouped by city for total sums of sales categories. #To do this we will need to do a groupby sum and then a groupby sum df_cat_agg = df . groupby ([ 'City' , 'Zip Code' ]) . sum () df_cat_agg = df_cat_agg . reset_index () df_cat_agg = df_cat_agg . groupby ([ 'City' ]) . sum () df_cat_agg = df_cat_agg . reset_index () df_cat_agg = df_cat_agg [[ 'City' , 'Zip Code' , 'State Bottle Cost' , 'Sale (Dollars)' , 'ml_sold' , 'BRANDY' , 'GIN' , 'LIQUEUR' , 'MISC' , 'MIXED' , 'RUM' , 'TEQUILA' , 'VODKA' , 'WHISKEY' ]] df_cat_agg . head () City Zip Code State Bottle Cost Sale (Dollars) ml_sold BRANDY GIN LIQUEUR MISC MIXED RUM TEQUILA VODKA WHISKEY 0 ACKLEY 50601 8635.46 81583.88 7121525.0 74000.0 220750.0 432875.0 65250.0 28500.0 1021000.0 55000.0 2115400.0 3108750.0 1 ADAIR 50002 3278.76 48094.26 3276300.0 54900.0 51000.0 141000.0 0.0 0.0 473500.0 36375.0 1272875.0 1246650.0 2 ADEL 50003 24305.16 412478.63 31819450.0 399575.0 692125.0 1523125.0 331750.0 1956250.0 4810500.0 644150.0 10168975.0 11293000.0 3 AFTON 50830 1514.07 35284.62 2853900.0 45000.0 0.0 121500.0 0.0 93000.0 183000.0 29400.0 956700.0 1425300.0 4 AKRON 51001 5148.41 50029.53 4074625.0 45975.0 6750.0 291750.0 0.0 2250.0 591125.0 58500.0 1480600.0 1597675.0 4.1. Do specialty alcohols sell better in more urban areas? #Now that we got two dataframes aggregated by city we'll concatenate them together with the columns we are #interested in. city_agg = pd . concat ([ df_city_agg [[ 'City' , 'Total Population' ]], df_cat_agg [[ 'ml_sold' , 'BRANDY' , 'GIN' , 'LIQUEUR' , 'MISC' , 'MIXED' , 'RUM' , 'TEQUILA' , 'VODKA' , 'WHISKEY' ]]], axis = 1 ) #Index the dataframe with most populous city at the top city_agg = city_agg . sort_values ([ 'Total Population' ], ascending = False ) city_agg = city_agg . reset_index () city_agg . drop ([ 'index' ], axis = 1 , inplace = True ) city_agg . head () City Total Population ml_sold BRANDY GIN LIQUEUR MISC MIXED RUM TEQUILA VODKA WHISKEY 0 DES MOINES 260565.0 2.737350e+09 111901050.0 92562150.0 291899107.0 21993525.0 46209525.0 309433200.0 204755250.0 910949200.0 747646630.0 1 CEDAR RAPIDS 139975.0 1.677146e+09 60701150.0 49493825.0 175563944.0 38246025.0 56549650.0 218972150.0 58300750.0 547269950.0 472048550.0 2 DAVENPORT 106717.0 1.157200e+09 59139975.0 40239300.0 106694799.0 16302550.0 36091150.0 137470350.0 62455475.0 380296450.0 318509950.0 3 SIOUX CITY 87084.0 7.967826e+08 29191725.0 18879475.0 91298021.0 11412350.0 27882975.0 110612275.0 33366000.0 207301525.0 266838300.0 4 WATERLOO 80574.0 8.220125e+08 77652825.0 38660000.0 67757518.0 26733650.0 27258750.0 88387325.0 36272950.0 238445425.0 220844025.0 #Calculate the ml per person for each alcohol category. city_agg [ 'BRANDY' ] = city_agg [ 'BRANDY' ] / city_agg [ 'Total Population' ] city_agg [ 'GIN' ] = city_agg [ 'GIN' ] / city_agg [ 'Total Population' ] city_agg [ 'LIQUEUR' ] = city_agg [ 'LIQUEUR' ] / city_agg [ 'Total Population' ] city_agg [ 'MISC' ] = city_agg [ 'MISC' ] / city_agg [ 'Total Population' ] city_agg [ 'MIXED' ] = city_agg [ 'MIXED' ] / city_agg [ 'Total Population' ] city_agg [ 'RUM' ] = city_agg [ 'RUM' ] / city_agg [ 'Total Population' ] city_agg [ 'TEQUILA' ] = city_agg [ 'TEQUILA' ] / city_agg [ 'Total Population' ] city_agg [ 'VODKA' ] = city_agg [ 'VODKA' ] / city_agg [ 'Total Population' ] city_agg [ 'WHISKEY' ] = city_agg [ 'WHISKEY' ] / city_agg [ 'Total Population' ] #Separate between rural and urban using the Census 50,000 population cutoff standard. city_agg [ 'pop' ] = [ 'Urban' if x else 'Rural' for x in city_agg [ 'Total Population' ] >= 50000 ] #Create a dataframe to compare the ml per person consumption by rural vs. urban. pop_agg = city_agg . groupby ([ 'pop' ]) . mean () pop_agg = pop_agg . reset_index () pop_agg . drop ([ 'Total Population' , 'ml_sold' ], axis = 1 , inplace = True ) pop_agg = pd . melt ( pop_agg , id_vars = [ 'pop' ]) pop_agg . rename ( columns = { 'variable' : 'Alcohol Category' , 'value' : 'Average Mililiters Consumed Per Person' }, inplace = True ) sns . factorplot ( x = \"Alcohol Category\" , y = \"Average Mililiters Consumed Per Person\" , hue = \"pop\" , data = pop_agg , size = 8 , kind = \"bar\" , palette = \"muted\" ) <seaborn.axisgrid.FacetGrid at 0x11dd39ed0> ANSWER: YES, specialty alcohol does sell better in urban areas over rural once. However all alcohol sells better in the urban areas and Vodka is the best seller with the biggest difference between urban and rural. The urban population consumes 1400ml more Vodka per person than rural Vodka drinkers. 4.2. If people drink a lot do they pay less for it?(price per ml). #Create a dataframe from the ones made earlier to focus on the areas we're interested in. price_per_agg = pd . concat ([ df_city_agg [[ 'City' , 'Total Population' ]], df_cat_agg [[ 'ml_sold' , 'Sale (Dollars)' ]]], axis = 1 ) #Create two new variables called price per ml and ml per person. These will let us see clearly where consumption #tracks with cost. price_per_agg [ 'price_per_ml' ] = price_per_agg [ 'Sale (Dollars)' ] / price_per_agg [ 'ml_sold' ] price_per_agg [ 'ml_per_person' ] = price_per_agg [ 'ml_sold' ] / price_per_agg [ 'Total Population' ] sns . lmplot ( x = \"ml_per_person\" , y = \"price_per_ml\" , data = price_per_agg , size = 7 ) <seaborn.axisgrid.FacetGrid at 0x1192f7310> ANSWER: No, there does not seem to be a clear correlation between the per person ml consumption and the price per ml variables. 4.3. Do certain types of liqour sell better on specific days of the week? weekday_numeric = { 'Mon' : 1 , 'Tues' : 2 , 'Weds' : 3 , 'Thurs' : 4 , 'Fri' : 5 , 'Sat' : 6 , 'Sun' : 7 } #Create a dataframe grouped by weekday for total sold of each category. weekday_dif = df . groupby ([ 'weekday' ]) . mean () weekday_dif = weekday_dif . reset_index () weekday_dif = weekday_dif [[ 'weekday' , 'BRANDY' , 'GIN' , 'LIQUEUR' , 'MISC' , 'MIXED' , 'RUM' , 'TEQUILA' , 'VODKA' , 'WHISKEY' ]] weekday_dif [ 'weekday_numeric' ] = weekday_dif [ 'weekday' ] . map ( weekday_numeric ) weekday_dif = weekday_dif . sort_values ([ 'weekday_numeric' ], ascending = True ) weekday_dif = weekday_dif . reset_index () weekday_dif . drop ([ 'index' , 'weekday_numeric' ], axis = 1 , inplace = True ) #Calculate new columns that show the change in sales between one day and the previous day for each alcohol category. weekday_dif [ 'BRANDY_CHANGE' ] = weekday_dif [ 'BRANDY' ] - weekday_dif [ 'BRANDY' ] . shift ( 1 ) weekday_dif [ 'BRANDY_CHANGE' ][ 0 ] = weekday_dif [ 'BRANDY' ][ 0 ] - weekday_dif [ 'BRANDY' ][ 5 ] weekday_dif [ 'GIN_CHANGE' ] = weekday_dif [ 'GIN' ] - weekday_dif [ 'GIN' ] . shift ( 1 ) weekday_dif [ 'GIN_CHANGE' ][ 0 ] = weekday_dif [ 'GIN' ][ 0 ] - weekday_dif [ 'GIN' ][ 5 ] weekday_dif [ 'LIQUEUR_CHANGE' ] = weekday_dif [ 'LIQUEUR' ] - weekday_dif [ 'LIQUEUR' ] . shift ( 1 ) weekday_dif [ 'LIQUEUR_CHANGE' ][ 0 ] = weekday_dif [ 'LIQUEUR' ][ 0 ] - weekday_dif [ 'LIQUEUR' ][ 5 ] weekday_dif [ 'MISC_CHANGE' ] = weekday_dif [ 'MISC' ] - weekday_dif [ 'MISC' ] . shift ( 1 ) weekday_dif [ 'MISC_CHANGE' ][ 0 ] = weekday_dif [ 'MISC' ][ 0 ] - weekday_dif [ 'MISC' ][ 5 ] weekday_dif [ 'MIXED_CHANGE' ] = weekday_dif [ 'MIXED' ] - weekday_dif [ 'MIXED' ] . shift ( 1 ) weekday_dif [ 'MIXED_CHANGE' ][ 0 ] = weekday_dif [ 'MIXED' ][ 0 ] - weekday_dif [ 'MIXED' ][ 5 ] weekday_dif [ 'RUM_CHANGE' ] = weekday_dif [ 'RUM' ] - weekday_dif [ 'RUM' ] . shift ( 1 ) weekday_dif [ 'RUM_CHANGE' ][ 0 ] = weekday_dif [ 'RUM' ][ 0 ] - weekday_dif [ 'RUM' ][ 5 ] weekday_dif [ 'TEQUILA_CHANGE' ] = weekday_dif [ 'TEQUILA' ] - weekday_dif [ 'TEQUILA' ] . shift ( 1 ) weekday_dif [ 'TEQUILA_CHANGE' ][ 0 ] = weekday_dif [ 'TEQUILA' ][ 0 ] - weekday_dif [ 'TEQUILA' ][ 5 ] weekday_dif [ 'VODKA_CHANGE' ] = weekday_dif [ 'VODKA' ] - weekday_dif [ 'VODKA' ] . shift ( 1 ) weekday_dif [ 'VODKA_CHANGE' ][ 0 ] = weekday_dif [ 'VODKA' ][ 0 ] - weekday_dif [ 'VODKA' ][ 5 ] weekday_dif [ 'WHISKEY_CHANGE' ] = weekday_dif [ 'WHISKEY' ] - weekday_dif [ 'WHISKEY' ] . shift ( 1 ) weekday_dif [ 'WHISKEY_CHANGE' ][ 0 ] = weekday_dif [ 'WHISKEY' ][ 0 ] - weekday_dif [ 'WHISKEY' ][ 5 ] weekday_dif . drop ([ 'BRANDY' , 'GIN' , 'LIQUEUR' , 'MISC' , 'MIXED' , 'RUM' , 'TEQUILA' , 'VODKA' , 'WHISKEY' ], axis = 1 , inplace = True ) weekday_dif = pd . melt ( weekday_dif , id_vars = [ 'weekday' ]) weekday_dif . rename ( columns = { 'variable' : 'Alcohol Category' , 'value' : 'Change in Mililiters Sold Per Day' }, inplace = True ) /Users/nathanhall/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy app.launch_new_instance() /Users/nathanhall/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy /Users/nathanhall/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy /Users/nathanhall/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy /Users/nathanhall/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy /Users/nathanhall/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy /Users/nathanhall/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:15: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy /Users/nathanhall/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:17: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy /Users/nathanhall/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:19: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy sns . factorplot ( x = 'weekday' , y = 'Change in Mililiters Sold Per Day' , data = weekday_dif , hue = 'Alcohol Category' , size = 7 , kind = \"bar\" , palette = \"muted\" ) <seaborn.axisgrid.FacetGrid at 0x11ce60450> ANSWER: For the most part sales follow the expected weekend spike trend. One important exception to note is that on Thursdays Vodka is the best selling liqour instead of Whiskey. 4.4. Is there a correlation between number of food businesses and sales/consumption? #Create a dataframe from the ones made earlier to focus on the areas we're interested in. biz_per = pd . concat ([ df_city_agg [[ 'City' , 'Total Population' , 'Food Businesses' ]], df_cat_agg [[ 'ml_sold' ]]], axis = 1 ) #Create two new variables called price per ml and ml per person. These will let us see clearly where consumption #tracks with cost. biz_per [ 'Mililiters Sold Per Person' ] = biz_per [ 'ml_sold' ] / biz_per [ 'Total Population' ] biz_per [ 'Food Businesses Per 100 People' ] = biz_per [ 'Food Businesses' ] / ( biz_per [ 'Total Population' ] / 100 ) sns . lmplot ( x = \"Mililiters Sold Per Person\" , y = \"Food Businesses Per 100 People\" , data = biz_per , size = 7 ) <seaborn.axisgrid.FacetGrid at 0x11c270a50> ANSWER: Not enough information to determine correlation. Outliers would need to be addressed for any meaningful determination. 4.5. Where is the most demand? #Create a dataframe from the ones made earlier to focus on the areas we're interested in. demand = pd . concat ([ df_city_agg [[ 'Zip Code' , 'City' , 'Total Population' , 'Per Capita Income' , 'Food Businesses' ]], df_cat_agg [[ 'ml_sold' , 'Sale (Dollars)' ]]], axis = 1 ) #Filter to show only the top 20. demand = demand . sort_values ( 'Sale (Dollars)' , ascending = False ) . iloc [: 20 ] f , ax = plt . subplots ( figsize = ( 8 , 8 )) sns . barplot ( x = 'Sale (Dollars)' , y = \"City\" , data = demand ) <matplotlib.axes._subplots.AxesSubplot at 0x118c4f110> 4.6 . Where is the strongest demand? #Create a dataframe from the ones made earlier to focus on the areas we're interested in. demand_strength = pd . concat ([ df_city_agg [[ 'Zip Code' , 'City' , 'Total Population' , 'Per Capita Income' , 'Food Businesses' ]], df_cat_agg [[ 'ml_sold' , 'Sale (Dollars)' ]]], axis = 1 ) #Calculate a simple per capita demand in terms of ml_sold demand_strength [ 'per_capita_demand' ] = demand_strength [ 'ml_sold' ] / demand_strength [ 'Total Population' ] #Calculate a simple percent of total income spent demand_strength [ 'per_capita_sales' ] = demand_strength [ 'Sale (Dollars)' ] / demand_strength [ 'Total Population' ] demand_strength [ 'percent_of_income' ] = demand_strength [ 'per_capita_sales' ] / demand_strength [ 'Per Capita Income' ] #Combine the two metrics above to measure the demand of liqueur in that city simply by multiplying the two. demand_strength [ 'demand_score' ] = demand_strength [ 'per_capita_sales' ] * demand_strength [ 'percent_of_income' ] #Filter to show the top 20 cities. demand_strength_filtered = demand_strength . sort_values ( 'demand_score' , ascending = False ) . iloc [: 20 ] f , ax = plt . subplots ( figsize = ( 8 , 8 )) sns . barplot ( x = 'demand_score' , y = \"City\" , data = demand_strength_filtered ) <matplotlib.axes._subplots.AxesSubplot at 0x11cca5310> #A table to prove the numbers that I did not believe with my eyes. demand_table = demand_strength [[ 'Zip Code' , 'City' , 'Total Population' , 'per_capita_sales' , 'percent_of_income' , 'demand_score' ]] demand_table = demand_table . sort_values ( 'demand_score' , ascending = False ) demand_table . head () Zip Code City Total Population per_capita_sales percent_of_income demand_score 232 50163 MELCHER-DALLAS 228.0 701.242807 0.033308 23.357311 328 52166 ST LUCAS 156.0 523.275705 0.016314 8.536521 245 52314 MOUNT VERNON 6093.0 449.256936 0.015559 6.990088 369 50483 WESLEY 686.0 420.552259 0.013007 5.470083 25 50517 BANCROFT 1027.0 373.490029 0.014556 5.436486 4.7. Where is the most opportunity? #Create a dataframe from the ones made earlier to focus on the areas we're interested in. opportunity = pd . concat ([ demand_strength , df_store_agg [ 'Store Number' ], df_cat_agg [[ 'State Bottle Cost' ]]], axis = 1 ) #We don't do stores to population since we obviously know that some populations buy more or less than others. #Therefore just looking at mililiters solds is most sufficient. opportunity [ 'stores_to_ml_sold' ] = opportunity [ 'ml_sold' ] / opportunity [ 'Store Number' ] #Multiply the two variables we have so far to get a measure of opportunity. opportunity [ 'opportunity_score' ] = opportunity [ 'stores_to_ml_sold' ] * opportunity [ 'demand_score' ] #Filter to show the top 20. opportunity_filtered = opportunity . sort_values ( 'opportunity_score' , ascending = False ) . iloc [: 20 ] f , ax = plt . subplots ( figsize = ( 8 , 8 )) sns . barplot ( x = 'opportunity_score' , y = \"City\" , data = opportunity_filtered ) <matplotlib.axes._subplots.AxesSubplot at 0x11c219c90> opp_table = opportunity [[ 'Zip Code' , 'City' , 'Total Population' , 'per_capita_sales' , 'stores_to_ml_sold' , 'opportunity_score' ]] opp_table . head () Zip Code City Total Population per_capita_sales stores_to_ml_sold opportunity_score 245 52314.0 MOUNT VERNON 6093.0 449.256936 97575225.0 6.820594e+08 232 50163.0 MELCHER-DALLAS 228.0 701.242807 10847150.0 2.533603e+08 234 51351.0 MILFORD 4711.0 318.862127 52797375.0 1.912796e+08 25 50517.0 BANCROFT 1027.0 373.490029 26187728.0 1.423692e+08 369 50483.0 WESLEY 686.0 420.552259 20724875.0 1.133668e+08 Build your models Using scikit-learn or statsmodels, build the necessary models for your scenario. Evaluate model fit. from sklearn.preprocessing import StandardScaler from sklearn import linear_model from sklearn.metrics import r2_score from sklearn.linear_model import LassoCV import statsmodels.api as sm from sklearn.cross_validation import train_test_split #Lets build a quick correlation table comparison to see what we're dealing with. df_validate = opportunity [[ 'stores_to_ml_sold' , 'demand_score' , 'opportunity_score' , 'State Bottle Cost' , 'Sale (Dollars)' ]] df_validate [ 'ROI_profit' ] = opportunity [ 'Sale (Dollars)' ] - opportunity [ 'State Bottle Cost' ] df_validate . drop ([ 'State Bottle Cost' , 'Sale (Dollars)' ], axis = 1 , inplace = True ) df_validate . head () /Users/nathanhall/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy app.launch_new_instance() /Users/nathanhall/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy stores_to_ml_sold demand_score opportunity_score ROI_profit 0 3.560762e+06 0.041831 148950.252732 72948.42 1 1.638150e+06 0.051297 84032.201625 44815.50 2 1.060648e+07 0.090245 957183.537765 388173.47 3 2.853900e+06 0.018920 53994.939773 33770.55 4 4.074625e+06 0.013227 53894.443523 44881.12 # Compute the correlation matrix corr = df_validate . corr () # Generate a mask for the upper triangle mask = np . zeros_like ( corr , dtype = np . bool ) mask [ np . triu_indices_from ( mask )] = True # Set up the matplotlib figure f , ax = plt . subplots ( figsize = ( 8 , 8 )) # Generate a custom diverging colormap cmap = sns . diverging_palette ( 220 , 10 , as_cmap = True ) # Draw the heatmap with the mask and correct aspect ratio sns . heatmap ( corr , mask = mask , cmap = cmap , square = True , linewidths =. 5 , cbar_kws = { \"shrink\" : . 5 }, ax = ax , annot = True , fmt = '0.0' ) <matplotlib.axes._subplots.AxesSubplot at 0x11d0c2510> There is some collinearity it seems like with stores_to_ml_sold and opportunity_score. However, this is to be expected since opportunity score is made up of stores_to_ml_sold. Otherwise we have a pretty good start here. #Lets start trying to validate our findings. Is opportunity score correlated to sales. model_df1 = df_validate [[ 'stores_to_ml_sold' , 'demand_score' ]] target1 = df_validate [ 'ROI_profit' ] # Setting my X and y for modeling X1 = model_df1 y1 = target1 X1_train , X1_test , y1_train , y1_test = train_test_split ( X1 , y1 , test_size =. 2 , random_state = 99 ) # fit a model lm1 = linear_model . LinearRegression () model1 = lm1 . fit ( X1_train , y1_train ) predictions1 = lm1 . predict ( X1_test ) ## The line / model plt . scatter ( y1_test , predictions1 ) plt . xlabel ( \"True Values\" ) plt . ylabel ( \"Predictions\" ) print \"Score:\" , model1 . score ( X1_test , y1_test ) print \"R2 Score:\" , r2_score ( y1_test , predictions1 ) Score : 0.149334090592 R2 Score : 0.149334090592 Present the Results Click here to see the presentation of the above analysis. Further Research: Conduct further modeling without outliers. Look at further relationships between population density and ml_sold. Look at further relationships between population of families and ml_sold_per_capita. Look at further relationships between gender population and ml_sold_per_capita. Look at further relationships between children under 10 and ml_sold_per_capita.","tags":"Articles","loc":"http://www.instantinate.com/articles/analyzing_iowa_liqour_sales_project.html","title":"Analyzing Iowa Liquor Sales Project"},{"url":"http://www.instantinate.com/articles/finding_a_data_science_job_with_data_science.html","text":"PROCESS OF METHODS: SCRAPING - 1: Scrape jobs from Indeed.com. - 2: Scrape job descriptions from root job post. - 3: Scrape employer salary data from Payscale.com. FILTERING - 4: Filter down to job posts that have \"Data Scientist\" in the title. - 5: Get keywords from these \"Data Scientist\" job posts by using TFIDF vectorizer. - 6: Using the TFIDF scores as weights, analyze ALL job posts. - 7: Use the TFIDF scores for each job post to filter to the most \"Data Sciencey\" jobs. - 8: Use the TFIDF score for each keyword in job titles as well. WEIGHTING - 9: Extract salary data from job post text for all \"Data Sciencey\" jobs. - 10: For each company, calculate the percentage increase from the median salary to the \"Data Science\" salary. - TEST: Calculate the percentage increase for keywords in the job titles. (Like Chief, Senior, etc.) ESTIMATING - 11: For job posts without salary data, use the Data Science salary weight to estimate a salary for each job post relative to the company. MODELING - 12: Run all of the features through a Random Forest model to find what most influences a job being above or below the median salary. - 13: Run all of the features through a Logistic Regression model. Compare coefficients to feature importance from Random Forest and also adjust the threshold to minimize False Positives. VISUALIZATIONS - Several visuals to demonstrate what was done. from bs4 import BeautifulSoup from time import sleep import random import requests import urllib from readability.readability import Document from html_table_parser import parser_functions as parse import re from selenium import webdriver from selenium.webdriver.common.keys import Keys chrome_options = webdriver . ChromeOptions () chrome_options . add_argument ( \"--incognito\" ) import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline from sklearn.preprocessing import LabelEncoder from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import RandomForestRegressor from sklearn.cross_validation import train_test_split from sklearn.cross_validation import cross_val_score from sklearn.metrics import accuracy_score , precision_score , confusion_matrix , classification_report from sklearn import svm , linear_model from sklearn.grid_search import GridSearchCV SCRAPING 1: Scrape jobs from Indeed.com Below is the indeed scraper that was run to get a dataframe that has title, company name, referral link, salary range/description, and the location of the job posts. #Dataframe we will store the collected data. indeed_jobs = pd . DataFrame ( columns = [ \"title\" , \"company\" , \"url\" , \"srange\" , \"location\" ]) #Locations we're going to search through. locations = [ 'Washington, DC' , 'Herndon, VA' , 'Atlanta, GA' , 'New York, NY' ] #Function to run for the search. for i in locations : #open Chrome driver = webdriver . Chrome ( chrome_options = chrome_options ) #navigate to indeed.com driver . get ( \"https://www.indeed.com/\" ) # find the search position elem = driver . find_element_by_name ( \"q\" ) # type in data scientist elem . send_keys ( \"Data Scientist\" ) # find the location position elem_loc = driver . find_element_by_name ( \"l\" ) # clear autofilled data elem_loc . clear () # type in the next location in my list. elem_loc . send_keys ( i ) # click search elem . send_keys ( Keys . RETURN ) #close the popup that appears. #driver.find_element_by_xpath(\".//div[@id='prime-popover-x']/button\").click() #Filter the search results to only have full-time jobs. driver . find_element_by_link_text ( 'Full-time' ) . click () #The number of jobs returned from the search. Divide by 10 to get how many times you should click the next button. end = int ( BeautifulSoup ( driver . page_source ) . find ( 'div' , { 'id' : 'searchCount' }) . text . partition ( 'of ' )[ - 1 ] . replace ( ',' , '' )) / 12 #Indeed only lets you view the first 1,000 jobs of each search. If more than 1,000 jobs are returned we'll #adjust the end number to move on after its viewed all that it is allowed. if end > 100 : end = 100 #Function to return the elements of the search. for i in xrange ( 1 , end ): soup = BeautifulSoup ( driver . page_source , 'html.parser' , from_encoding = \"utf-8\" ) try : driver . find_element_by_link_text ( 'No, thanks' ) . click () except : for entry in soup . find_all ( 'div' , { 'class' : ' row result' }): title = entry . find ( 'a' ) . text . encode ( 'utf-8' ) company = entry . find ( 'span' , { 'itemprop' : 'name' }) . text . encode ( 'utf-8' ) location = entry . find ( 'span' , { 'itemprop' : 'jobLocation' }) . text . encode ( 'utf-8' ) srange = [] try : salary = entry . find ( 'td' , { 'class' : 'snip' }) . text . encode ( 'utf-8' ) srange . append ( salary ) except : srange . append ( np . nan ) #description = entry.find('span', {'itemprop':'description'}).text url = \"http://indeed.com\" + entry . find ( 'a' ) . get ( 'href' ) indeed_jobs . loc [ len ( indeed_jobs )] = [ title , company , url , srange , location ] sleep ( float ( round ( random . uniform ( 1.5 , 5.9 ), 1 ))) #sleep before next step. driver . find_element_by_link_text ( 'Next »' ) . click () #click the next button for search results. print indeed_jobs . shape #close the driver. driver . close () #Remove any duplicated rows. print \"Shape before duplicates removed =\" , indeed_jobs . shape indeed_jobs = indeed_jobs . drop_duplicates ([ 'url' ]) print \"Shape after duplicates removed =\" , indeed_jobs . shape #backup the scraped jobs to csv. indeed_jobs . to_csv ( 'data/FULL_TIME_INDEED_SCRAPED_DATA_RAW.csv' ) 2: Scrape job descriptions from root job post. #Bring in the scraped csv from the above scraper. scraper1_data = pd . read_csv ( 'data/FULL_TIME_INDEED_SCRAPED_DATA_RAW.csv' ) scraper2 = pd . DataFrame ( scraper1_data ) scraper2 . drop ([ 'Unnamed: 0' ], axis = 1 , inplace = True ) print scraper2 . shape scraper2 . head ( 2 ) (2132, 5) title company url srange location 0 Forensic Scientist (Crime Scene) \\n Dept of Forensic Sciences http://indeed.com/rc/clk?jk=70192ba18fa061a8&f... ['\\n$61,491 - $79,275 a year\\n\\n\\nAnd knowledg... Washington, DC 1 Data Scientist \\n Excella Consulting http://indeed.com/rc/clk?jk=42cb30526e07b4ec&f... ['\\n\\n\\nUsing machine learning and data mining... Washington, DC 20006 (Foggy Bottom area) Below is the scraper that we used to get the job descriptions and resolve any encoding/formatting errors that arose from those descriptions. #A function we will use later to strip out the html tags. def striphtml ( data ): p = re . compile ( r'<.*?>' ) return p . sub ( '' , data ) #empty list to hold the job descriptions. desc = [] for i in scraper2 [ 'url' ]: try : html = urllib . urlopen ( i ) . read () readable = Document ( html ) . summary () desc . append ( str ( readable )) except : desc . append ( np . nan ) print len ( desc ) #sleep(float(round(random.uniform(2.5, 5.2),1))) #sleep before next step. #set scraped list to a column in our dataframe. scraper2 [ 'desc' ] = desc #unicode decoding attempt. desc_unicode = [] for i in scraper2 [ 'desc' ]: try : text = i . encode ( 'utf-8' , 'replace' ) desc_unicode . append ( text ) except : desc_unicode . append ( np . nan ) scraper2 [ 'desc' ] = desc_unicode print scraper2 [ 'desc' ] . isnull () . sum () #format all the column values to strings. scraper2 [ 'desc' ] = [ str ( job ) for job in scraper2 [ 'desc' ]] #strip out the html tags. scraper2 [ 'desc' ] = [ striphtml ( job ) for job in scraper2 [ 'desc' ]] #remove the formatting annotations. scraper2 [ 'desc' ] = [ i . translate ( None , ' \\t\\n ' ) for i in scraper2 [ 'desc' ]] scraper2 [ 'company' ] = [ co . translate ( None , ' \\t\\n ' ) for co in scraper2 [ 'company' ]] #remove any this formatting errors that doesn't seem to go away. scraper2 [ 'desc' ] = [ x . replace ( '&#13;' , ' ' ) for x in scraper2 [ 'desc' ]] #check again for duplicates. print \"Shape before duplicates removed =\" , scraper2 . shape scraper2 = scraper2 . drop_duplicates ([ 'url' ]) print \"Shape after duplicates removed =\" , scraper2 . shape #backup the scraped job descriptions to csv. scraper2 . to_csv ( 'data/SCRAPED_JOB_DESCRIPTION_DATA_RAW.csv' ) 3: Scrape salary and benefits data for employer from Payscale.com #Bring in the scraped csv from the above scraper. scraper2_data = pd . read_csv ( 'data/SCRAPED_JOB_DESCRIPTION_DATA_RAW.csv' ) indeed = pd . DataFrame ( scraper2_data ) indeed . drop ([ 'Unnamed: 0' ], axis = 1 , inplace = True ) print indeed . shape indeed . head ( 2 ) (2132, 6) title company url srange location desc 0 Forensic Scientist (Crime Scene) Dept of Forensic Sciences http://indeed.com/rc/clk?jk=70192ba18fa061a8&f... ['\\n$61,491 - $79,275 a year\\n\\n\\nAnd knowledg... Washington, DC Forensic Scientist (Crime Scene)CS-401-11Caree... 1 Data Scientist Excella Consulting http://indeed.com/rc/clk?jk=42cb30526e07b4ec&f... ['\\n\\n\\nUsing machine learning and data mining... Washington, DC 20006 (Foggy Bottom area) Please Enable Cookies to ContinuePlease enabl... df_company = indeed . groupby ( 'company' ) . count () df_company = df_company . reset_index () print 'Number of companies =' , len ( df_company [ 'company' ]) df_company . head () Number of companies = 946 company title url srange location desc 0 1010data 2 2 2 2 2 1 2U 2 2 2 2 2 2 3M 1 1 1 1 1 3 ADP 21 21 21 21 21 4 AECOM 1 1 1 1 0 Below is the code that was used to scrape company salary data from payscale.com. #Empty dataframe to hold our results. ds_salary = pd . DataFrame () for i in df_company [ 'company' ]: try : #open Chrome driver = webdriver . Chrome ( chrome_options = chrome_options ) #navigate to payscale.com driver . get ( \"http://www.payscale.com/research/US/Country=United_States/Salary\" ) #find the dropdown nav driver . find_element_by_xpath ( \".//a[@id='rcSearchDropDownBtn']\" ) . click () #find the employer option in the dropdown driver . find_element_by_xpath ( \".//li[@title='Employers']\" ) . click () #find the search bar elem = driver . find_element_by_name ( 'str' ) #pass in the company name from the indeed post. elem . send_keys ( i ) #wait two seconds. sleep ( 3.0 ) #click on the first result from the suggested search options. driver . find_element_by_xpath ( \".//li[@class='ui-menu-item'][1]\" ) . click () #click on the years of experience tab on that companies page. driver . find_element_by_link_text ( 'By Years Experience' ) . click () #wait for the whole page to load. sleep ( 5.0 ) #soupify the page. soup = BeautifulSoup ( driver . page_source , 'html.parser' , from_encoding = \"utf-8\" ) #find the first table with the salaries by years of experience. table = soup . find_all ( 'table' , { 'class' : 'tlf f11 w600' })[ 0 ] #parse the table into a dataframe. twodim_table = parse . make2d ( table ) df_scrape_test = pd . DataFrame ( twodim_table ) #drop the top and bottom rows in the dataframe to remove the header and footer from the html page. df_scrape_test . drop ( 0 , axis = 0 , inplace = True ) df_scrape_test = df_scrape_test [: - 1 ] #remove the number of salaries data from the first column. df_scrape_test [ 0 ] = df_scrape_test [ 0 ] . apply ( lambda x : x . split ( 'year' )[ 0 ]) #return only the first two columns of the dataframe. df_scrape_test . set_index ( 0 , inplace = True ) df_scrape_test = df_scrape_test . rename_axis ( None ) df_scrape_test . drop ([ 2 , 3 , 4 , 5 ], axis = 1 , inplace = True ) df_scrape_test = df_scrape_test . transpose () #Add in the company name so that you can compare against the data from Indeed. df_scrape_test [ 'salary_company_source' ] = soup . find ( 'h1' ) . text ds_salary = ds_salary . append ( df_scrape_test ) except : s2 = pd . Series ([ np . nan , np . nan , np . nan , np . nan , np . nan ], index = [ '1-4 ' , '5-9 ' , '10-19 ' , '20 ' , 'Less than 1 ' ]) ds_salary = ds_salary . append ( s2 , ignore_index = True ) driver . close () print \"scraped %d out of %d \" % ( len ( ds_salary ), len ( df_company [ 'company' ])) #since you're running this while you go to bed. Export it as a csv to check in the morning. ds_salary . to_csv ( 'data/PAYSCALE_SCRAPED_COMPANY_SALARY_DATA_RAW.csv' ) Below is the code that was used to scrape company benefits data. ds_benefits = pd . DataFrame () for i in df_company [ 'company' ]: try : #open Chrome driver = webdriver . Chrome ( chrome_options = chrome_options ) #navigate to payscale.com driver . get ( \"http://www.payscale.com/research/US/Country=United_States/Salary\" ) #find the dropdown nav driver . find_element_by_xpath ( \".//a[@id='rcSearchDropDownBtn']\" ) . click () #find the employer option in the dropdown driver . find_element_by_xpath ( \".//li[@title='Employers']\" ) . click () #find the search bar elem = driver . find_element_by_name ( 'str' ) #pass in the company name from the indeed post. elem . send_keys ( i ) #wait two seconds. sleep ( 3.0 ) #click on the first result from the suggested search options. driver . find_element_by_xpath ( \".//li[@class='ui-menu-item'][1]\" ) . click () #soupify the page. soup = BeautifulSoup ( driver . page_source , 'html.parser' , from_encoding = \"utf-8\" ) #find the first table with the salaries by years of experience. table = soup . find_all ( 'table' , { 'class' : 'table table-condensed no-margin' })[ 0 ] #parse the table into a dataframe. twodim_table = parse . make2d ( table ) df_scrape_test = pd . DataFrame ( twodim_table ) #drop the rows and columns we don't need df_scrape_test = df_scrape_test [[ 0 , 1 ]] df_scrape_test . drop ([ 0 , 2 , 3 , 4 , 7 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 ], inplace = True ) df_scrape_test = df_scrape_test . transpose () df_scrape_test . columns = df_scrape_test . loc [ 0 ] df_scrape_test . drop ( 0 , inplace = True ) df_scrape_test [ 'benefits_company_source' ] = soup . find ( 'h1' ) . text ds_benefits = ds_benefits . append ( df_scrape_test ) except : s2 = pd . Series ([ np . nan , np . nan , np . nan , np . nan ], index = [ 'Job Satisfaction' , 'Work Stress' , 'Job Flexibility' , 'Vacation Weeks' ]) ds_benefits = ds_benefits . append ( s2 , ignore_index = True ) driver . close () print \"scraped %d out of %d \" % ( len ( ds_benefits ), len ( df_company [ 'company' ])) #since you're running this while you go to bed. Export it as a csv to check in the morning. ds_benefits . to_csv ( 'data/PAYSCALE_SCRAPED_BENEFITS_DATA_RAW.csv' ) Run the scraper tonight. median_salary = [] for i in df_company [ 'company' ]: try : #open Chrome driver = webdriver . Chrome ( chrome_options = chrome_options ) #navigate to payscale.com driver . get ( \"http://www.payscale.com/research/US/Country=United_States/Salary\" ) #find the dropdown nav driver . find_element_by_xpath ( \".//a[@id='rcSearchDropDownBtn']\" ) . click () #find the employer option in the dropdown driver . find_element_by_xpath ( \".//li[@title='Employers']\" ) . click () #find the search bar elem = driver . find_element_by_name ( 'str' ) #pass in the company name from the indeed post. elem . send_keys ( i ) #wait two seconds. sleep ( 3.0 ) #click on the first result from the suggested search options. driver . find_element_by_xpath ( \".//li[@class='ui-menu-item'][1]\" ) . click () #click on the by job tab on that companies page. driver . find_element_by_link_text ( 'By Job' ) . click () #get the url for the jobs page. url_source = driver . current_url #adjust it to have the data_scientist job. url = url_source [: - 6 ] + 'Job/Data-Scientist' #Go to that url you just retrieved. driver . get ( url ) #wait for the whole page to load. sleep ( 5.0 ) #soupify the page. soup = BeautifulSoup ( driver . page_source , 'html.parser' , from_encoding = \"utf-8\" ) #empty list to hold median salary so we can clean up the data. med_salary = [] #get the median salary from the chart. med_salary . append ( soup . find ( 'div' , { 'class' : 'you_label' }) . text ) #encode it properly #med_salary = [job.encode('ascii','ignore') for job in med_salary] #clean/format the returned text. for i in med_salary : head , sep , tail = i . partition ( 'MEDIAN:' ) tail = tail . translate ( None , ' \\n ' ) tail = tail . translate ( None , ' ' ) #append it to the running list. median_salary . append ( tail ) except : median_salary . append ( np . nan ) print median_salary [ - 1 ] driver . close () #Backup the scraper data to a csv. ds_salary . to_csv ( 'data/PAYSCALE_SCRAPED_DATASCIENCE_COMPANY_SALARY_DATA_RAW.csv' ) #Bring in the scraped csv from the above scraper. scraper3_data = pd . read_csv ( 'data/PAYSCALE_SCRAPED_COMPANY_SALARY_DATA_RAW.csv' ) payscale_scraped_a = pd . DataFrame ( scraper3_data ) payscale_scraped_a . drop ([ 'Unnamed: 0' ], axis = 1 , inplace = True ) print payscale_scraped_a . shape payscale_scraped_a . head ( 2 ) (946, 6) 1-4 10-19 20 5-9 Less than 1 salary_company_source 0 NaN NaN NaN NaN NaN NaN 1 NaN NaN NaN NaN NaN NaN #Bring in the scraped csv from the above scraper. scraper4_data = pd . read_csv ( 'data/PAYSCALE_SCRAPED_BENEFITS_DATA_RAW.csv' ) payscale_scraped_b = pd . DataFrame ( scraper4_data ) payscale_scraped_b . drop ([ 'Unnamed: 0' ], axis = 1 , inplace = True ) print payscale_scraped_b . shape payscale_scraped_b . head ( 2 ) (946, 5) Job Flexibility Job Satisfaction Vacation Weeks Work Stress benefits_company_source 0 NaN NaN NaN NaN NaN 1 NaN NaN NaN NaN NaN #Concat the two scraped dataframes from Payscale.com df_payscale_scraped = pd . concat ([ payscale_scraped_a , payscale_scraped_b ], axis = 1 ) print df_payscale_scraped . shape df_payscale_scraped . head ( 2 ) (946, 11) 1-4 10-19 20 5-9 Less than 1 salary_company_source Job Flexibility Job Satisfaction Vacation Weeks Work Stress benefits_company_source 0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN #Add in the two csv's we scraped above to the company dataframe for us to check and make sure we scraped #the right companies when we did the search. #First cleaning up the company source columns so we can better evaluate the search results. df_company_scraped = pd . concat ([ df_payscale_scraped , df_company ], axis = 1 ) df_company_scraped [ 'salary_company_source' ] = df_company_scraped [ 'salary_company_source' ] . str . replace ( 'Average Salary for ' , '' ) df_company_scraped [ 'salary_company_source' ] = df_company_scraped [ 'salary_company_source' ] . str . replace ( ' Employees' , '' ) df_company_scraped [ 'benefits_company_source' ] = df_company_scraped [ 'benefits_company_source' ] . str . replace ( 'Average Salary for ' , '' ) df_company_scraped [ 'benefits_company_source' ] = df_company_scraped [ 'benefits_company_source' ] . str . replace ( ' Employees' , '' ) df_company_scraped . head () 1-4 10-19 20 5-9 Less than 1 salary_company_source Job Flexibility Job Satisfaction Vacation Weeks Work Stress benefits_company_source company title url srange location desc 0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1010data 2 2 2 2 2 1 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2U 2 2 2 2 2 2 $66,892 $95,353 $108,094 $81,724 $58,687 3M Company Data Not Available Extremely satisfied 2.7 Weeks Stressful 3M Company 3M 1 1 1 1 1 3 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ADP 21 21 21 21 21 4 $59,207 $89,051 $112,775 $70,423 $53,903 Aecom Corporation NaN NaN NaN NaN NaN AECOM 1 1 1 1 0 #Export it as a csv to check we got the right companies. df_company_scraped . to_csv ( 'data/PAYSCALE_SCRAPED_DATA_ALL_RAW.csv' ) #Bring in the cleaned CSV for companies scraped to join to our original dataframe. payscale_clean = pd . read_csv ( 'data/PAYSCALE_SCRAPED_DATA_ALL_CLEAN.csv' ) payscale = pd . DataFrame ( payscale_clean ) payscale . drop ([ 'Unnamed: 0' ], axis = 1 , inplace = True ) #Rename the column headers since excel screws them up. payscale . rename ( columns = { '4-Jan' : '1-4' , '19-Oct' : '10-19' , '9-May' : '5-9' }, inplace = True ) print payscale . shape payscale . head ( 2 ) (946, 10) company 1-4 10-19 20 5-9 Less than 1 Job Flexibility Job Satisfaction Vacation Weeks Work Stress 0 1010data NaN NaN NaN $91,233 NaN NaN NaN NaN NaN 1 2U NaN NaN NaN NaN NaN NaN NaN NaN NaN #Take the spaces at the begining of the company out so that you can have more accurate data. #company_test['company'] = [i.strip() for i in company_test['company']] #Merge the two dataframes joining on 'company'. I know from running this before that the pay bracket of 1-4 was #the most accurate for building an estimator for a data_scientists salary so I am going to only import that #column here. df = pd . merge ( indeed , payscale [[ 'company' , '1-4' , 'Job Flexibility' , 'Job Satisfaction' , 'Vacation Weeks' , 'Work Stress' ]], how = 'left' , on = 'company' , copy = False , sort = False ) print df . shape df . head () (2132, 11) title company url srange location desc 1-4 Job Flexibility Job Satisfaction Vacation Weeks Work Stress 0 Forensic Scientist (Crime Scene) Dept of Forensic Sciences http://indeed.com/rc/clk?jk=70192ba18fa061a8&f... ['\\n$61,491 - $79,275 a year\\n\\n\\nAnd knowledg... Washington, DC Forensic Scientist (Crime Scene)CS-401-11Caree... NaN NaN NaN NaN NaN 1 Data Scientist Excella Consulting http://indeed.com/rc/clk?jk=42cb30526e07b4ec&f... ['\\n\\n\\nUsing machine learning and data mining... Washington, DC 20006 (Foggy Bottom area) Please Enable Cookies to ContinuePlease enabl... NaN NaN NaN NaN NaN 2 Data Scientist Booz Allen Hamilton http://indeed.com/rc/clk?jk=33f586dafa3deb4c&f... ['\\n\\n\\nApply expertise in advanced analytics,... Washington, DC Booz Allen Hamilt... $64,587 Extremely flexible Highly satisfied 2.8 Weeks Stressful 3 Data Scientist Vox Media http://indeed.com/rc/clk?jk=0a9009fd2170cf66&f... ['\\n\\n\\nData Scientists are responsible for de... Washington, DC U.S. Equal Opportunity Employment Informatio... NaN NaN NaN NaN NaN 4 Data Scientist/Engineer ICF International http://indeed.com/rc/clk?jk=d29754cbfaf00024&f... ['\\n\\n\\nSpecifically, it provides psychologica... Silver Spring, MD Copyright 1992-2016 ICF International, Inc. Al... $55,720 NaN NaN NaN NaN FILTERING We know that the median data scientist job salary in Wasington DC is 95,000. However when we scrape all the data scientist jobs posted by Indeed.com we get a median of 75,000. You can interprete that as there are not high paying data scientist jobs available or there are jobs that are not TRULY data scientist jobs being returned by the search. The assumption I chose was the latter because following that route would result in the cleanest data set possible to make a prediction. So below we are going to build a filter to see how many Data Scientist jobs are ACTUALLY being returned by the indeed.com search. 5: Filter jobs to only ones that have \"Data Scientist\" in the title. #def uppercase(x): #return(x.upper()) #map_dict = { #'title' : 'new_title' #} #df['new_title'] = df['title'].map(map_dict) #df['title'].apply(uppercase()) #Convert the title column to uppercase so we can search easily. df [ 'title' ] . apply ( lambda x : x . upper (), df [ 'title' ]) #Convert all the data here to strings to resolve a float error. df [ 'desc' ] = df [ 'desc' ] . astype ( str ) #Convert the job descriptions to all uppercase words so that we are comparing apples to apples. df [ 'desc' ] = map ( lambda x : x . upper (), df [ 'desc' ]) #create a desc length column that we will use later. df [ 'desc_len' ] = [ len ( i ) for i in df [ 'desc' ]] ds_in_title = [] #Find the job titles that match the parameters below and put the results into a new column. for i in df [ 'title' ]: x = 0 if 'DATA SCIENTIST' in i : x = 1 if x >= 1 : ds_in_title . append ( 1 ) else : ds_in_title . append ( np . nan ) #Make a column with a 1 if its a match to the above search. df [ 'ds_in_title' ] = ds_in_title #Now filter this dataframe by only jobs that have data scientist in the title. df1 = df [ df [ 'ds_in_title' ] >= 1.0 ] #Filter out job descriptions that are below 800 characters so that we get the best data possible here. df1 = df1 [ df1 [ 'desc_len' ] >= 800 ] df1 . shape (277, 13) 6: Get keywords from these posts by using TFIDF vectorizer. #Declare the X target as the job descriptions for only jobs with Data Scientist in the title. X = df1 [ 'desc' ] #Get the most important keywords across jobs that have data scientist in the title. We'll use these keywords #to find other data science jobs that might not have data scientist in the title. #You can pass a function into analyzer that can parse sentences into different parts of speech. #Look into stemming for the words that are passed into the TFIDFvect, \"WALK vs. WALKS\", tokenizer. tfidfv = TfidfVectorizer ( analyzer = 'word' , ngram_range = ( 1 , 2 ), min_df = 0 , stop_words = 'english' , max_features = 5000 ) vect_model = tfidfv . fit_transform ( X ) dense = vect_model . todense () #Below is the list of most important words and their tfidf scores for importance to all the jobs that had #data scientist in the title. We'll use these keyword weights to then get a weighted score for whether or not #a certain job description is likely to be a data scientist job or not. dense_test = pd . DataFrame ( dense , columns = [ tfidfv . get_feature_names ()]) dense_test = dense_test . transpose () dense_test [ 'mean_tfidf' ] = dense_test . mean ( axis = 1 ) dense_test = dense_test [[ 'mean_tfidf' ]] dense_test = dense_test . reset_index () dense_test = dense_test . sort_values ( 'mean_tfidf' , ascending = False ) dense_test . head ( 20 ) index mean_tfidf 1117 data 0.125944 1782 experience 0.055743 4921 work 0.031810 311 analytics 0.031547 273 analysis 0.031239 622 business 0.029713 3946 science 0.029436 4503 team 0.027925 2635 learning 0.027859 4144 skills 0.027441 3066 new 0.027022 4310 statistical 0.026229 2752 machine 0.024288 3972 scientist 0.023858 82 ability 0.023826 2753 machine learning 0.023634 4200 solutions 0.023620 1166 data scientist 0.023415 1164 data science 0.021775 4740 using 0.021517 7: Using the TFIDF scores as weights analyze ALL job descriptions. #We have 5,000 words in the TFIDF dataframe which is WAY more than we need. So lets filter that to be fewer #keywords so its easier to track. ds_keywords = dense_test [ dense_test [ 'mean_tfidf' ] > 0.018 ] ds_keywords . shape (38, 2) #Now lets format this dataframe into something that we can use to search through all our job descriptions. ds_key_test = ds_keywords . transpose () ds_key_test . columns = ds_key_test . iloc [ 0 ] ds_key_test = ds_key_test . reset_index () ds_key_test . drop ( 0 , inplace = True ) ds_key_test . drop ( 'index' , axis = 1 , inplace = True ) ds_key_test . columns . name = None ds_key_test . columns = map ( lambda x : x . upper (), ds_key_test . columns ) ds_key_test DATA EXPERIENCE WORK ANALYTICS ANALYSIS BUSINESS SCIENCE TEAM LEARNING SKILLS ... MODELS INCLUDING TECHNOLOGY JOB ALGORITHMS PROBLEMS STRONG LARGE KNOWLEDGE METHODS 1 0.125944 0.0557435 0.0318102 0.0315467 0.0312387 0.0297132 0.0294358 0.0279249 0.0278588 0.0274412 ... 0.0201235 0.0199092 0.0195241 0.0194687 0.01901 0.0188275 0.0187724 0.0187513 0.0182813 0.0181009 1 rows × 38 columns #Split up the strings of each job description so we can search through every word. df [ 'desc_test' ] = df [ 'desc' ] . str . split () #All that work for this sweet little search engine filtering thingy. df_test = pd . DataFrame ( columns = ds_key_test . columns ) for x in df [ 'desc_test' ]: tl = [] try : for i in ds_key_test . columns : if i in x : tl . append ( ds_key_test [ i ] . loc [ 1 ]) else : tl . append ( np . nan ) except : tl . append ( np . nan ) append_test = dict ( zip ( list ( ds_key_test . columns ), tl )) df_test = df_test . append ( append_test , ignore_index = True ) /Users/nathanhall/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:7: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal 8: Sum the TFIDF scores for each post and filter to the most \"Data Sciency\" jobs. #Take the above resulting dataframe and sum up each job descprtion \"Data Sciency\" score. df_test [ 'likelihood' ] = df_test . sum ( axis = 1 ) df_test . shape (2132, 39) #Repeat the keyword analysis on the resulting job titles. #Declare the X target as the titles for the jobs that were returned from our data scientist filter. X2 = df [ 'title' ] #Get the most important keywords for these titles. This will serve as a better measure of the titles #than the VERY many types that are in that column already. tfidfv2 = TfidfVectorizer ( analyzer = 'word' , ngram_range = ( 1 , 2 ), min_df = 0 , stop_words = 'english' , max_features = 5000 ) vect_model2 = tfidfv2 . fit_transform ( X2 ) dense2 = vect_model2 . todense () #Below is the list of most important words and their tfidf scores for importance to all the jobs that had #data scientist in the title. We'll use these keyword weights to then get a weighted score for whether or not #a certain job description is likely to be a data scientist job or not. dense_test2 = pd . DataFrame ( dense2 , columns = [ tfidfv2 . get_feature_names ()]) dense_test2 = dense_test2 . transpose () dense_test2 [ 'mean_tfidf' ] = dense_test2 . mean ( axis = 1 ) dense_test2 = dense_test2 [[ 'mean_tfidf' ]] dense_test2 = dense_test2 . reset_index () dense_test2 = dense_test2 . sort_values ( 'mean_tfidf' , ascending = False ) dense_test2 . head ( 15 ) index mean_tfidf 3343 scientist 0.082106 992 data 0.077585 1035 data scientist 0.069339 3119 research 0.049677 160 analyst 0.049665 3567 senior 0.034015 3122 research analyst 0.033665 1346 engineer 0.029786 2297 manager 0.017995 3171 research scientist 0.016047 433 associate 0.015385 3586 senior data 0.015095 3809 sr 0.014920 3722 software 0.014719 3024 quantitative 0.012095 #We have 5,000 words in the TFIDF dataframe which is WAY more than we need. So lets filter that to be fewer #keywords so its easier to track. We should be pretty strict on this since there shouldn't be that much #variation. ds_keywords2 = dense_test2 [ dense_test2 [ 'mean_tfidf' ] > 0.018 ] ds_keywords2 . shape (8, 2) #Now lets format this dataframe into something that we can use to search through all our job titles. ds_key_test2 = ds_keywords2 . transpose () ds_key_test2 . columns = ds_key_test2 . iloc [ 0 ] ds_key_test2 = ds_key_test2 . reset_index () ds_key_test2 . drop ( 0 , inplace = True ) ds_key_test2 . drop ( 'index' , axis = 1 , inplace = True ) ds_key_test2 . columns . name = None ds_key_test2 . columns = map ( lambda x : x . upper (), ds_key_test2 . columns ) ds_key_test2 SCIENTIST DATA DATA SCIENTIST RESEARCH ANALYST SENIOR RESEARCH ANALYST ENGINEER 1 0.0821064 0.0775845 0.0693389 0.049677 0.0496651 0.0340151 0.0336652 0.0297859 #Split up the strings of each job description so we can search through every word. df [ 'title_test' ] = df [ 'title' ] . str . split () #All that work for this sweet little search engine filtering thingy. df_test2 = pd . DataFrame ( columns = ds_key_test2 . columns ) for x in df [ 'title_test' ]: t2 = [] try : for i in ds_key_test2 . columns : if i in x : t2 . append ( ds_key_test2 [ i ] . loc [ 1 ]) else : t2 . append ( np . nan ) except : t2 . append ( np . nan ) append_test2 = dict ( zip ( list ( ds_key_test2 . columns ), t2 )) df_test2 = df_test2 . append ( append_test2 , ignore_index = True ) /Users/nathanhall/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:7: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal #Take the above resulting dataframe and sum up each job title \"Data Sciency\" score. df_test2 [ 'likelihood' ] = df_test2 . sum ( axis = 1 ) df_test2 . columns = [ str ( col ) + '_title' for col in df_test2 . columns ] df_test . columns = [ str ( col ) + '_keyword' for col in df_test . columns ] #Concapt the resulting dataframes together df_keywords = pd . concat ([ df_test , df_test2 ], axis = 1 ) print df_keywords . shape (2132, 48) #Concat that score to the original dataframe. df = pd . concat ([ df , df_keywords ], axis = 1 ) #Create a new dataframe that now filters our results to the most \"Data Sciency\" jobs. print \"df Shape Before Filter =\" , df . shape #This threshold should remain near the same to catch the most \"data sciencey\" jobs. df = df [ df [ 'likelihood_keyword' ] > 0.52 ] print \"df Shape After Filter =\" , df . shape df . sort_values ( 'likelihood_keyword' , ascending = False , inplace = True ) df . head ( 5 ) df Shape Before Filter = (2132, 63) df Shape After Filter = (472, 63) title company url srange location desc 1-4 Job Flexibility Job Satisfaction Vacation Weeks ... likelihood_keyword SCIENTIST_title DATA_title DATA SCIENTIST_title RESEARCH_title ANALYST_title SENIOR_title RESEARCH ANALYST_title ENGINEER_title likelihood_title 1172 STATISTICAL CONSULTANT Equifax http://indeed.com/rc/clk?jk=e3db711f65629c93&f... ['\\n\\n\\nManipulate large data sets, integrate ... Alpharetta, GA STATISTICAL CONSULTANT APPLY JOB ... $60,404 Data Not Available Extremely satisfied 2.7 Weeks ... 0.904738 NaN NaN NaN NaN NaN NaN NaN NaN 0.000000 2016 BUSINESS ANALYTICS MANAGER Medidata Solutions http://indeed.com/rc/clk?jk=eb009ead7e4f2013&f... ['\\n\\n\\nManage data science projects involving... New York, NY 10003 (Greenwich Village area) OUR IN... $52,118 NaN NaN NaN ... 0.835336 NaN NaN NaN NaN NaN NaN NaN NaN 0.000000 29 DATA SCIENTIST Ukpeagvik Iñupiat Corporation/Bowhead Fami... http://indeed.com/rc/clk?jk=65885ab70c3dbd8c&f... ['\\n\\n\\nCrashes, The selected candidate will w... Washington, DC TRACKING CODE DTNH22-16-1780-F JOB CODE... NaN NaN NaN NaN ... 0.835324 0.082106 0.077585 NaN NaN NaN NaN NaN NaN 0.159691 927 DATA SCIENTIST ( MACHINE LEARNING) IHS Markit http://indeed.com/rc/clk?jk=493df6b925152ae3&f... ['\\n\\n\\nMachine learning, data mining, quantit... Washington, DC ABOUT IHS MARKIT IHS MARKIT HARNESSES DEEP SO... NaN NaN NaN NaN ... 0.828642 0.082106 0.077585 NaN NaN NaN NaN NaN NaN 0.159691 1072 DATA SCIENTIST Cotiviti http://indeed.com/rc/clk?jk=c95e60155151f4e2&f... ['\\n\\n\\nThis is a pioneering data scientist wh... Atlanta, GA REQUISITION NUMBER 16-0948 TITLE DATA S... NaN NaN NaN NaN ... 0.818920 0.082106 0.077585 NaN NaN NaN NaN NaN NaN 0.159691 5 rows × 63 columns WEIGHTING 9: Extract salary data from job post text for ALL \"Data Sciency\" jobs. #format the srange column so you can extract the salary data. df [ 'srange' ] = df [ 'srange' ] . str . replace ( '[' , '' ) df [ 'srange' ] = df [ 'srange' ] . str . replace ( ']' , '' ) df [ 'srange' ] = df [ 'srange' ] . str . replace ( r\" \\\\ n\" , ' ' ) #exctract salary data that matches the following formats. df [ 'srange1' ] = df [ 'srange' ] . str . extract ( '(.\\d\\d,\\d\\d\\d - .\\d\\d,\\d\\d\\d)' , expand = True ) df [ 'srange2' ] = df [ 'srange' ] . str . extract ( '(.\\d\\d,\\d\\d\\d - .\\d\\d\\d,\\d\\d\\d)' , expand = True ) #partition the strings in srange1 and srange2 low_range1 = [] high_range1 = [] for i in df [ 'srange1' ]: try : head1 , sep1 , tail1 = i . partition ( ' - ' ) low_range1 . append ( head1 ) high_range1 . append ( tail1 ) except : low_range1 . append ( np . nan ) high_range1 . append ( np . nan ) #Append the data to the dataframe that was extracted from the srange1 column. df [ 'low_srange1' ] = low_range1 df [ 'high_srange1' ] = high_range1 #drop the srange column. df . drop ( 'srange1' , axis = 1 , inplace = True ) #repeat the above step for srange2 low_range2 = [] high_range2 = [] for i in df [ 'srange2' ]: try : head2 , sep2 , tail2 = i . partition ( ' - ' ) low_range2 . append ( head2 ) high_range2 . append ( tail2 ) except : low_range2 . append ( np . nan ) high_range2 . append ( np . nan ) #Append the data to the dataframe that was extracted from the srange1 column. df [ 'low_srange2' ] = low_range2 df [ 'high_srange2' ] = high_range2 #drop the srange2 column. df . drop ( 'srange2' , axis = 1 , inplace = True ) #drop the srange column since we now have the data we need from it. df . drop ( 'srange' , axis = 1 , inplace = True ) #exctract salary data that matches the format for a 6 figure job from each job description. df [ 'extract_test_high' ] = df [ 'desc' ] . str . extract ( '(\\d\\d\\d,\\d\\d\\d\\.\\d\\d)' , expand = True ) #extract salary data that matches the format for a 5 figure job. df [ 'extract_test' ] = df [ 'desc' ] . str . extract ( '(\\d\\d,\\d\\d\\d\\.\\d\\d)' , expand = True ) #for comparison... extract the salary data that does not have decimal places but is five figures. df [ 'extract_test1' ] = df [ 'desc' ] . str . extract ( '(\\d\\d,\\d\\d\\d)' , expand = True ) #To accomplish this we need to reformat some columns. format_cols = [ 'low_srange1' , '1-4' , 'high_srange1' , 'low_srange2' , 'high_srange2' , 'extract_test_high' , 'extract_test' , 'extract_test1' ] #Format all dollar value rows so that they can be converted to floats. for i in df [ format_cols ]: df [ i ] = df [ i ] . astype ( str ) #A way to compline the two str.replace into one line. df [ i ] = df [ i ] . str . replace ( '[\\$,]' , '' , regex = True ) df [ i ] = df [ i ] . str . replace ( '$' , '' ) df [ i ] = df [ i ] . astype ( float ) #Account for error values that may have been extracted. All our salary estimates should be over the median starting #salary of an employee AT LEAST! df [ df [ format_cols ] < 45000.0 ] = np . nan df [ df [ format_cols ] > 140000.0 ] = np . nan 10: Calculate the percentage increase over the median salary at each company. #We will also take the high and low range salary estimate from the post and average it together so that we're #working with only one number. df [ 'post_estimate' ] = df [[ 'low_srange1' , 'high_srange1' , 'low_srange2' , 'high_srange2' , 'extract_test_high' , 'extract_test' , 'extract_test1' ]] . mean ( axis = 1 ) #drop all those srange columns now. df . drop ([ 'low_srange1' , 'high_srange1' , 'low_srange2' , 'high_srange2' , 'extract_test_high' , 'extract_test' , 'extract_test1' ], axis = 1 , inplace = True ) df [ 'post_estimate' ] . value_counts () 50000.000000 8 74000.000000 4 75000.000000 3 90000.000000 3 112946.666667 2 100000.000000 2 45000.000000 1 80000.000000 1 96366.500000 1 62000.000000 1 125000.000000 1 94411.600000 1 140000.000000 1 58000.000000 1 76000.000000 1 93159.333333 1 67419.000000 1 62338.000000 1 100306.500000 1 Name: post_estimate, dtype: int64 #Now we'll take the post_estimate and known data science salaries at that company and create an initial estimate. #df['initial_estimate'] = df[['post_estimate', 'data_scientist_salary']].mean(axis=1) #Now we're going to make a dataframe of only the data we can make our first two estimates off of. #What does thresh=1 do again? df2 = df [[ 'post_estimate' ]] . dropna ( thresh = 1 ) df2 = df2 . join ( df , lsuffix = '_left' , rsuffix = '_right' ) df2 . drop ([ 'post_estimate_left' ], axis = 1 , inplace = True ) #Filter the dataframe to only have values from the 1-4 column. This will return just the posts that have BOTH #1-4 year company data and the job post specific salary data. df2 = df2 [ df2 [ '1-4' ] > 0 ] print df2 . shape (10, 63) #Now we'll use these job postings to calculate a percentage increase over a standard job at each of these companies. df2 [ 'weight' ] = ( df2 [ 'post_estimate_right' ] / df2 [ '1-4' ]) #Average percentage increase over just normal year data that a data scientist salary should be making. ds_generic_weight = df2 [ 'weight' ] . mean () ds_generic_weight 1.2451427181054453 #now the same thing but for each company. df3 = df2 . groupby ( 'company' ) . mean () df3 = df3 . reset_index () df4 = df3 [[ 'company' , 'weight' ]] df4 company weight 0 BAE Systems 1.418797 1 Centers for Disease Control and Preven... 1.661176 2 Central Intelligence Agency 1.023344 3 FactSet Research Systems 0.741228 4 Mars 1.127447 5 Teva Pharmaceuticals 0.828382 6 U.S. Department of Labor 1.686012 #Make a dictionary mapping function for the above companies. weight_dict = df4 . set_index ( 'company' ) . to_dict ()[ 'weight' ] ESTIMATING You'll notice that above if we only used jobs that had salaries we'd have 49 out of every 560. That is why I built a salary estimator. With the salary data that is available for these companies if I use a weighted measure to estimate the salary of a data scientist then I will be able to keep posts that either have a salary range posted, a median data scientist salary posted, OR a median salary for a 1-4 year employee posted. 11: For jobs without a salary in the post use the Data Science salary weight to estimate a salary for each company. #Add a column of company weights we calclated earlier to the dataframe. df [ 'co_weight' ] = df [ 'company' ] . map ( weight_dict ) #Fill all the unknown weights for companies with the average increase of 25%. df [ 'co_weight' ] = df [ 'co_weight' ] . fillna ( ds_generic_weight ) #Now calculate a weighted salary estimate for each of the \"Data Sciency\" jobs we have in our dataframe. df [ 'weighted_estimate' ] = df [ 'co_weight' ] * df [ '1-4' ] #Now for the FINAL estimate we will average together the data that we have available to us. df [ 'estimate' ] = df [[ 'post_estimate' , 'weighted_estimate' ]] . mean ( axis = 1 ) #Before we move on to modeling though we need to drop the jobs in our dataframe that did not end up with an estimate. print \"df shape before estimate =\" , df . shape df = df [ df [ 'estimate' ] > 0 ] print \"df shape after estimate =\" , df . shape df . head () df shape before estimate = (472, 66) df shape after estimate = (219, 66) title company url location desc 1-4 Job Flexibility Job Satisfaction Vacation Weeks Work Stress ... RESEARCH_title ANALYST_title SENIOR_title RESEARCH ANALYST_title ENGINEER_title likelihood_title post_estimate co_weight weighted_estimate estimate 1172 STATISTICAL CONSULTANT Equifax http://indeed.com/rc/clk?jk=e3db711f65629c93&f... Alpharetta, GA STATISTICAL CONSULTANT APPLY JOB ... 60404.0 Data Not Available Extremely satisfied 2.7 Weeks Stressful ... NaN NaN NaN NaN NaN 0.000000 NaN 1.245143 75211.600744 75211.600744 2016 BUSINESS ANALYTICS MANAGER Medidata Solutions http://indeed.com/rc/clk?jk=eb009ead7e4f2013&f... New York, NY 10003 (Greenwich Village area) OUR IN... 52118.0 NaN NaN NaN NaN ... NaN NaN NaN NaN NaN 0.000000 NaN 1.245143 64894.348182 64894.348182 927 DATA SCIENTIST ( MACHINE LEARNING) IHS Markit http://indeed.com/rc/clk?jk=493df6b925152ae3&f... Washington, DC ABOUT IHS MARKIT IHS MARKIT HARNESSES DEEP SO... NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN 0.159691 50000.0 1.245143 NaN 50000.000000 1496 CHIEF DATA SCIENTIST S&P GLOBAL http://indeed.com/rc/clk?jk=307ac18e77a35c8a&f... New York, NY 10002 (Lower East Side area) POSITION DESCRIPTION ... 67348.0 NaN NaN NaN NaN ... NaN NaN NaN NaN NaN 0.159691 NaN 1.245143 83857.871779 83857.871779 1355 PRINCIPAL DATA SCIENTIST Aetna http://indeed.com/rc/clk?jk=6a7a681f17bfde42&f... New York, NY 10016 (Gramercy area) POSITION SUMMARY THE PRINCIPAL SCIENTIST PROVI... 55214.0 Data Not Available Extremely satisfied 3.0 Weeks Highly stressful ... NaN NaN NaN NaN NaN 0.159691 NaN 1.245143 68749.310037 68749.310037 5 rows × 66 columns #Predict against the estimate. df_regressor = df target2 = df_regressor [ 'estimate' ] df_regressor . columns Index([u'title', u'company', u'url', u'location', u'desc', u'1-4', u'Job Flexibility', u'Job Satisfaction', u'Vacation Weeks', u'Work Stress', u'desc_len', u'ds_in_title', u'desc_test', u'title_test', u'DATA_keyword', u'EXPERIENCE_keyword', u'WORK_keyword', u'ANALYTICS_keyword', u'ANALYSIS_keyword', u'BUSINESS_keyword', u'SCIENCE_keyword', u'TEAM_keyword', u'LEARNING_keyword', u'SKILLS_keyword', u'NEW_keyword', u'STATISTICAL_keyword', u'MACHINE_keyword', u'SCIENTIST_keyword', u'ABILITY_keyword', u'MACHINE LEARNING_keyword', u'SOLUTIONS_keyword', u'DATA SCIENTIST_keyword', u'DATA SCIENCE_keyword', u'USING_keyword', u'RESEARCH_keyword', u'YEARS_keyword', u'TOOLS_keyword', u'INFORMATION_keyword', u'DEVELOPMENT_keyword', u'TECHNICAL_keyword', u'COMPANY_keyword', u'WORKING_keyword', u'MODELS_keyword', u'INCLUDING_keyword', u'TECHNOLOGY_keyword', u'JOB_keyword', u'ALGORITHMS_keyword', u'PROBLEMS_keyword', u'STRONG_keyword', u'LARGE_keyword', u'KNOWLEDGE_keyword', u'METHODS_keyword', u'likelihood_keyword', u'SCIENTIST_title', u'DATA_title', u'DATA SCIENTIST_title', u'RESEARCH_title', u'ANALYST_title', u'SENIOR_title', u'RESEARCH ANALYST_title', u'ENGINEER_title', u'likelihood_title', u'post_estimate', u'co_weight', u'weighted_estimate', u'estimate'], dtype='object') df_regressor . drop ([ '1-4' , 'post_estimate' , 'weighted_estimate' , 'estimate' ], axis = 1 , inplace = True ) ## ALTERNATIVE ESTIMATING le = LabelEncoder () X2 = df_regressor . apply ( LabelEncoder () . fit_transform ) y2 = le . fit_transform ( target2 ) / Users / nathanhall / anaconda / lib / python2 .7 / site - packages / numpy / lib / arraysetops . py : 200 : FutureWarning : numpy not_equal will not check object identity in the future . The comparison did not return the same result as suggested by the identity ( ` is ` )) and will change . flag = np . concatenate (([ True ], aux [ 1 : ] ! = aux [:- 1 ])) #Get a train and teset dataset for the above variables to put into random forest. X2_train , X2_test , y2_train , y2_test = train_test_split ( X2 , y2 , test_size =. 33 , random_state = 99 ) #Run Random Forest on the data. rf2 = RandomForestRegressor () rf2_model = rf2 . fit ( X2_train , y2_train ) y2_pred = rf2 . predict ( X2_test ) print \"RandomForest Cross_Val Score: \\t \" , cross_val_score ( rf2 , X2_train , y2_train , cv = 5 ) . mean () print \"Train/Test RandomForest Score: \\t \" , rf2 . score ( X2_test , y2_test ) RandomForest Cross_Val Score: 0.179160728784 Train/Test RandomForest Score: -0.205902706264 X2_test . shape (73, 62) len ( y2_pred ) 73 X2_test [ 'estimate' ] = y2_test X2_test [ 'predicted' ] = y2_pred / Users / nathanhall / anaconda / lib / python2 . 7 / site - packages / ipykernel / __main__ . py : 1 : SettingWithCopyWarning : A value is trying to be set on a copy of a slice from a DataFrame. Try using . loc [ row_indexer , col_indexer ] = value instead See the caveats in the documentation : http : // pandas . pydata . org / pandas - docs / stable / indexing . html #indexing-view-versus-copy if __name__ == '__main__' : / Users / nathanhall / anaconda / lib / python2 . 7 / site - packages / ipykernel / __main__ . py : 2 : SettingWithCopyWarning : A value is trying to be set on a copy of a slice from a DataFrame. Try using . loc [ row_indexer , col_indexer ] = value instead See the caveats in the documentation : http : // pandas . pydata . org / pandas - docs / stable / indexing . html #indexing-view-versus-copy from ipykernel import kernelapp as app MODELING 12a. Format dataframe features into dummy variables. df_keywords . drop ([ 'likelihood_keyword' , 'likelihood_title' ], axis = 1 , inplace = True ) #Get the keywords for each job post as a dummy variable. for x in df_keywords . columns : i_list = [] for i in df [ x ]: if i > 0 : i_list . append ( 1 ) else : i_list . append ( np . nan ) df [ x ] = i_list model_dummies = pd . get_dummies ( df [[ 'ds_in_title' , 'Job Flexibility' , 'Job Satisfaction' , 'Work Stress' ]], drop_first = True ) #Lets make a dataframe with all the things we think might be interesting to pass into a random forest model. df_model = pd . concat ([ df , model_dummies ], axis = 1 ) #Label each job post as being above or below the median to create our target. target = [ 1 if x else 0 for x in df_model [ 'estimate' ] >= 85000 ] #Dropping estimate from the model since that would be giving too much information to the random forest model. df_model . drop ([ 'estimate' , '1-4' , 'post_estimate' , 'co_weight' , 'weighted_estimate' , 'title' , 'company' , 'url' , 'location' , 'desc' , 'desc_test' , 'title_test' , 'Job Flexibility' , 'Job Satisfaction' , 'ds_in_title' , 'Work Stress' , 'Vacation Weeks' , 'COMPANY' , 'WORK' , 'NEW' , 'SKILLS' ], axis = 1 , inplace = True ) print \"Shape of the model dataframe =\" , df_model . shape Shape of the model dataframe = (219, 56) #Make a dataset for logistic regression. from sklearn.preprocessing import StandardScaler log_model = df_model log_model [ 'target' ] = target log_model = log_model . fillna ( 0.0 ) log_model . head () desc_len DATA_keyword EXPERIENCE_keyword WORK_keyword ANALYTICS_keyword ANALYSIS_keyword BUSINESS_keyword SCIENCE_keyword TEAM_keyword LEARNING_keyword ... ENGINEER_title likelihood_title Job Flexibility_Extremely flexible Job Flexibility_Highly flexible Job Satisfaction_Highly satisfied Job Satisfaction_Satisfied Work Stress_Highly stressful Work Stress_Somewhat stressful Work Stress_Stressful target 1172 4637 1 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 ... 0.0 0.000000 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0 2016 6052 1 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 ... 0.0 0.000000 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 927 7079 1 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 ... 0.0 0.159691 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1496 6500 1 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 ... 0.0 0.159691 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1355 4835 1 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 ... 0.0 0.159691 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0 5 rows × 57 columns scaler = StandardScaler () log_model1 = scaler . fit_transform ( log_model1 ) log_model1 = log_model . drop ([ 'target' , 'desc_len' ], axis = 1 ) X1 = log_model1 y1 = log_model [ 'target' ] #Get a train and teset dataset for the above variables to put into random forest. X1_train , X1_test , y1_train , y1_test = train_test_split ( X1 , y1 , test_size =. 33 , random_state = 99 ) 12: Run all of our features through a Random Forest model to find what most influences a job being above or below the median salary. df_model . drop ( 'target' , axis = 1 , inplace = True ) le = LabelEncoder () X = df_model . apply ( LabelEncoder () . fit_transform ) y = le . fit_transform ( target ) #Get a train and teset dataset for the above variables to put into random forest. X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size =. 33 , random_state = 99 ) #Grid_search on the data above to find the best parameters to pass to Random Forest rf_grid = RandomForestClassifier ( random_state = 99 , n_jobs = 50 ) #Parameter dictionary of settings options for the model we're passing grid search. param_grid = { 'criterion' : [ 'gini' , 'entropy' ], 'max_depth' : [ None , 2 , 5 , 8 ], 'max_features' : [ 'auto' , 'sqrt' , 'log2' ], 'class_weight' : [ 'balanced' , None ] } #Instantiate grid search. grid = GridSearchCV ( rf_grid , param_grid , cv = 5 , scoring = 'accuracy' ) #Fit the grid search to X, and y. grid . fit ( X_train , y_train ) #Store the best parameters in a variable. params = grid . best_params_ print \"Best score =\" , grid . best_score_ print params Best score = 0.684931506849 {'max_features': 'log2', 'criterion': 'entropy', 'max_depth': None, 'class_weight': 'balanced'} #Run Random Forest on the data. rf = RandomForestClassifier ( ** params ) rf_model = rf . fit ( X_train , y_train ) y_pred = rf . predict ( X_test ) print \"RandomForest Cross_Val Score: \\t \" , cross_val_score ( rf , X_train , y_train , cv = 5 ) . mean () print \"Train/Test RandomForest Score: \\t \" , rf . score ( X_test , y_test ) RandomForest Cross_Val Score: 0.684827586207 Train/Test RandomForest Score: 0.58904109589 #CONFUSION MATRIX FOR LOGISTIC REGRESSION confusion = np . array ( confusion_matrix ( y_test , y_pred )) # calculate true positives, the number of 1s correctly predicted to be 1 print \"TP =\" , confusion [ 0 , 0 ] # calculate false positives, the number of 0s incorrectly predicted to be 1 print \"FP =\" , confusion [ 1 , 0 ] # calculate true negatives, the number of 0s correctly predicted to be 0 print \"TN =\" , confusion [ 1 , 1 ] # calculate false negatives, the number of 1s incorrectly predicted to be 0 print \"FN =\" , confusion [ 0 , 1 ] # print out the classification report as well. print classification_report ( y_test , y_pred ) TP = 36 FP = 20 TN = 7 FN = 10 precision recall f1-score support 0 0.64 0.78 0.71 46 1 0.41 0.26 0.32 27 avg / total 0.56 0.59 0.56 73 df_features = pd . DataFrame ( columns = [ 'Features' , 'Importance (Gini Index)' ]) df_features [ 'Features' ] = df_model . columns df_features [ 'Importance (Gini Index)' ] = rf . feature_importances_ df_features . sort_values ( 'Importance (Gini Index)' , ascending = False , inplace = True ) #Grid_search on the data above to find the best parameters to pass to Logistic Regression log_grid = linear_model . LogisticRegressionCV ( random_state = 99 , n_jobs =- 1 , solver = 'newton-cg' , penalty = 'l2' ) #Parameter dictionary of settings options for the model we're passing grid search. param_grid1 = { 'Cs' : [ 1 , 2 , 5 , 7 , 9 ], 'cv' : [ 2 , 5 ], 'scoring' : [ 'accuracy' , 'r2' ], 'multi_class' : [ 'ovr' , 'multinomial' ] } #Scoring to try = roc_auc, f1, precision, and accuracy #Instantiate grid search. grid1 = GridSearchCV ( log_grid , param_grid1 , cv = 3 , scoring = 'accuracy' ) #Fit the grid search to X, and y. grid1 . fit ( X1_train , y1_train ) #Store the best parameters in a variable. params1 = grid1 . best_params_ print \"Best score =\" , grid1 . best_score_ print params1 Best score = 0.602739726027 {'Cs': 5, 'multi_class': 'ovr', 'cv': 5, 'scoring': 'accuracy'} #def logistic_vs_svm(X_train, X_test, y_train, y_test): # Looking at logistic regression compared to our target to see what a baseline accuracy would be. logistic = linear_model . LogisticRegressionCV ( ** params1 ) logistic . fit ( X1_train , y1_train ) print \"----------------\" print \"LOGISTIC REGRESSION CV ACCURACY SCORE\" print cross_val_score ( logistic , X1_train , y1_train , cv = 5 , scoring = 'accuracy' ) . mean () #Show the r2 score as well. print \"----------------\" print \"LOGISTIC REGRESSION R2 SCORE\" print cross_val_score ( logistic , X1_train , y1_train , cv = 5 , scoring = 'r2' ) . mean () #Logistic confusion matrix y1_pred = logistic . predict ( X1_test ) #CONFUSION MATRIX FOR LOGISTIC REGRESSION CV confusion = np . array ( confusion_matrix ( y1_test , y1_pred )) # calculate true positives, the number of 1s correctly predicted to be 1 print \"------------------\" print \"LOGISTIC REGRESSION CONFUSION MATRIX\" print \"TP =\" , confusion [ 0 , 0 ] # calculate false positives, the number of 0s incorrectly predicted to be 1 print \"FP =\" , confusion [ 1 , 0 ] # calculate true negatives, the number of 0s correctly predicted to be 0 print \"TN =\" , confusion [ 1 , 1 ] # calculate false negatives, the number of 1s incorrectly predicted to be 0 print \"FN =\" , confusion [ 0 , 1 ] ---------------- LOGISTIC REGRESSION CV ACCURACY SCORE 0.65724137931 ---------------- LOGISTIC REGRESSION R2 SCORE -0.41568627451 ------------------ LOGISTIC REGRESSION CONFUSION MATRIX TP = 34 FP = 17 TN = 10 FN = 12 df_model [ 'target' ] = target df_model [ 'target' ] . value_counts () 0 132 1 87 Name: target, dtype: int64 logistic_coef = [] for i in logistic . coef_ . tolist (): for j in i : logistic_coef . append ( j ) print len ( logistic_coef ) 55 logistic_coef = [] for i in logistic . coef_ . tolist (): for j in i : logistic_coef . append ( j ) print len ( logistic_coef ) log_features = pd . DataFrame ( columns = [ 'Features' , 'Importance (coef)' ]) log_features [ 'Features' ] = X1_train . columns log_features [ 'Importance (coef)' ] = logistic_coef log_features . sort_values ( 'Importance (coef)' , ascending = False , inplace = True ) #log_features.drop('Importance (Gini Index)', axis=1, inplace=True) #log_features.drop(0, inplace=True) log_features . head () Features Importance (coef) 53 Work Stress_Somewhat stressful 30.475260 0 DATA_keyword 26.847726 43 ANALYST_title 23.199324 33 PROBLEMS_keyword 21.660053 13 SCIENTIST_keyword 18.722064 Starting accuracy 169/(169+45) == 0.789 We have an increase in accuracy here of about 8%. VISUALIZATIONS #Number of jobs we analyzed and got salary data for. df = df [ df [ 'estimate' ] > 0 ] df . shape (219, 66) #Median salary of our data. df . estimate . median () 76353.39661694401 #Plot it on a histogram. df . hist ( column = 'estimate' ) array([[<matplotlib.axes._subplots.AxesSubplot object at 0x13ac7e710>]], dtype=object) #Sort the dataframe so that we can have a better output df . sort_values ( 'estimate' , ascending = False , inplace = True ) #Look at the best companies. f , ax = plt . subplots ( figsize = ( 8 , 40 )) sns . barplot ( x = 'estimate' , y = \"company\" , data = df ) <matplotlib.axes._subplots.AxesSubplot at 0x133659750> <matplotlib.axes._subplots.AxesSubplot at 0x1331c3b90> Do a scatter plot of \"likelihood keywords+titles\" against salary estimate. #Look at the most important features. f , ax = plt . subplots ( figsize = ( 8 , 12 )) sns . barplot ( x = 'Importance (Gini Index)' , y = 'Features' , data = df_features . iloc [ 0 : 23 ]) <matplotlib.axes._subplots.AxesSubplot at 0x128c0eb90> df_features . iloc [ 0 : 50 ] Features Importance (Gini Index) 18 DATA SCIENTIST_keyword 0.049955 12 STATISTICAL_keyword 0.049172 47 ENGINEER_title 0.043087 45 SENIOR_title 0.038337 48 likelihood_title 0.038186 7 SCIENCE_keyword 0.036519 28 WORKING_keyword 0.035016 27 COMPANY_keyword 0.034195 33 ALGORITHMS_keyword 0.032109 4 ANALYTICS_keyword 0.030765 46 RESEARCH ANALYST_title 0.029616 34 PROBLEMS_keyword 0.029420 22 YEARS_keyword 0.028194 26 TECHNICAL_keyword 0.028128 41 DATA_title 0.025736 9 LEARNING_keyword 0.023099 53 Work Stress_Highly stressful 0.021466 13 MACHINE_keyword 0.020744 44 ANALYST_title 0.020652 36 LARGE_keyword 0.020500 43 RESEARCH_title 0.020026 10 SKILLS_keyword 0.018685 14 SCIENTIST_keyword 0.018643 35 STRONG_keyword 0.018581 0 desc_len 0.017105 39 likelihood_keyword 0.017065 25 DEVELOPMENT_keyword 0.016854 29 MODELS_keyword 0.016477 42 DATA SCIENTIST_title 0.016293 16 MACHINE LEARNING_keyword 0.015944 8 TEAM_keyword 0.015683 19 DATA SCIENCE_keyword 0.015024 21 RESEARCH_keyword 0.014750 23 TOOLS_keyword 0.012869 11 NEW_keyword 0.012513 24 INFORMATION_keyword 0.012495 30 INCLUDING_keyword 0.011906 40 SCIENTIST_title 0.011582 37 KNOWLEDGE_keyword 0.011511 17 SOLUTIONS_keyword 0.010887 15 ABILITY_keyword 0.009985 38 METHODS_keyword 0.008828 31 TECHNOLOGY_keyword 0.006824 6 BUSINESS_keyword 0.006701 51 Job Satisfaction_Highly satisfied 0.006249 20 USING_keyword 0.005854 32 JOB_keyword 0.005231 55 Work Stress_Stressful 0.003666 5 ANALYSIS_keyword 0.003363 2 EXPERIENCE_keyword 0.002129 df [ 'ds_ratio' ] = df [ 'estimate' ] / df [ 'likelihood_keyword' ] df [ 'ds_in_title' ] = df [ 'ds_in_title' ] . fillna ( 0.0 ) df [ 'target' ] = [ 1.0 if x else 0.0 for x in df [ 'estimate' ] >= 85000 ] df . columns Index([u'title', u'company', u'url', u'location', u'desc', u'1-4', u'Job Flexibility', u'Job Satisfaction', u'Vacation Weeks', u'Work Stress', u'desc_len', u'ds_in_title', u'desc_test', u'title_test', u'DATA_keyword', u'EXPERIENCE_keyword', u'WORK_keyword', u'ANALYTICS_keyword', u'ANALYSIS_keyword', u'BUSINESS_keyword', u'SCIENCE_keyword', u'TEAM_keyword', u'LEARNING_keyword', u'SKILLS_keyword', u'NEW_keyword', u'STATISTICAL_keyword', u'MACHINE_keyword', u'SCIENTIST_keyword', u'ABILITY_keyword', u'MACHINE LEARNING_keyword', u'SOLUTIONS_keyword', u'DATA SCIENTIST_keyword', u'DATA SCIENCE_keyword', u'USING_keyword', u'RESEARCH_keyword', u'YEARS_keyword', u'TOOLS_keyword', u'INFORMATION_keyword', u'DEVELOPMENT_keyword', u'TECHNICAL_keyword', u'COMPANY_keyword', u'WORKING_keyword', u'MODELS_keyword', u'INCLUDING_keyword', u'TECHNOLOGY_keyword', u'JOB_keyword', u'ALGORITHMS_keyword', u'PROBLEMS_keyword', u'STRONG_keyword', u'LARGE_keyword', u'KNOWLEDGE_keyword', u'METHODS_keyword', u'likelihood_keyword', u'SCIENTIST_title', u'DATA_title', u'DATA SCIENTIST_title', u'RESEARCH_title', u'ANALYST_title', u'SENIOR_title', u'RESEARCH ANALYST_title', u'ENGINEER_title', u'likelihood_title', u'post_estimate', u'co_weight', u'weighted_estimate', u'estimate', u'ds_ratio', u'target'], dtype='object') X2_test . columns Index([u'title', u'company', u'url', u'location', u'desc', u'Job Flexibility', u'Job Satisfaction', u'Vacation Weeks', u'Work Stress', u'desc_len', u'ds_in_title', u'desc_test', u'title_test', u'DATA_keyword', u'EXPERIENCE_keyword', u'WORK_keyword', u'ANALYTICS_keyword', u'ANALYSIS_keyword', u'BUSINESS_keyword', u'SCIENCE_keyword', u'TEAM_keyword', u'LEARNING_keyword', u'SKILLS_keyword', u'NEW_keyword', u'STATISTICAL_keyword', u'MACHINE_keyword', u'SCIENTIST_keyword', u'ABILITY_keyword', u'MACHINE LEARNING_keyword', u'SOLUTIONS_keyword', u'DATA SCIENTIST_keyword', u'DATA SCIENCE_keyword', u'USING_keyword', u'RESEARCH_keyword', u'YEARS_keyword', u'TOOLS_keyword', u'INFORMATION_keyword', u'DEVELOPMENT_keyword', u'TECHNICAL_keyword', u'COMPANY_keyword', u'WORKING_keyword', u'MODELS_keyword', u'INCLUDING_keyword', u'TECHNOLOGY_keyword', u'JOB_keyword', u'ALGORITHMS_keyword', u'PROBLEMS_keyword', u'STRONG_keyword', u'LARGE_keyword', u'KNOWLEDGE_keyword', u'METHODS_keyword', u'likelihood_keyword', u'SCIENTIST_title', u'DATA_title', u'DATA SCIENTIST_title', u'RESEARCH_title', u'ANALYST_title', u'SENIOR_title', u'RESEARCH ANALYST_title', u'ENGINEER_title', u'likelihood_title', u'co_weight', u'predicted'], dtype='object') X2_test [ 'predicted' ] = y2_pred #Scatter plot analysis sns . set_context ( \"notebook\" , font_scale = 1.1 ) sns . set_style ( \"ticks\" ) sns . lmplot ( 'predicted' , 'estimate' , data = X2_test , fit_reg = False , scatter_kws = { \"marker\" : \"D\" , \"s\" : 100 }) plt . title ( 'Scatter Plot Analysis' ) plt . xlabel ( 'DS_Likelihood' ) plt . ylabel ( 'Title_Likelihood' ) <matplotlib.text.Text at 0x12e97cd90> df [[ 'company' , 'likelihood_keyword' , 'ds_in_title' ]] company likelihood_keyword ds_in_title 262 Smith Hanley Associates 0.611106 1.0 1256 UCB 0.700532 1.0 1336 Facebook 0.616334 1.0 44 Solebrity, Inc. 0.598436 0.0 765 Dovel Technologies 0.616292 0.0 510 Dept of Forensic Sciences 0.671009 0.0 475 Dept of Forensic Sciences 0.651485 0.0 688 The Aerospace Corporation 0.637496 0.0 1062 The Aerospace Corporation 0.610304 0.0 994 The Aerospace Corporation 0.526587 0.0 55 The Aerospace Corporation 0.660001 1.0 533 Exponent 0.537793 0.0 697 Exponent 0.536683 0.0 2024 Celgene 0.603297 0.0 2101 Celgene 0.570444 0.0 1882 Celgene Corporation 0.658505 1.0 1562 Celgene 0.547107 1.0 2015 Celgene Corporation 0.633491 1.0 2091 Celgene Corporation 0.603297 0.0 1679 Celgene 0.648625 0.0 1808 Celgene 0.658505 1.0 2119 Celgene Corporation 0.522608 0.0 951 Bloomberg BNA 0.563972 0.0 1933 Genzyme 0.530128 0.0 27 Department of Health 0.558110 0.0 1385 Scienaptic Systems Inc 0.674084 0.0 1890 Driversiti 0.576417 0.0 1922 OppenheimerFunds 0.630253 0.0 1269 McKinsey & Company 0.721504 0.0 1068 Centers for Disease Control and Preven... 0.530710 0.0 ... ... ... ... 71 Teaching Strategies, LLC 0.636353 1.0 107 Truth Initiative 0.712770 1.0 1596 New York Life Insurance Co 0.684175 0.0 1340 New York Life Insurance Co 0.565238 1.0 1554 New York Life Insurance Co 0.707351 0.0 527 Altamira Technologies Corporation 0.637990 1.0 56 Altamira Technologies Corporation 0.557730 1.0 906 Altamira Technologies Corporation 0.617774 1.0 270 Altamira Technologies Corporation 0.637990 1.0 752 Conservation International 0.575591 0.0 92 Altamira Technologies Corporation 0.613087 1.0 1097 ASSURANT 0.684795 1.0 509 University of Maryland 0.573420 0.0 600 OnPoint Consulting, Inc. 0.653654 0.0 1518 ISO 0.704586 1.0 1657 ISO 0.642413 0.0 311 Synchronoss Technologies, Inc. 0.674990 1.0 469 Meso Scale Diagnostics 0.552441 0.0 351 Meso Scale Diagnostics 0.594435 0.0 2031 Teva Pharmaceuticals 0.626989 0.0 917 United Educators 0.623457 1.0 2026 RM Dayton Analytics, Ltd. Co. 0.553595 0.0 314 Veteran Staffing Network of Easter Seals 0.598300 1.0 1440 Averity 0.594164 1.0 1546 Selby Jennings 0.584915 1.0 1754 Harnham 0.624282 1.0 927 IHS Markit 0.828642 1.0 1000 The Wallach Search Group 0.685785 0.0 1809 Harnham 0.665668 1.0 577 RockHammer Talent Solutions 0.561362 0.0 219 rows × 3 columns","tags":"Articles","loc":"http://www.instantinate.com/articles/finding_a_data_science_job_with_data_science.html","title":"Finding a Data Science Job with Data Science"},{"url":"http://www.instantinate.com/visual_analysis/sat_scores_visual_analysis.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. Conduct visual analysis and complete the prompts below. This project was completed for the General Assembly Data Science Immersive course. The purpose was to build and refine our foundation of python and visually show interesting findings from the data set. Side Notes: The data was sparse, it only showed data for the 50 states, distric of columbia. Methodology: Accomplish the python prompts with as few lines of code as possible. Check the data for any cleanliness issues. Visualize the different distributions. Visualize any relationships in the data and identify outliers. Plot the state data as a USA heatmap. PROMPT 1: Open the sat_scores.csv file. Investigate the data, and answer the questions below. import pandas as pd #Data was imported using the pandas library for quick analysis. data = pd . read_csv ( '../data/sat_scores.csv' ) df = pd . DataFrame ( data ) df . head () State Rate Verbal Math 0 CT 82 509 510 1 NJ 81 499 513 2 MA 79 511 515 3 NY 77 495 505 4 NH 72 520 516 #After seeing state data there should be 50 rows. Check the shape to see if there is 50 rows. df . shape (52, 4) #Ok.... there is 52... cool. We'll come back to that. PROMPT 2. What does the data describe? #The average SAT section score for each state. And the participation rate of that state in the SAT progam. PROMPT 3. Does the data look complete? Are there any obvious issues with the observations? #I printed the value counts and sorted them multiple times to check for data cleanliness. #NOTE: I am only showing the top 5 of each here because I completed the analysis. print \"--------------\" print \"STATE VALUE COUNTS\" print df [ 'State' ] . value_counts ()[: 5 ] print \"--------------\" print \"RATE VALUE COUNTS\" print df [ 'Rate' ] . value_counts ()[: 5 ] print \"--------------\" print \"VERBAL VALUE COUNTS\" print df [ 'Verbal' ] . value_counts ()[: 5 ] print \"--------------\" print \"MATH VALUE COUNTS\" print df [ 'Math' ] . value_counts ()[: 5 ] -------------- STATE VALUE COUNTS SD 1 OR 1 CA 1 VA 1 NV 1 Name: State, dtype: int64 -------------- RATE VALUE COUNTS 9 3 4 3 8 3 5 2 65 2 Name: Rate, dtype: int64 -------------- VERBAL VALUE COUNTS 577 3 562 3 511 2 501 2 527 2 Name: Verbal, dtype: int64 -------------- MATH VALUE COUNTS 499 6 510 3 515 3 542 3 550 2 Name: Math, dtype: int64 # Data issues to note from earier. There since 50 rows were expected and 52 were found we looked to see what they were. # 1: Is the district of columbia and should be included. # 2: Is called \"All\" which needs further investigation below. #Check what the values of the All row is. df . iloc [ 51 ] State All Rate 45 Verbal 506 Math 514 Name: 51, dtype: object #This might be the sum total average for all the other rows in the dataframe. The below code attempts to confirm that theory. #Look at the average score of the other states not including \"All\" and comparing it to the score above. df [ df [ 'State' ] != 'All' ] . Rate . mean () 37.0 #CONCLUSION: The above number is nowhere near the mean of the rate column. #So this data does not represent the average of the column. #However we will leave it in for now and if it proves to be an outlier we'll consider removing it. PROMPT 4. Create a data dictionary for the dataset. Column Description State The state the data was taken from Rate Participation rate in taking the SAT Verbal Average verbal score for that state Math Average math score for that state PROMPT 5. Load the data into a list of lists import csv #This is easier to accomplish with csv reader rather than Pandas so we'll reimport the data here. with open ( '../data/sat_scores.csv' , 'rU' ) as f : data = [] reader = csv . reader ( f ) for row in reader : data . append ( row ) PROMPT 6. Print the data print data [['State', 'Rate', 'Verbal', 'Math'], ['CT', '82', '509', '510'], ['NJ', '81', '499', '513'], ['MA', '79', '511', '515'], ['NY', '77', '495', '505'], ['NH', '72', '520', '516'], ['RI', '71', '501', '499'], ['PA', '71', '500', '499'], ['VT', '69', '511', '506'], ['ME', '69', '506', '500'], ['VA', '68', '510', '501'], ['DE', '67', '501', '499'], ['MD', '65', '508', '510'], ['NC', '65', '493', '499'], ['GA', '63', '491', '489'], ['IN', '60', '499', '501'], ['SC', '57', '486', '488'], ['DC', '56', '482', '474'], ['OR', '55', '526', '526'], ['FL', '54', '498', '499'], ['WA', '53', '527', '527'], ['TX', '53', '493', '499'], ['HI', '52', '485', '515'], ['AK', '51', '514', '510'], ['CA', '51', '498', '517'], ['AZ', '34', '523', '525'], ['NV', '33', '509', '515'], ['CO', '31', '539', '542'], ['OH', '26', '534', '439'], ['MT', '23', '539', '539'], ['WV', '18', '527', '512'], ['ID', '17', '543', '542'], ['TN', '13', '562', '553'], ['NM', '13', '551', '542'], ['IL', '12', '576', '589'], ['KY', '12', '550', '550'], ['WY', '11', '547', '545'], ['MI', '11', '561', '572'], ['MN', '9', '580', '589'], ['KS', '9', '577', '580'], ['AL', '9', '559', '554'], ['NE', '8', '562', '568'], ['OK', '8', '567', '561'], ['MO', '8', '577', '577'], ['LA', '7', '564', '562'], ['WI', '6', '584', '596'], ['AR', '6', '562', '550'], ['UT', '5', '575', '570'], ['IA', '5', '593', '603'], ['SD', '4', '577', '582'], ['ND', '4', '592', '599'], ['MS', '4', '566', '551'], ['All', '45', '506', '514']] PROMPT 7. Extract a list of the labels from the data, and remove them from the data. #Setting the first row as a columns variable. columns = data [ 0 ][:] #Removing the first row(a.k.a the first list object) from the original data list. data . pop ( 0 ) ['State', 'Rate', 'Verbal', 'Math'] PROMPT 8. Create a list of State names extracted from the data. (Hint: use the list of labels to index on the State column) #Use list comprehension for extracting the first element of each list into a new list. states_list = [ row [ 0 ] for row in data ] print states_list ['CT', 'NJ', 'MA', 'NY', 'NH', 'RI', 'PA', 'VT', 'ME', 'VA', 'DE', 'MD', 'NC', 'GA', 'IN', 'SC', 'DC', 'OR', 'FL', 'WA', 'TX', 'HI', 'AK', 'CA', 'AZ', 'NV', 'CO', 'OH', 'MT', 'WV', 'ID', 'TN', 'NM', 'IL', 'KY', 'WY', 'MI', 'MN', 'KS', 'AL', 'NE', 'OK', 'MO', 'LA', 'WI', 'AR', 'UT', 'IA', 'SD', 'ND', 'MS', 'All'] PROMPT 9. Print the types of each column df . dtypes State object Rate int64 Verbal int64 Math int64 dtype: object PROMPT 10. Do any types need to be reassigned? If so, go ahead and do it. #None that I can see. I should be able to work wiht floats just fine for what I need. PROMPT 11. Create a dictionary for each column mapping the State to its respective value for that column. #Using data coprehension to set the first element of each list item as a #key and then use the rest of the list as the value. sat_dict = { d [ 0 ]: d [ 1 :] for d in data } print sat_dict {'WA': ['53', '527', '527'], 'DE': ['67', '501', '499'], 'DC': ['56', '482', '474'], 'WI': ['6', '584', '596'], 'WV': ['18', '527', '512'], 'HI': ['52', '485', '515'], 'FL': ['54', '498', '499'], 'WY': ['11', '547', '545'], 'NH': ['72', '520', '516'], 'NJ': ['81', '499', '513'], 'NM': ['13', '551', '542'], 'TX': ['53', '493', '499'], 'LA': ['7', '564', '562'], 'NC': ['65', '493', '499'], 'ND': ['4', '592', '599'], 'NE': ['8', '562', '568'], 'TN': ['13', '562', '553'], 'NY': ['77', '495', '505'], 'PA': ['71', '500', '499'], 'RI': ['71', '501', '499'], 'NV': ['33', '509', '515'], 'VA': ['68', '510', '501'], 'CO': ['31', '539', '542'], 'AK': ['51', '514', '510'], 'AL': ['9', '559', '554'], 'AR': ['6', '562', '550'], 'VT': ['69', '511', '506'], 'IL': ['12', '576', '589'], 'GA': ['63', '491', '489'], 'IN': ['60', '499', '501'], 'IA': ['5', '593', '603'], 'OK': ['8', '567', '561'], 'AZ': ['34', '523', '525'], 'CA': ['51', '498', '517'], 'ID': ['17', '543', '542'], 'CT': ['82', '509', '510'], 'ME': ['69', '506', '500'], 'MD': ['65', '508', '510'], 'All': ['45', '506', '514'], 'MA': ['79', '511', '515'], 'OH': ['26', '534', '439'], 'UT': ['5', '575', '570'], 'MO': ['8', '577', '577'], 'MN': ['9', '580', '589'], 'MI': ['11', '561', '572'], 'KS': ['9', '577', '580'], 'MT': ['23', '539', '539'], 'MS': ['4', '566', '551'], 'SC': ['57', '486', '488'], 'KY': ['12', '550', '550'], 'OR': ['55', '526', '526'], 'SD': ['4', '577', '582']} PROMPT 12. Create a dictionary with the values for each of the numeric columns #To accomplish this prompt we need a new list of columns that does not have the state column. numeric_columns = columns [ 0 :][ 1 : 4 ] numeric_columns ['Rate', 'Verbal', 'Math'] #Dictionary comprehension with enumerate and an embedded list comprehension. #NOTE: The start argument for the enumerate function is ignoring the state data in each list #and returning only the numeric values to be mapped to our new numeric columns variable. numeric_data = { column_name : [ row [ i ] for row in data ] for i , column_name in enumerate ( numeric_columns , start = 1 )} print numeric_data {'Rate': ['82', '81', '79', '77', '72', '71', '71', '69', '69', '68', '67', '65', '65', '63', '60', '57', '56', '55', '54', '53', '53', '52', '51', '51', '34', '33', '31', '26', '23', '18', '17', '13', '13', '12', '12', '11', '11', '9', '9', '9', '8', '8', '8', '7', '6', '6', '5', '5', '4', '4', '4', '45'], 'Math': ['510', '513', '515', '505', '516', '499', '499', '506', '500', '501', '499', '510', '499', '489', '501', '488', '474', '526', '499', '527', '499', '515', '510', '517', '525', '515', '542', '439', '539', '512', '542', '553', '542', '589', '550', '545', '572', '589', '580', '554', '568', '561', '577', '562', '596', '550', '570', '603', '582', '599', '551', '514'], 'Verbal': ['509', '499', '511', '495', '520', '501', '500', '511', '506', '510', '501', '508', '493', '491', '499', '486', '482', '526', '498', '527', '493', '485', '514', '498', '523', '509', '539', '534', '539', '527', '543', '562', '551', '576', '550', '547', '561', '580', '577', '559', '562', '567', '577', '564', '584', '562', '575', '593', '577', '592', '566', '506']} PROMPT 13. Print the min and max of each column print \"RATE MINIMUM VALUE = \" , df [ 'Rate' ] . min () print \"RATE MAXIMUM VALUE = \" , df [ 'Rate' ] . max () print \"VERBAL MINIMUM VALUE = \" , df [ 'Verbal' ] . min () print \"VERBAL MAXIMUM VALUE = \" , df [ 'Verbal' ] . max () print \"MATH MINIMUM VALUE = \" , df [ 'Math' ] . min () print \"MATH MAXIMUM VALUE = \" , df [ 'Math' ] . max () RATE MINIMUM VALUE = 4 RATE MAXIMUM VALUE = 82 VERBAL MINIMUM VALUE = 482 VERBAL MAXIMUM VALUE = 593 MATH MINIMUM VALUE = 439 MATH MAXIMUM VALUE = 603 PROMPT 14. Write a function using only list comprehensions, no loops, to compute Standard Deviation. Print the Standard Deviation of each numeric column. #This is a simple formula to accomplish the prompt. There is a better way to do it in numpy so please do not #use the example below as the best way. import math import numpy as np def std ( col ): std = math . sqrt ( sum (( df [ col ] - np . mean ( df [ col ])) ** 2 ) / ( len ( df ) - 1 )) return std print ( 'Standard Deviation for Rate: ' + str ( std ( 'Rate' ))) print ( 'Standard Deviation for Verbal: ' + str ( std ( 'Verbal' ))) print ( 'Standard Deviation for Math: ' + str ( std ( 'Math' ))) Standard Deviation for Rate: 27.3017880729 Standard Deviation for Verbal: 33.2362254438 Standard Deviation for Math: 36.0149750989 PROMPT 15. Using MatPlotLib and PyPlot, plot the distribution of the Rate using histograms. #Using seaborn, matplot lib, and plotly to accomplish the below tasks. import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline #Rugplots are in general more informative than histograms so I default to using them. plt . hist ( df . Rate , alpha =. 3 ) sns . rugplot ( df . Rate ); #average rate by state: df [ 'Rate' ] . mean () 37.15384615384615 PROMPT 16. Plot the Math distribution plt . hist ( df . Math , alpha =. 3 ) sns . rugplot ( df . Math ); #Average math score by state. df [ 'Math' ] . mean () 531.5 PROMPT 17. Plot the Verbal distribution plt . hist ( df . Verbal , alpha =. 3 ) sns . rugplot ( df . Verbal ); #Average Verbal score by state. df [ 'Verbal' ] . mean () 532.0192307692307 PROMPT 18. What is the typical assumption for data distribution? #One typical assumption is that the data follows some kind of normal distribution. #NORMAL CURVE: In this case the data does not seem to follow a true normal distribution. #LOW SPREAD: Not a large spread with outliers of data. #CENTERED: Data does not seem to have a good set of large points around the mean except in the case of the math column. PROMPT 19. Does that distribution hold true for our data? #Perhaps for the math data but not the other two. #However we can still plot some interesting relationships below. PROMPT 20. Plot some scatterplots. BONUS : Use a PyPlot figure to present multiple plots at once. #VERBAL vs. MATH data points. This is not colored by state because there would be too many colors to track #and draw any meaningful conclusions. sns . lmplot ( 'Verbal' , 'Math' , data = df , fit_reg = False , aspect = 2 , size = 6 ) <seaborn.axisgrid.FacetGrid at 0x11d763350> #All in all, the above plot is showing a pretty good relationship between math and verbal scores however #There is one outlier to note that we will investigate further. #Same scatter plot but with a hue for Rate. The reason being you can kind of see relationships between #where the colors cluster but at the same time it also demonstrates why you don't color by large lists of points. sns . lmplot ( 'Verbal' , 'Math' , data = df , hue = 'Rate' , fit_reg = False , aspect = 2 , size = 6 ) <seaborn.axisgrid.FacetGrid at 0x11d13a150> PROMPT 21. Are there any interesting relationships to note? #Calling a correlation plotting function that allows me to see relationships with more information than the standard #pairplot. def corr_pairs ( df_input , coef_percentile ): c = df_input . corr () s = c . unstack () so = s . sort_values ( kind = \"quicksort\" ) df_output = pd . DataFrame ( so . abs (), columns = [ 'coef' ]) df_output = df_output . reset_index () df_output . drop_duplicates ( 'coef' , inplace = True ) df_output . dropna ( inplace = True ) df_output = df_output [( df_output [ 'coef' ] < 1 ) & ( df_output . coef > np . percentile ( df_output [ 'coef' ], coef_percentile ))] #& (df_output.mse < np.percentile(df_output['mse'],mse_percentile))] #Plot the best pairs. for i in range ( len ( df_output . iloc [:, 0 : 2 ])): colors = [ 'r' , 'b' ] plt . scatter ( df_output . iloc [ i , 0 ], df_output . iloc [ i , 1 ], data = df_input , c = colors ) plt . xlabel ( df_output . iloc [ i , 0 ]) plt . ylabel ( df_output . iloc [ i , 1 ]) plt . legend () plt . show () return df_output #First looking at numbers to see if anything is interesting here. df . corr () Rate Verbal Math Rate 1.000000 -0.886432 -0.773746 Verbal -0.886432 1.000000 0.899871 Math -0.773746 0.899871 1.000000 corr_pairs ( df , 0 ) level_0 level_1 coef 0 Rate Verbal 0.886432 4 Verbal Math 0.899871 ### Conclusion: #There is a strong correlation between doing well on Verbal and well on Math unsuprisingly. #But another interesting takeaway is having a good participation rate is MUCH more strongly correlated #to a good verbal score rather than a good Math score. Which suggests that the more people participated #in a states SAT test the higher the Verbal score but the not as high of a math score. #Another interesting note is that Ohio is the outlier in the Math to Verbal correlation where #a Math score of only 439 was achieved but a Verbal score of 534 was achieved. PROMPT 22. Create box plots for each variable. #Done using seaborn plt . figure ( figsize = ( 10 , 10 )) sns . boxplot ( data = df [[ 'Verbal' , 'Math' ]]) <matplotlib.axes._subplots.AxesSubplot at 0x11c76fcd0> #Rate is on a different scale so needed to be plotted separately. plt . figure ( figsize = ( 5 , 10 )) sns . boxplot ( data = df [ 'Rate' ]) <matplotlib.axes._subplots.AxesSubplot at 0x11cf570d0> BONUS: Using Tableau, create a heat map for each variable using a map of the US. #So I didn't use tableau as I wanted to try my hand at using plotly for the first time. I don't know much #about what is being done below but by using some of their out of the box features you get the benefit #of having some interactive charts inside your jupyter notebook. #Click on the jupyter notebook. import plotly.plotly as py from plotly.graph_objs import * import plotly.tools as tls scl = [[ 0.0 , 'rgb(242,240,247)' ],[ 0.2 , 'rgb(218,218,235)' ],[ 0.4 , 'rgb(188,189,220)' ], \\ [ 0.6 , 'rgb(158,154,200)' ],[ 0.8 , 'rgb(117,107,177)' ],[ 1.0 , 'rgb(84,39,143)' ]] df [ 'text' ] = df [ 'State' ] data = [ dict ( type = 'choropleth' , colorscale = scl , autocolorscale = False , locations = df [ 'State' ], z = df [ 'Rate' ] . astype ( float ), locationmode = 'USA-states' , text = df [ 'text' ], hoverinfo = 'location+z' , marker = dict ( line = dict ( color = 'rgb(255,255,255)' , width = 2 ) ), colorbar = dict ( title = \"Rates\" ) )] layout = dict ( title = 'SAT Participation Rates<br>(Hover for breakdown)' , geo = dict ( scope = 'usa' , projection = dict ( type = 'albers usa' ), showlakes = True , lakecolor = 'rgb(255, 255, 255)' ) ) fig = dict ( data = data , layout = layout ) py . iplot ( fig , validate = False , filename = 'd3-electoral-map' ) High five! You successfuly sent some data to your account on plotly. View your plot in your browser at https://plot.ly/~natejhall89/0 or inside your plot.ly account where it is named 'd3-electoral-map' scl = [[ 0.0 , 'rgb(242,240,247)' ],[ 0.2 , 'rgb(218,218,235)' ],[ 0.4 , 'rgb(188,189,220)' ], \\ [ 0.6 , 'rgb(158,154,200)' ],[ 0.8 , 'rgb(117,107,177)' ],[ 1.0 , 'rgb(84,39,143)' ]] df [ 'text' ] = df [ 'State' ] data = [ dict ( type = 'choropleth' , colorscale = scl , autocolorscale = False , locations = df [ 'State' ], z = df [ 'Math' ] . astype ( float ), locationmode = 'USA-states' , text = df [ 'text' ], hoverinfo = 'location+z' , marker = dict ( line = dict ( color = 'rgb(255,255,255)' , width = 2 ) ), colorbar = dict ( title = \"Rates\" ) )] layout = dict ( title = 'SAT Participation Rates<br>(Hover for breakdown)' , geo = dict ( scope = 'usa' , projection = dict ( type = 'albers usa' ), showlakes = True , lakecolor = 'rgb(255, 255, 255)' ) ) fig = dict ( data = data , layout = layout ) py . iplot ( fig , validate = False , filename = 'd3-electoral-map' ) High five! You successfuly sent some data to your account on plotly. View your plot in your browser at https://plot.ly/~natejhall89/0 or inside your plot.ly account where it is named 'd3-electoral-map' scl = [[ 0.0 , 'rgb(242,240,247)' ],[ 0.2 , 'rgb(218,218,235)' ],[ 0.4 , 'rgb(188,189,220)' ], \\ [ 0.6 , 'rgb(158,154,200)' ],[ 0.8 , 'rgb(117,107,177)' ],[ 1.0 , 'rgb(84,39,143)' ]] df [ 'text' ] = df [ 'State' ] data = [ dict ( type = 'choropleth' , colorscale = scl , autocolorscale = False , locations = df [ 'State' ], z = df [ 'Verbal' ] . astype ( float ), locationmode = 'USA-states' , text = df [ 'text' ], hoverinfo = 'location+z' , marker = dict ( line = dict ( color = 'rgb(255,255,255)' , width = 2 ) ), colorbar = dict ( title = \"Rates\" ) )] layout = dict ( title = 'SAT Participation Rates<br>(Hover for breakdown)' , geo = dict ( scope = 'usa' , projection = dict ( type = 'albers usa' ), showlakes = True , lakecolor = 'rgb(255, 255, 255)' ) ) fig = dict ( data = data , layout = layout ) py . iplot ( fig , validate = False , filename = 'd3-electoral-map' ) High five! You successfuly sent some data to your account on plotly. View your plot in your browser at https://plot.ly/~natejhall89/0 or inside your plot.ly account where it is named 'd3-electoral-map'","tags":"visual_analysis","loc":"http://www.instantinate.com/visual_analysis/sat_scores_visual_analysis.html","title":"SAT Scores Visual Analysis"},{"url":"http://www.instantinate.com/how_to/edit_column_names.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. In this example we'll be working with the billboard hot 100 dataset. The overall structure of this dataset could be setup better for what we need to do with some interesting correlations so we'll be using it for a lot of dataset manipulation examples. One method for organizing a data table is for each row to represent the \"lowest level\" of data available. Here we have each row representing a song, but if you notice the column names there is one for each week. And if you look at the shape of our data we have 83 columns total. Can you guess what the lowest level of data really is for this dataset then? import pandas as pd data = pd . read_csv ( '../data/billboard.csv' ) df = pd . DataFrame ( data ) df . head ( 1 ) year artist.inverted track time genre date.entered date.peaked x1st.week x2nd.week x3rd.week ... x67th.week x68th.week x69th.week x70th.week x71st.week x72nd.week x73rd.week x74th.week x75th.week x76th.week 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Lowest level of data as rows. If you guessed the lowest level would be each a pair combo of each week for each song then you'd be correct. Goal: To get each row to represent the week performance of a song. 1. Get the week performance column names to be just numbers This will allow us to convert them into integers and sort them later. Now its as easy as looking for the things that are in those week columns that are not numbers and removing them. We will take all our edits and apply them to the \"df.columns\" variable. This will overwrite the column names in our dataframe. df . columns = Since df.columns is a list of column names we have to pass it a list back so we start our function with brackets. df . columns = [ ] Remove values with .replace Next we get to say what we want to happen to the list we are going to pass back to df.column to replace its current list of column names. One way to remove values and get down to numbers is to replace the values with just blanks and thereby remove the values. So we use the \"col.replace\" method to define what values in the column header list we're going to replace. df . columns = [ col . replace ( ) Now we get to tell df.columns what we want its new names to be. So what do you think is the first thing we can remove? It looks like \".week\" is on every column name so lets take that out and see what we have left. NOTE: We are using a list comprehension so the values at the beginning of the list are what we will get back. In this case it will be all the columns in df.columns but with the \".week\" removed(replaced with blanks). df . columns = [ col . replace ( '.week' , '' ) for col in df . columns ] Cool! That's gone now what? What's the next thing to be removed? Looks, like they all have \"x\" in front of them (for whatever reason...). Lets replace that with blanks. df . columns = [ col . replace ( '.week' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'x' , '' ) for col in df . columns ] Now we're getting somewhere. We are just left with the values that come after the 1, 2, 3, and 4. This part might be tedious but since from 4 onward every value ends with a \"th\" it's not too bad. So we run the list comprehension another four times to replace these values with blanks. df . columns = [ col . replace ( '.week' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'x' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'st' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'nd' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'rd' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'th' , '' ) for col in df . columns ] Check your work to make sure it worked as expected. You only need to load one row since we are looking at columns. df . head ( 1 ) year arti.inverted track time genre date.entered date.peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Success!!!! Or... so it seems... Can you spot the one thing we inadvertedly did by focusing on the number values and running our replace method across the entire column header list? It looks like the \"artist.inverted\" column name had the \"st\" removed when we applied our replace function to remove the value after the 1. Luckily this is easily fixable and introduces us to another useful column editing method called \".rename\". 2. Fix the \"artist.inverted\" column name This is powerful function that we will use again in other contexts. But for columns it is very useful. Remember that dictionaries contain a key and a value. For this application we are telling pandas to go through all our columns and find the ones that match the \"key\" in our dictionary and when it does to replace it with the \"value\" in our dictionary. So this is less flexible than other dictionaries but very useful for our applications. The .rename method can be applied to both rows and columns so it actually goes with the entire datafarme instead of just the columns like the .replace function we used earlier. It also is different because it doesn't have to be used with a list comprehension it takes arguments that can be found in the pandas documentation here. So we start by applying the .rename method to the dataframe and add ( ) at the end because it takes arguments. df . rename () What do you think we should pass to it as an argument? If you look at the pandas documentation you'll see all kinds of things you can make the .rename() method do. For our application we're going to pass it a dictionary like list of column names. So we use the columns= argument. df . rename ( columns = ) Remember we're passing it a dictionary like list. So we don't use brackets [ ] like normal lists. It has to be the dictionary brackets { }. df . rename ( columns = { }) Now we tell the rename method which columns to look for by giving it \"keys\". df . rename ( columns = { 'arti.inverted' : }) Then all we have to do is say what we want to change the column name to. To make the nameing scheme simpler we'll make it be just \"artist\". #The [:1] is slicing this data to only load one row since we're just interested in columns right now. #Notice it gives the same result as the df.head(1) we used earlier. df . rename ( columns = { 'arti.inverted' : 'artist' })[: 1 ] year artist track time genre date.entered date.peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Check your work to make sure it applied like you expect. df . head ( 1 ) year arti.inverted track time genre date.entered date.peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Huh? What happened? Why isn't it changed? Like all debugging, lets backtrack. What happens when you run the code without the df.head() after it. df . rename ( columns = { 'arti.inverted' : 'artist' })[: 1 ] year artist track time genre date.entered date.peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Ok, it works then, what's going on? Pandas is a very smart and powerful tool. Notice when we first used the .replace function we were applying it to the df.columns variable. This was physically changing the list that was already at that variable and writing over it with a new list that we gave it. Do you see any of that type of thing happening with the .rename() method? Are we writing over something as explicitly as redeclaring a variable? Because pandas does not see you declaring something so explicitly it assumes(BY DEFAULT) that you are not intending to apply the change to the original dataframe and are just interested in running the function in a sort of alternate dimension dataframe. Wait... I thought you said it was smart? That doesn't sound very smart... What would happen if pandas over wrote everything you did and you wanted to do something either as an experiment or as a step in a process? You would have to COPY the entire dataset A LOT. And some of these things can get really big. So instead it creates this one alternate universe result of the method and you get to decide if you want to use it to overwrite the current dataframe or use it as an experiment or step in process with a no harm no foul safety net. Neat huh? Use the inplace argument To get every method that can be applied to a dataframe object to write over the existing dataframe you have to say that you want inplace=True (it defaults to False). This will write over your original dataframe in the place where you apply a method and argument to it. 2a. Fix the arti.inverted column with inplace=True df . rename ( columns = { 'arti.inverted' : 'artist' }, inplace = True ) Now, check your work again. df . head ( 1 ) year artist track time genre date.entered date.peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Yay! Success!!! BONUS: You can rename more than one column at a time. Note that we removed the \".\" from \"artist.inverted\" when we renamed it. However, \"date.entered\" and \"date.peaked\" still have them. From what we know about dictionaries that have have many keys and values so why not simply add a comma after the first key value pair and then type in another one? df . rename ( columns = { 'date.entered' : 'entered' , 'date.peaked' : 'peaked' }, inplace = True ) Check your work and see what happens. df . head ( 1 ) year artist track time genre entered peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Success!! Way to go! To see the next step click here to go to unpivoting columns.","tags":"how_to","loc":"http://www.instantinate.com/how_to/edit_column_names.html","title":"How To: Edit Column Names"},{"url":"http://www.instantinate.com/wrangling_data/editing_column_names.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. In this example we'll be working with the billboard hot 100 dataset. The overall structure of this dataset could be setup better for what we need to do with some interesting correlations so we'll be using it for a lot of dataset manipulation examples. One method for organizing a data table is for each row to represent the \"lowest level\" of data available. Here we have each row representing a song, but if you notice the column names there is one for each week. And if you look at the shape of our data we have 83 columns total. Can you guess what the lowest level of data really is for this dataset then? import pandas as pd data = pd . read_csv ( '../data/billboard.csv' ) df = pd . DataFrame ( data ) df . head ( 1 ) year artist.inverted track time genre date.entered date.peaked x1st.week x2nd.week x3rd.week ... x67th.week x68th.week x69th.week x70th.week x71st.week x72nd.week x73rd.week x74th.week x75th.week x76th.week 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Lowest level of data as rows. If you guessed the lowest level would be each a pair combo of each week for each song then you'd be correct. Goal: To get each row to represent the week performance of a song. 1. Get the week performance column names to be just numbers This will allow us to convert them into integers and sort them later. Now its as easy as looking for the things that are in those week columns that are not numbers and removing them. We will take all our edits and apply them to the \"df.columns\" variable. This will overwrite the column names in our dataframe. df . columns = Since df.columns is a list of column names we have to pass it a list back so we start our function with brackets. df . columns = [ ] Remove values with .replace Next we get to say what we want to happen to the list we are going to pass back to df.column to replace its current list of column names. One way to remove values and get down to numbers is to replace the values with just blanks and thereby remove the values. So we use the \"col.replace\" method to define what values in the column header list we're going to replace. df . columns = [ col . replace ( ) Now we get to tell df.columns what we want its new names to be. So what do you think is the first thing we can remove? It looks like \".week\" is on every column name so lets take that out and see what we have left. NOTE: We are using a list comprehension so the values at the beginning of the list are what we will get back. In this case it will be all the columns in df.columns but with the \".week\" removed(replaced with blanks). df . columns = [ col . replace ( '.week' , '' ) for col in df . columns ] Cool! That's gone now what? What's the next thing to be removed? Looks, like they all have \"x\" in front of them (for whatever reason...). Lets replace that with blanks. df . columns = [ col . replace ( '.week' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'x' , '' ) for col in df . columns ] Now we're getting somewhere. We are just left with the values that come after the 1, 2, 3, and 4. This part might be tedious but since from 4 onward every value ends with a \"th\" it's not too bad. So we run the list comprehension another four times to replace these values with blanks. df . columns = [ col . replace ( '.week' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'x' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'st' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'nd' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'rd' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'th' , '' ) for col in df . columns ] Check your work to make sure it worked as expected. You only need to load one row since we are looking at columns. df . head ( 1 ) year arti.inverted track time genre date.entered date.peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Success!!!! Or... so it seems... Can you spot the one thing we inadvertedly did by focusing on the number values and running our replace method across the entire column header list? It looks like the \"artist.inverted\" column name had the \"st\" removed when we applied our replace function to remove the value after the 1. Luckily this is easily fixable and introduces us to another useful column editing method called \".rename\". 2. Fix the \"artist.inverted\" column name This is powerful function that we will use again in other contexts. But for columns it is very useful. Remember that dictionaries contain a key and a value. For this application we are telling pandas to go through all our columns and find the ones that match the \"key\" in our dictionary and when it does to replace it with the \"value\" in our dictionary. So this is less flexible than other dictionaries but very useful for our applications. The .rename method can be applied to both rows and columns so it actually goes with the entire datafarme instead of just the columns like the .replace function we used earlier. It also is different because it doesn't have to be used with a list comprehension it takes arguments that can be found in the pandas documentation here. So we start by applying the .rename method to the dataframe and add ( ) at the end because it takes arguments. df . rename () What do you think we should pass to it as an argument? If you look at the pandas documentation you'll see all kinds of things you can make the .rename() method do. For our application we're going to pass it a dictionary like list of column names. So we use the columns= argument. df . rename ( columns = ) Remember we're passing it a dictionary like list. So we don't use brackets [ ] like normal lists. It has to be the dictionary brackets { }. df . rename ( columns = { }) Now we tell the rename method which columns to look for by giving it \"keys\". df . rename ( columns = { 'arti.inverted' : }) Then all we have to do is say what we want to change the column name to. To make the nameing scheme simpler we'll make it be just \"artist\". #The [:1] is slicing this data to only load one row since we're just interested in columns right now. #Notice it gives the same result as the df.head(1) we used earlier. df . rename ( columns = { 'arti.inverted' : 'artist' })[: 1 ] year artist track time genre date.entered date.peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Check your work to make sure it applied like you expect. df . head ( 1 ) year arti.inverted track time genre date.entered date.peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Huh? What happened? Why isn't it changed? Like all debugging, lets backtrack. What happens when you run the code without the df.head() after it. df . rename ( columns = { 'arti.inverted' : 'artist' })[: 1 ] year artist track time genre date.entered date.peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Ok, it works then, what's going on? Pandas is a very smart and powerful tool. Notice when we first used the .replace function we were applying it to the df.columns variable. This was physically changing the list that was already at that variable and writing over it with a new list that we gave it. Do you see any of that type of thing happening with the .rename() method? Are we writing over something as explicitly as redeclaring a variable? Because pandas does not see you declaring something so explicitly it assumes(BY DEFAULT) that you are not intending to apply the change to the original dataframe and are just interested in running the function in a sort of alternate dimension dataframe. Wait... I thought you said it was smart? That doesn't sound very smart... What would happen if pandas over wrote everything you did and you wanted to do something either as an experiment or as a step in a process? You would have to COPY the entire dataset A LOT. And some of these things can get really big. So instead it creates this one alternate universe result of the method and you get to decide if you want to use it to overwrite the current dataframe or use it as an experiment or step in process with a no harm no foul safety net. Neat huh? Use the inplace argument To get every method that can be applied to a dataframe object to write over the existing dataframe you have to say that you want inplace=True (it defaults to False). This will write over your original dataframe in the place where you apply a method and argument to it. 2a. Fix the arti.inverted column with inplace=True df . rename ( columns = { 'arti.inverted' : 'artist' }, inplace = True ) Now, check your work again. df . head ( 1 ) year artist track time genre date.entered date.peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Yay! Success!!! BONUS: You can rename more than one column at a time. Note that we removed the \".\" from \"artist.inverted\" when we renamed it. However, \"date.entered\" and \"date.peaked\" still have them. From what we know about dictionaries that have have many keys and values so why not simply add a comma after the first key value pair and then type in another one? df . rename ( columns = { 'date.entered' : 'entered' , 'date.peaked' : 'peaked' }, inplace = True ) Check your work and see what happens. df . head ( 1 ) year artist track time genre entered peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Success!! Way to go! To see the next step click here to go to unpivoting columns.","tags":"wrangling_data","loc":"http://www.instantinate.com/wrangling_data/editing_column_names.html","title":"Editing Column Names"},{"url":"http://www.instantinate.com/how_to/filter_columns.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. This is a slightly more complicated application of filtering out columns than the standard df.drop method you may have seen. But if you stick with each element I promise it will begin to make sense. import pandas as pd data = pd . read_csv ( '../data/billboard.csv' ) df = pd . DataFrame ( data ) df . head ( 1 ) year artist.inverted track time genre date.entered date.peaked x1st.week x2nd.week x3rd.week ... x67th.week x68th.week x69th.week x70th.week x71st.week x72nd.week x73rd.week x74th.week x75th.week x76th.week 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Filter out columns that contain a keyword Remember what method we use to drop columns or rows in a dataset? We will be using the same one again here. So we begin our function by saying... df . drop ( ) Now remember in the documentation what we can pass this method to drop? It says it can take on a \"list-like\" set of items to be dropped. Conceptualize what to filter Conceptually lets lay out what we're trying to accomplish. We want the .drop method to get automatically populated with a list of columns that match some type of filter criteria. To get a list of columns that match a criteria we would use a for loop. However, that is too cumbersome to try to embed in the .drop method which is why list comprehensions are very useful to be placed in arguments. Use list comprehension to return the columns to be dropped Remember that list comprehensions you start by declaring what variable you will be getting and then go on to declare the function setup. For our example we're going to say give me all the columns that are in our dataframe if they have the number \"6\" in them. (No real reason behind 6 this is purely for example purposes). We put our comprehension in brackets since its returning a list and declare the first variable we're getting back \"col\". [ col ] Next we need to give it the for loop (what we want it to look through). In this case we want it to give all the \"col\" values that match our filter so we say \"for col in df.columns\" since we're looking through all the columns. [ col for col in df . columns ] Lastly, we get to delcare our filtering criteria, which we said was columns that have the number \"6\" in them. Note, that all the columns are strings so we need to put the 6 in quotations when declaring what to filter. At any rate, we are looking at all the \"col\" so we want all the ones if \"6\" is in them. [ col for col in df . columns if \"6\" in col ] And that's it we have our filter setup with a easy logic filtering test to apply to all the columns. We can place this straight into the df.drop method and add the other arguments for the column axis and inplace being set to true so we apply this to our dataframe. df . drop ([ col for col in df . columns if \"6\" in col ], axis = 1 , inplace = True ) #Check your work df . head ( 1 ) year artist.inverted track time genre date.entered date.peaked x1st.week x2nd.week x3rd.week ... x55th.week x57th.week x58th.week x59th.week x70th.week x71st.week x72nd.week x73rd.week x74th.week x75th.week 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 66 columns Cool! We see when we scroll over that all the columns from the sixties are removed and the ones like 56. Everything with a 6 is gone you powerful data scientist you. Filter out columns that start with \"__\" Because we did a logical argument with the list comprehension the sky is the limit for what we want it to return. In the below example I am asking it to take it one step further and look at the first letter of all the columns, if the letter is an \"x\" then it should drop that column. df . drop ([ col for col in df . columns if col [: 1 ] == 'x' ], axis = 1 , inplace = True ) #Check your work df . head ( 1 ) year artist.inverted track time genre date.entered date.peaked 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 Bam! You can now conquer any mangy dataset columns you come across.","tags":"how_to","loc":"http://www.instantinate.com/how_to/filter_columns.html","title":"How To: Filter Columns"},{"url":"http://www.instantinate.com/wrangling_data/filter_out_columns.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. This is a slightly more complicated application of filtering out columns than the standard df.drop method you may have seen. But if you stick with each element I promise it will begin to make sense. import pandas as pd data = pd . read_csv ( '../data/billboard.csv' ) df = pd . DataFrame ( data ) df . head ( 1 ) year artist.inverted track time genre date.entered date.peaked x1st.week x2nd.week x3rd.week ... x67th.week x68th.week x69th.week x70th.week x71st.week x72nd.week x73rd.week x74th.week x75th.week x76th.week 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Filter out columns that contain a keyword Remember what method we use to drop columns or rows in a dataset? We will be using the same one again here. So we begin our function by saying... df . drop ( ) Now remember in the documentation what we can pass this method to drop? It says it can take on a \"list-like\" set of items to be dropped. Conceptualize what to filter Conceptually lets lay out what we're trying to accomplish. We want the .drop method to get automatically populated with a list of columns that match some type of filter criteria. To get a list of columns that match a criteria we would use a for loop. However, that is too cumbersome to try to embed in the .drop method which is why list comprehensions are very useful to be placed in arguments. Use list comprehension to return the columns to be dropped Remember that list comprehensions you start by declaring what variable you will be getting and then go on to declare the function setup. For our example we're going to say give me all the columns that are in our dataframe if they have the number \"6\" in them. (No real reason behind 6 this is purely for example purposes). We put our comprehension in brackets since its returning a list and declare the first variable we're getting back \"col\". [ col ] Next we need to give it the for loop (what we want it to look through). In this case we want it to give all the \"col\" values that match our filter so we say \"for col in df.columns\" since we're looking through all the columns. [ col for col in df . columns ] Lastly, we get to delcare our filtering criteria, which we said was columns that have the number \"6\" in them. Note, that all the columns are strings so we need to put the 6 in quotations when declaring what to filter. At any rate, we are looking at all the \"col\" so we want all the ones if \"6\" is in them. [ col for col in df . columns if \"6\" in col ] And that's it we have our filter setup with a easy logic filtering test to apply to all the columns. We can place this straight into the df.drop method and add the other arguments for the column axis and inplace being set to true so we apply this to our dataframe. df . drop ([ col for col in df . columns if \"6\" in col ], axis = 1 , inplace = True ) #Check your work df . head ( 1 ) year artist.inverted track time genre date.entered date.peaked x1st.week x2nd.week x3rd.week ... x55th.week x57th.week x58th.week x59th.week x70th.week x71st.week x72nd.week x73rd.week x74th.week x75th.week 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 66 columns Cool! We see when we scroll over that all the columns from the sixties are removed and the ones like 56. Everything with a 6 is gone you powerful data scientist you. Filter out columns that start with \"__\" Because we did a logical argument with the list comprehension the sky is the limit for what we want it to return. In the below example I am asking it to take it one step further and look at the first letter of all the columns, if the letter is an \"x\" then it should drop that column. df . drop ([ col for col in df . columns if col [: 1 ] == 'x' ], axis = 1 , inplace = True ) #Check your work df . head ( 1 ) year artist.inverted track time genre date.entered date.peaked 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 Bam! You can now conquer any mangy dataset columns you come across.","tags":"wrangling_data","loc":"http://www.instantinate.com/wrangling_data/filter_out_columns.html","title":"Filter Out Columns"},{"url":"http://www.instantinate.com/how_to/remove_columns.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. This is a quick introduction to some basic methods for removing columns from a dataset. We will use the billboard hot 100 dataset as an example. import pandas as pd data = pd . read_csv ( '../data/billboard_weeks_edited.csv' ) df = pd . DataFrame ( data ) df . head ( 1 ) year artist track time genre entered peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Drop a column The simplest way to remove a column is with the \"df.drop\" method from pandas. See more info at the documentation here. We specify what dataframe to apply the .drop method. Then we tell it if we're dropping a column or row with the \"axis\" argument. And finally we use the \"inplace\" argument to either create the results in an alternate universe or apply it back to our current dataframe. #Can you guess why I am dropping this column? Its not just for an example. See if you can figure it out. df . drop ( 'year' , axis = 1 , inplace = True ) df . head ( 1 ) artist track time genre entered peaked 1 2 3 4 ... 67 68 69 70 71 72 73 74 75 76 0 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 33.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 82 columns Drop multiple columns Notice that for the \"labels\" argument in the documentation it says that the elements to drop can be \"list-like\". Which means that if we want to drop multiple columns we can give it a list of column names since that is how all the column headers are stored in pandas anyways. So the function would be... #This time we are doing it for example purposes only so note that inplace is set to False to make this abundantly clear. #It defaults to False automatically but this makes it clear we are not applying it to the original dataframe. df . drop ([ 'entered' , 'peaked' ], axis = 1 , inplace = False )[: 1 ] #note we aren't using df.head to see how it works. Because df.head shows the original dataframe values. #since we are not applying this to the original dataframe we can slice the results to show the first row #only and that acts the same as using df.head(1) artist track time genre 1 2 3 4 5 6 ... 67 68 69 70 71 72 73 74 75 76 0 Destiny's Child Independent Women Part I 3:38 Rock 78 63.0 49.0 33.0 23.0 15.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 80 columns","tags":"how_to","loc":"http://www.instantinate.com/how_to/remove_columns.html","title":"How To: Remove Columns"},{"url":"http://www.instantinate.com/wrangling_data/removing_columns.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. This is a quick introduction to some basic methods for removing columns from a dataset. We will use the billboard hot 100 dataset as an example. import pandas as pd data = pd . read_csv ( '../data/billboard_weeks_edited.csv' ) df = pd . DataFrame ( data ) df . head ( 1 ) year artist track time genre entered peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns Drop a column The simplest way to remove a column is with the \"df.drop\" method from pandas. See more info at the documentation here. We specify what dataframe to apply the .drop method. Then we tell it if we're dropping a column or row with the \"axis\" argument. And finally we use the \"inplace\" argument to either create the results in an alternate universe or apply it back to our current dataframe. #Can you guess why I am dropping this column? Its not just for an example. See if you can figure it out. df . drop ( 'year' , axis = 1 , inplace = True ) df . head ( 1 ) artist track time genre entered peaked 1 2 3 4 ... 67 68 69 70 71 72 73 74 75 76 0 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 33.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 82 columns Drop multiple columns Notice that for the \"labels\" argument in the documentation it says that the elements to drop can be \"list-like\". Which means that if we want to drop multiple columns we can give it a list of column names since that is how all the column headers are stored in pandas anyways. So the function would be... #This time we are doing it for example purposes only so note that inplace is set to False to make this abundantly clear. #It defaults to False automatically but this makes it clear we are not applying it to the original dataframe. df . drop ([ 'entered' , 'peaked' ], axis = 1 , inplace = False )[: 1 ] #note we aren't using df.head to see how it works. Because df.head shows the original dataframe values. #since we are not applying this to the original dataframe we can slice the results to show the first row #only and that acts the same as using df.head(1) artist track time genre 1 2 3 4 5 6 ... 67 68 69 70 71 72 73 74 75 76 0 Destiny's Child Independent Women Part I 3:38 Rock 78 63.0 49.0 33.0 23.0 15.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 80 columns","tags":"wrangling_data","loc":"http://www.instantinate.com/wrangling_data/removing_columns.html","title":"Removing Columns"},{"url":"http://www.instantinate.com/how_to/unpivot_columns.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. We'll be using the billboard hot 100 dataset as the example. We wanted to get the dataset to have rows representing the lowest level of data possible. We cleaned up the column names in this article. Now to finish the the final step to accomplish the goal below. Goal: To get each row to represent the week performance of a song. import pandas as pd data = pd . read_csv ( '../data/billboard_weeks_edited.csv' ) df = pd . DataFrame ( data ) df . head ( 1 ) year artist track time genre entered peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns #Check the shape of the dataset we're starting with. df . shape (317, 83) We know that we have 317 rows and 83 columns. If we are \"unpivoting\" this to be fewer columns we can expect the rows to increase substantially. Remember this for when we check the shape again later. Melt its face off! We will use the \".melt\" function to accomplish what we need for this dataset. More information on it can be found in the documentation here. Notes on pd.melt First off we see that it is not applied to a dataframe, so we don't do \"df.melt\" it actually gets passed the entire dataframe object as an argument and is called by starting with \"pd.\" pd . melt ( df ) Create a new dataframe Simply for the ease of still being able to track back to our original csv in case something doesn't work. We will then declare a new dataframe that will take on the results of the pd.melt( ) function. df2 = pd . melt ( df ) id_vars=[ ] We already passed the dataframe. The id_vars argument will be the columns that will be retained in the dataframe so we'll declare that by listing the columns we want to keep. Note that this argument is taking on a list so we can pass the columns to it with all the columns list methods that we know. For this example i'll just list out the columns since there are so few. df2 = pd . melt ( df , id_vars = [ 'year' , 'artist' , 'track' , 'time' , 'genre' , 'entered' , 'peaked' ]) value_vars=[ ] This argument says it is for stating all the columns to \"unpivot\". Phew, we have a lot of weeks... do we have to list them all? That's the beauty of this function. If we simply leave this blank it will default to unpivot all of the columns that are not listed in the \"id_vars=[ ]\" argument. SWEET!!! var_name=[ ] So we have this tricky thing to do with unpivoting... we'll have all the week columns get turned into row cells and they will need a column header. To do this we declare the \"var_name=[ ]\" argument with the column header we want. df2 = pd . melt ( df , id_vars = [ 'year' , 'artist' , 'track' , 'time' , 'genre' , 'entered' , 'peaked' ], var_name = 'week' ) value_name[ ] Now what about all the values that are listed in each week column right now? Where are those supposed to go? Glad you asked that. These are the values that we are \"unpivoting\" but they to will need a column header since we just took the week columns away. We know that those values represent the weekly ranking of a song during that week so we'll \"unpivot\" those values to a new column called \"rank\" df2 = pd . melt ( df , id_vars = [ 'year' , 'artist' , 'track' , 'time' , 'genre' , 'entered' , 'peaked' ], var_name = 'week' , value_name = 'rank' ) Check your work. df2 . head () year artist track time genre entered peaked week rank 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 1 78.0 1 2000 Santana Maria, Maria 4:18 Rock 2000-02-12 2000-04-08 1 15.0 2 2000 Savage Garden I Knew I Loved You 4:07 Rock 1999-10-23 2000-01-29 1 71.0 3 2000 Madonna Music 3:45 Rock 2000-08-12 2000-09-16 1 41.0 4 2000 Aguilera, Christina Come On Over Baby (All I Want Is You) 3:38 Rock 2000-08-05 2000-10-14 1 57.0 Cool! It worked! We now have the rows representing the lowest level of data in our dataset. What did this do to the shape of our table? Remember what it was at the beginning? Lets check it again to see what happened. df2 . shape (24092, 9) Woah. That is MUCH longer. Nothing to worry about though. You'll find that pandas can handle this length of a data table easily. Most importantly we have our dataset setup in a way where we can work with it much better.","tags":"how_to","loc":"http://www.instantinate.com/how_to/unpivot_columns.html","title":"How To: Unpivot Columns"},{"url":"http://www.instantinate.com/wrangling_data/unpivoting_columns.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. We'll be using the billboard hot 100 dataset as the example. We wanted to get the dataset to have rows representing the lowest level of data possible. We cleaned up the column names in this article. Now to finish the the final step to accomplish the goal below. Goal: To get each row to represent the week performance of a song. import pandas as pd data = pd . read_csv ( '../data/billboard_weeks_edited.csv' ) df = pd . DataFrame ( data ) df . head ( 1 ) year artist track time genre entered peaked 1 2 3 ... 67 68 69 70 71 72 73 74 75 76 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows × 83 columns #Check the shape of the dataset we're starting with. df . shape (317, 83) We know that we have 317 rows and 83 columns. If we are \"unpivoting\" this to be fewer columns we can expect the rows to increase substantially. Remember this for when we check the shape again later. Melt its face off! We will use the \".melt\" function to accomplish what we need for this dataset. More information on it can be found in the documentation here. Notes on pd.melt First off we see that it is not applied to a dataframe, so we don't do \"df.melt\" it actually gets passed the entire dataframe object as an argument and is called by starting with \"pd.\" pd . melt ( df ) Create a new dataframe Simply for the ease of still being able to track back to our original csv in case something doesn't work. We will then declare a new dataframe that will take on the results of the pd.melt( ) function. df2 = pd . melt ( df ) id_vars=[ ] We already passed the dataframe. The id_vars argument will be the columns that will be retained in the dataframe so we'll declare that by listing the columns we want to keep. Note that this argument is taking on a list so we can pass the columns to it with all the columns list methods that we know. For this example i'll just list out the columns since there are so few. df2 = pd . melt ( df , id_vars = [ 'year' , 'artist' , 'track' , 'time' , 'genre' , 'entered' , 'peaked' ]) value_vars=[ ] This argument says it is for stating all the columns to \"unpivot\". Phew, we have a lot of weeks... do we have to list them all? That's the beauty of this function. If we simply leave this blank it will default to unpivot all of the columns that are not listed in the \"id_vars=[ ]\" argument. SWEET!!! var_name=[ ] So we have this tricky thing to do with unpivoting... we'll have all the week columns get turned into row cells and they will need a column header. To do this we declare the \"var_name=[ ]\" argument with the column header we want. df2 = pd . melt ( df , id_vars = [ 'year' , 'artist' , 'track' , 'time' , 'genre' , 'entered' , 'peaked' ], var_name = 'week' ) value_name[ ] Now what about all the values that are listed in each week column right now? Where are those supposed to go? Glad you asked that. These are the values that we are \"unpivoting\" but they to will need a column header since we just took the week columns away. We know that those values represent the weekly ranking of a song during that week so we'll \"unpivot\" those values to a new column called \"rank\" df2 = pd . melt ( df , id_vars = [ 'year' , 'artist' , 'track' , 'time' , 'genre' , 'entered' , 'peaked' ], var_name = 'week' , value_name = 'rank' ) Check your work. df2 . head () year artist track time genre entered peaked week rank 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 1 78.0 1 2000 Santana Maria, Maria 4:18 Rock 2000-02-12 2000-04-08 1 15.0 2 2000 Savage Garden I Knew I Loved You 4:07 Rock 1999-10-23 2000-01-29 1 71.0 3 2000 Madonna Music 3:45 Rock 2000-08-12 2000-09-16 1 41.0 4 2000 Aguilera, Christina Come On Over Baby (All I Want Is You) 3:38 Rock 2000-08-05 2000-10-14 1 57.0 Cool! It worked! We now have the rows representing the lowest level of data in our dataset. What did this do to the shape of our table? Remember what it was at the beginning? Lets check it again to see what happened. df2 . shape (24092, 9) Woah. That is MUCH longer. Nothing to worry about though. You'll find that pandas can handle this length of a data table easily. Most importantly we have our dataset setup in a way where we can work with it much better.","tags":"wrangling_data","loc":"http://www.instantinate.com/wrangling_data/unpivoting_columns.html","title":"Unpivoting Columns"},{"url":"http://www.instantinate.com/python/dictionary_comprehension_for_list_of_lists.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. Turn a list of lists into a dictionary with one line of code... the magical comprehension. Click here for a refresher on how to read in data with the csv.reader import csv with open ( '../data/sales_data.csv' , 'rU' ) as f : data = [] reader = csv . reader ( f ) for row in reader : data . append ( row ) print data [['volume_sold', '2015_margin', '2015_q1_sales', '2016_q1_sales'], ['18.4207604861', '93.8022814583', '337166.53', '337804.05'], ['4.77650991918', '21.0824246877', '22351.86', '21736.63'], ['16.6024006077', '93.6124943024', '277764.46', '306942.27'], ['4.29611149826', '16.8247038328', '16805.11', '9307.75'], ['8.15602328201', '35.0114570034', '54411.42', '58939.9'], ['5.00512242518', '31.8774372328', '255939.81', '332979.03'], ['14.60675', '76.5189730216', '319020.69', '302592.88'], ['4.45646649485', '19.3373453608', '45340.33', '55315.23'], ['5.04752965097', '26.142470349', '57849.23', '42398.57'], ['5.38807023767', '22.4270237673', '51031.04', '56241.57'], ['9.34734863474', '41.892132964', '68657.91', '3536.14'], ['10.9303977273', '66.4030492424', '4151.93', '137416.93'], ['6.27020860495', '47.8693242069', '121837.56', '158476.55'], ['12.3959191176', '86.7601504011', '146725.31', '125731.51'], ['4.55771189614', '22.9481762576', '119287.76', '21834.49'], ['4.20012242627', '18.7060545353', '20335.03', '39609.55'], ['10.2528698945', '44.0411766297', '110552.94', '204038.87'], ['12.0767847594', '62.1990040107', '204237.78', '15689.8'], ['3.7250952381', '14.2518095238', '16939.15', '48545.69'], ['3.21072662722', '16.0432686391', '55498.12', '16320.74'], ['6.29097142857', '25.1911714286', '15983.98', '53182.55'], ['7.43482131661', '31.7530658307', '71758.66', '30402.43'], ['4.37622478386', '23.1614514016', '62988.17', '47217.4'], ['12.9889127838', '48.8207407407', '29637.75', '6367.76'], ['11.6974557522', '73.2315044248', '48759.71', '329252.09'], ['5.96517512509', '23.4503335716', '89736.7', '332976.05'], ['3.94522273425', '14.1447926267', '5577.61', '234926.02'], ['7.36958530901', '36.4085284899', '310035.66', '151934.45'], ['7.34350882699', '36.1718619066', '310718.21', '314068.92'], ['12.3500273544', '59.8934779211', '258284.84', '61847.52'], ['8.41791967737', '37.1085548647', '150049.71', '203644.27'], ['10.2608361718', '52.4055916932', '309568.16', '123632.78'], ['7.82435369972', '30.681099171', '66468.32', '1050711.75'], ['10.3314300532', '48.1333683392', '321983.24', '149791.31'], ['12.5284878049', '47.7406803594', '115531.13', '61560.7'], ['18.7447505256', '97.2243398739', '926706.76', '260966.41'], ['6.65773264189', '31.2923926822', '157981.2', '160278.07'], ['10.6321289355', '35.27017991', '51078.36', '78108.56'], ['6.92770422965', '31.9091555963', '272703.74', '253886.88'], ['6.61817422161', '29.1482532051', '180760.93', '173395.13'], ['7.12444444444', '32.6235916667', '72185.66', '91524.76'], ['9.84966032435', '47.9893704508', '263161.4', '247802.36'], ['11.5058377559', '55.5221865049', '164809.62', '115591.86'], ['6.30981315215', '31.941637952', '60986.22', '233350.35'], ['10.1866219839', '49.3420628537', '210690.45', '84940.69'], ['10.1221793301', '42.8693852124', '139068.33', '115993.15'], ['10.8003469032', '53.1849073341', '209403.19', '125207.35'], ['7.26782845188', '25.4050062762', '75110.03', '161300.14'], ['10.6737166742', '43.9390962494', '123291.57', '164939.85'], ['9.15026865672', '44.5348318408', '157591.18', '147632.84'], ['8.12418187744', '39.530065189', '163684.93', '168603.49'], ['6.27579970306', '31.5106033203', '146850.63', '342772.02'], ['10.6772953319', '50.1331972789', '143950.01', '102829.3'], ['5.88898828541', '28.7115801384', '136167.32', '216415.08'], ['10.6401714545', '52.4235630748', '327172.32', '174492.82'], ['4.75559643255', '24.0028010033', '105869.94', '41213.34'], ['10.246884068', '47.3184344342', '176266.68', '91213.98'], ['10.29268081', '49.1944300868', '176720.07', '256707.1'], ['4.41819548872', '19.9170827068', '44836.7', '75866.02'], ['7.10134734573', '32.71362436', '65830.25', '50434.54'], ['8.00611901938', '36.2773863187', '228680.53', '141712.31'], ['7.79050337838', '35.2223614865', '73810.68', '91203.94'], ['11.1293822598', '40.0093030623', '54655.98', '156730.02'], ['9.34847653987', '45.5890982815', '156056.28', '24206.67'], ['6.31088643791', '28.2592197712', '80923.05', '218335.21'], ['11.6256060606', '55.1925378788', '167606.75', '325127.17'], ['6.65440717629', '29.106349454', '25575.88', '94229.41'], ['7.93041476808', '39.1116473887', '211902.95', '117784.92'], ['9.003562316', '46.7565544652', '312502.67', '166847.54'], ['14.4394353772', '57.2886237318', '85217.68', '73117.79'], ['11.0115404852', '46.2992604502', '89525.66', '100545.74'], ['5.72389564186', '25.5950192707', '176252.03', '68966.63'], ['7.77732012195', '36.7157591463', '68258.49', '96059.65'], ['4.75918372602', '20.0006463242', '88008.31', '211600.93'], ['7.78586691659', '29.0692439863', '67219.54', '234774.87'], ['5.03499140759', '22.7097016091', '100722.77', '268226.7'], ['11.6845098446', '58.9404393782', '147242.83', '58122.85'], ['5.14772910448', '25.7666753731', '227702.34', '106662.37'], ['10.0860303763', '49.1620319871', '286763.37', '147781.41'], ['7.94465682362', '33.9956304868', '59531.82', '67514.95'], ['5.29439978635', '24.4766550941', '122811.86', '170466.11'], ['11.8265363003', '52.3184580805', '156513.07', '94158.15'], ['6.300925868', '30.5095964967', '55538.11', '139676.38'], ['8.64269487751', '36.2843302577', '143498.37', '167692.5'], ['6.04822838631', '30.1712350678', '84998.58', '180371.58'], ['9.47492913676', '41.0171194552', '138010.02', '298889.32'], ['3.99185767098', '20.0513661569', '146424.29', '133868.58'], ['8.59207381371', '45.5986531395', '175475.49', '39138.81'], ['7.21148957299', '35.8870443261', '247716.78', '343447.68'], ['8.19108557079', '37.9469113386', '130512.99', '191091.14'], ['7.69531160115', '29.3906663471', '52054.1', '184729.87'], ['13.6351500118', '64.7439062131', '332692.67', '37179.58'], ['6.96681681682', '35.0125088725', '135418.28', '40160.26'], ['11.2323237697', '52.2156089683', '163104.8', '40970.96'], ['5.09546375267', '22.4304264392', '45612.45', '135871.39'], ['11.9368836649', '49.1816335142', '56995.82', '92843.04'], ['5.90627376426', '25.0323193916', '39867.38', '295483.47'], ['9.12709469154', '45.8722118604', '133081.37', '130377.49'], ['7.7544572697', '34.7604217536', '108362.38', '192747.83'], ['7.58599675412', '39.9868794518', '271021.94', '103958.63'], ['7.43725207583', '34.6033448222', '134589.33', '106452.13'], ['9.8798900768', '41.6839918687', '216171.01', '12380.85'], ['9.30319388778', '40.8443061122', '104354.49', '94010.77'], ['9.21365050557', '44.7063598652', '105046.24', '51773.27'], ['5.18177205308', '21.1562997658', '13677.13', '242057.36'], ['8.55507774799', '41.547077748', '89053.08', '392226.28'], ['5.78126590331', '24.6607061069', '51427.05', '369424.59'], ['8.0710230118', '40.1006629307', '264408.55', '73141.35'], ['10.1250032713', '50.3450808487', '294990.21', '292690.86'], ['11.0196516733', '51.4318879891', '358098.75', '172189.31'], ['8.17666774927', '32.3875251705', '68077.3', '211032.4'], ['9.42171292239', '53.4813935145', '313345.86', '171119.44'], ['4.85870921867', '24.5552275389', '163324.44', '201433.01'], ['9.31378525791', '43.8089558984', '203099.8', '200599.95'], ['8.30018036767', '43.4441935484', '104044.33', '476439.16'], ['6.50688776965', '33.4178523235', '202835.96', '215106.77'], ['9.5852099677', '44.3454176281', '192934.41', '204808.54'], ['12.45572275', '65.5244131168', '447305.5', '37858.67'], ['8.09288312646', '39.7529703822', '205170.43', '382790.84'], ['8.68651837806', '46.2175087515', '201282.29', '58332.41'], ['8.34731752963', '32.7325389894', '36855.39', '2443253.18'], ['9.29224055739', '49.786953612', '381036.91', '209066.17'], ['9.77711182109', '44.9972396166', '46509.65', '137697.56'], ['29.878030039', '168.245861698', '2337324.42', '129489.89'], ['8.78393692369', '49.313768542', '208389.88', '72810.95'], ['11.9757685161', '56.569739846', '145742.16', '158368.58'], ['11.1401021385', '47.8089690393', '187407.93', '153305.98'], ['7.5605488194', '30.9038417358', '62335.59', '289199.43'], ['7.39098798637', '37.2649829658', '138878.63', '241311.79'], ['6.43360588592', '35.850617415', '169131.81', '343060.63'], ['13.7999774485', '68.1836227448', '280506.28', '128500.31'], ['6.44703955254', '34.2461556133', '223641.14', '79554.12'], ['8.01794477751', '46.0017090264', '361865.13', '191155.87'], ['6.2553554256', '26.9316699155', '134325.62', '111207.97'], ['9.69742181905', '48.3078228694', '145361.36', '83399.61'], ['7.77268351232', '36.216199621', '185580.5', '25290.33'], ['8.75192030725', '37.1569323092', '93901.53', '20962.08'], ['6.79288937945', '33.5548344371', '68438.51', '15307.3'], ['7.68249438202', '33.223011236', '32698.19', '10464.02'], ['4.38545511613', '20.8197112367', '22829.66', '52246.46'], ['3.60671020408', '15.8903673469', '13141.2', '75566.37'], ['8.45364705882', '35.4949411765', '8738.45', '103585.1'], ['5.21488185976', '26.0287842988', '48441.46', '276396.86'], ['8.40056149733', '37.0314331551', '87115.74', '305851.47'], ['6.84136327817', '29.1820981087', '102794.01', '260453.94'], ['12.5099672623', '72.4642572637', '306947.29', '30441.76'], ['9.0148700565', '52.1879774011', '225615.38', '10564.6'], ['7.20036424796', '40.9748793849', '327334.28', '19357.49'], ['5.77809677419', '23.8230645161', '27114.29', '613414.14'], ['4.94129392971', '24.1203833866', '7539.14', '63793.71'], ['6.00045070423', '26.6531361502', '20378.73', '78706.47'], ['11.9971174753', '65.067257829', '501953.12', '22675.76'], ['10.6377691184', '51.4916463712', '66097.53', '4566.74'], ['8.56422809829', '36.8113087607', '89327.88', '14050.14'], ['8.62268641471', '35.087834525', '20534.72', '11883.3'], ['4.83114713217', '17.9033416459', '5742.23', '11597.18'], ['10.2701848998', '38.2349306626', '15987.52', '23801.1'], ['12.5816945607', '41.9222384937', '13518.07', '22824.14'], ['16.0599706745', '50.8491202346', '17574.68', '18957.2'], ['11.8677385892', '36.282033195', '24357.22', '19832.94'], ['10.2945011338', '39.7768253968', '29611.32', '4258.14'], ['4.17606557377', '16.4770435274', '18571.65', '17116.11'], ['9.36189873418', '35.4246202532', '18712.28', '7499.47'], ['11.0917085427', '48.8443718593', '8458.2', '11367.4'], ['5.3244966443', '20.2525251678', '19089.74', '8041.09'], ['6.63090439276', '24.2994315245', '7305.46', '11966.94'], ['8.58392405063', '34.2229535865', '14796.7', '12707.72'], ['5.53106109325', '25.2890353698', '8200.55', '22130.31'], ['6.13912310287', '26.0607419899', '11412.54', '20817.32'], ['8.47737603306', '33.6023140496', '11228.97', '16786.98'], ['8.44393241167', '37.4071121352', '17232.45', '19203.82'], ['5.15196394076', '20.6980424984', '21340.78', '25302.92'], ['6.53706864564', '22.8882189239', '16617.85', '18650.96'], ['8.50044523598', '27.5077292965', '21711.71', '14626.29'], ['3.93154326923', '21.8116586538', '28128.95', '46323.73'], ['6.16368913858', '25.4285205993', '21803.2', '535381.86'], ['4.90444711538', '19.5683173077', '12517.29', '812011.78'], ['7.40241271087', '26.9353354257', '53988.92', '206247.57'], ['47.5032692308', '235.730677515', '555707.4', '6402.78'], ['55.7391800938', '268.869600245', '1082136.01', '100765.67'], ['11.8407803201', '56.4333884407', '192089.46', '20098.61'], ['7.00229357798', '26.4409174312', '5574.99', '68230.36'], ['8.75314206706', '45.2938472174', '80241.27', '596063.0'], ['3.14774130328', '15.5945516903', '27043.54', '87471.43'], ['7.1967787944', '27.2886517761', '61977.54', '431990.7'], ['76.2036918138', '367.225653291', '977772.62', '136717.57'], ['10.8043371086', '42.0311992005', '41905.18', '30008.81'], ['10.705327051', '45.8914772727', '87839.45', '19420.34'], ['51.8006862745', '255.153235294', '445058.32', '274752.6'], ['5.88277871731', '27.5122713672', '127495.18', '10315.35'], ['6.68640645161', '26.1030967742', '23874.67', '45252.42'], ['5.83335488041', '23.7416031028', '21535.87', '433832.28'], ['45.5560956385', '218.008349974', '276096.18', '74215.43'], ['5.17260575296', '21.5487817259', '8506.79', '78137.29'], ['10.118018018', '46.7007087087', '49163.01', '83915.72'], ['51.6755374204', '233.533188694', '434110.57', '372240.72'], ['2.79463149728', '11.9961176992', '73789.38', '148021.9'], ['7.61169779853', '38.8099733155', '88006.84', '31184.18'], ['15.6976512739', '105.035207006', '117958.96', '33324.36'], ['50.2758932156', '225.055138499', '407738.79', '32079.13']] Every dictionary needs some keys. Below we're \"slicing\" the data from the top row of the dataset (which is our headers) and assigning it to the \"header\" variable. Can you see where this is going? header = data [ 0 ][:] Next we're going back to the list of lists to remove that first row so we don't have any conflicts. At the end we'll end up with two lists one that can be used as \"Keys\" (the header row), and one that can be used as \"Values\" (everything else). data . pop ( 0 ) header = data [ 0 ][:] data . pop ( 0 ) ['volume_sold', '2015_margin', '2015_q1_sales', '2016_q1_sales'] Now for that scary comprehension thingy. So we've got two lists, one that has only 4 items and one that has lots and lots of items and each of those items has 4 items embedded(list of lists). Conceptually, we want to take the 1st entry from each of those embedded lists and map it to the first key value. Then we want to get the second and map it to the second value. etc. etc. Here's a refresher on how dictionary comprehensions are laid out. { key_extractor ( obj ): value_extractor ( obj ) for i , obj in enumerate ( objects )} Values to be returned. Remember that comprehensions are shorthand for other types of functions. So it starts with the values that are going to be returned, in this case we're going to extract a key, and a value for each list in the big list. { column_name : [ row [ i ] for row in data ] Values to iterate through. Then we define the variable \"i\" to iterate through each list. Since we are going through two things at a time (key and value) we need to specify the second item to pull when we iterrate through which will be the \"column name\". That will act as our \"key\" in the dictionary. { column_name : [ row [ i ] for row in data ] for i , column_name Enumerate is the key. Next we need to get two values to iterate through. The \"enumerate\" function is the magical key for this. It will return both the index number and the value in a list that can then be passed to the variables to iterate through.... phew that was a mouthful... so if that was difficult to follow unpack each statement and trace it to the full function until you see what is happening. { column_name : [ row [ i ] for row in data ] for i , column_name in enumerate ( header )} Did you notice that other thing in here? Yeah I did see that mean thing in there next to the first \"column name\". Because we're iterating over a list of lists we need to define all of the items to be returned as values with that key. So we put a second simple List Comprehension to go through all the list of lists and return the value that is at each index point that matches the \"i\" variable. [ row [ i ] for row in data ] Now to see how it all goes together and returns a dictionary of values called \"Sales_Data\" Notice that each of the \"Column Headers\" (Dictionary Keys), has a list of \"Dictionary Values\" that is the corresponding list item for each position (The first item in the list goes to the first column, the second item goes to the second column etc.). sales_data = { column_name : [ row [ i ] for row in data ] for i , column_name in enumerate ( header )} print sales_data {'volume_sold': ['18.4207604861', '4.77650991918', '16.6024006077', '4.29611149826', '8.15602328201', '5.00512242518', '14.60675', '4.45646649485', '5.04752965097', '5.38807023767', '9.34734863474', '10.9303977273', '6.27020860495', '12.3959191176', '4.55771189614', '4.20012242627', '10.2528698945', '12.0767847594', '3.7250952381', '3.21072662722', '6.29097142857', '7.43482131661', '4.37622478386', '12.9889127838', '11.6974557522', '5.96517512509', '3.94522273425', '7.36958530901', '7.34350882699', '12.3500273544', '8.41791967737', '10.2608361718', '7.82435369972', '10.3314300532', '12.5284878049', '18.7447505256', '6.65773264189', '10.6321289355', '6.92770422965', '6.61817422161', '7.12444444444', '9.84966032435', '11.5058377559', '6.30981315215', '10.1866219839', '10.1221793301', '10.8003469032', '7.26782845188', '10.6737166742', '9.15026865672', '8.12418187744', '6.27579970306', '10.6772953319', '5.88898828541', '10.6401714545', '4.75559643255', '10.246884068', '10.29268081', '4.41819548872', '7.10134734573', '8.00611901938', '7.79050337838', '11.1293822598', '9.34847653987', '6.31088643791', '11.6256060606', '6.65440717629', '7.93041476808', '9.003562316', '14.4394353772', '11.0115404852', '5.72389564186', '7.77732012195', '4.75918372602', '7.78586691659', '5.03499140759', '11.6845098446', '5.14772910448', '10.0860303763', '7.94465682362', '5.29439978635', '11.8265363003', '6.300925868', '8.64269487751', '6.04822838631', '9.47492913676', '3.99185767098', '8.59207381371', '7.21148957299', '8.19108557079', '7.69531160115', '13.6351500118', '6.96681681682', '11.2323237697', '5.09546375267', '11.9368836649', '5.90627376426', '9.12709469154', '7.7544572697', '7.58599675412', '7.43725207583', '9.8798900768', '9.30319388778', '9.21365050557', '5.18177205308', '8.55507774799', '5.78126590331', '8.0710230118', '10.1250032713', '11.0196516733', '8.17666774927', '9.42171292239', '4.85870921867', '9.31378525791', '8.30018036767', '6.50688776965', '9.5852099677', '12.45572275', '8.09288312646', '8.68651837806', '8.34731752963', '9.29224055739', '9.77711182109', '29.878030039', '8.78393692369', '11.9757685161', '11.1401021385', '7.5605488194', '7.39098798637', '6.43360588592', '13.7999774485', '6.44703955254', '8.01794477751', '6.2553554256', '9.69742181905', '7.77268351232', '8.75192030725', '6.79288937945', '7.68249438202', '4.38545511613', '3.60671020408', '8.45364705882', '5.21488185976', '8.40056149733', '6.84136327817', '12.5099672623', '9.0148700565', '7.20036424796', '5.77809677419', '4.94129392971', '6.00045070423', '11.9971174753', '10.6377691184', '8.56422809829', '8.62268641471', '4.83114713217', '10.2701848998', '12.5816945607', '16.0599706745', '11.8677385892', '10.2945011338', '4.17606557377', '9.36189873418', '11.0917085427', '5.3244966443', '6.63090439276', '8.58392405063', '5.53106109325', '6.13912310287', '8.47737603306', '8.44393241167', '5.15196394076', '6.53706864564', '8.50044523598', '3.93154326923', '6.16368913858', '4.90444711538', '7.40241271087', '47.5032692308', '55.7391800938', '11.8407803201', '7.00229357798', '8.75314206706', '3.14774130328', '7.1967787944', '76.2036918138', '10.8043371086', '10.705327051', '51.8006862745', '5.88277871731', '6.68640645161', '5.83335488041', '45.5560956385', '5.17260575296', '10.118018018', '51.6755374204', '2.79463149728', '7.61169779853', '15.6976512739', '50.2758932156'], '2015_q1_sales': ['337166.53', '22351.86', '277764.46', '16805.11', '54411.42', '255939.81', '319020.69', '45340.33', '57849.23', '51031.04', '68657.91', '4151.93', '121837.56', '146725.31', '119287.76', '20335.03', '110552.94', '204237.78', '16939.15', '55498.12', '15983.98', '71758.66', '62988.17', '29637.75', '48759.71', '89736.7', '5577.61', '310035.66', '310718.21', '258284.84', '150049.71', '309568.16', '66468.32', '321983.24', '115531.13', '926706.76', '157981.2', '51078.36', '272703.74', '180760.93', '72185.66', '263161.4', '164809.62', '60986.22', '210690.45', '139068.33', '209403.19', '75110.03', '123291.57', '157591.18', '163684.93', '146850.63', '143950.01', '136167.32', '327172.32', '105869.94', '176266.68', '176720.07', '44836.7', '65830.25', '228680.53', '73810.68', '54655.98', '156056.28', '80923.05', '167606.75', '25575.88', '211902.95', '312502.67', '85217.68', '89525.66', '176252.03', '68258.49', '88008.31', '67219.54', '100722.77', '147242.83', '227702.34', '286763.37', '59531.82', '122811.86', '156513.07', '55538.11', '143498.37', '84998.58', '138010.02', '146424.29', '175475.49', '247716.78', '130512.99', '52054.1', '332692.67', '135418.28', '163104.8', '45612.45', '56995.82', '39867.38', '133081.37', '108362.38', '271021.94', '134589.33', '216171.01', '104354.49', '105046.24', '13677.13', '89053.08', '51427.05', '264408.55', '294990.21', '358098.75', '68077.3', '313345.86', '163324.44', '203099.8', '104044.33', '202835.96', '192934.41', '447305.5', '205170.43', '201282.29', '36855.39', '381036.91', '46509.65', '2337324.42', '208389.88', '145742.16', '187407.93', '62335.59', '138878.63', '169131.81', '280506.28', '223641.14', '361865.13', '134325.62', '145361.36', '185580.5', '93901.53', '68438.51', '32698.19', '22829.66', '13141.2', '8738.45', '48441.46', '87115.74', '102794.01', '306947.29', '225615.38', '327334.28', '27114.29', '7539.14', '20378.73', '501953.12', '66097.53', '89327.88', '20534.72', '5742.23', '15987.52', '13518.07', '17574.68', '24357.22', '29611.32', '18571.65', '18712.28', '8458.2', '19089.74', '7305.46', '14796.7', '8200.55', '11412.54', '11228.97', '17232.45', '21340.78', '16617.85', '21711.71', '28128.95', '21803.2', '12517.29', '53988.92', '555707.4', '1082136.01', '192089.46', '5574.99', '80241.27', '27043.54', '61977.54', '977772.62', '41905.18', '87839.45', '445058.32', '127495.18', '23874.67', '21535.87', '276096.18', '8506.79', '49163.01', '434110.57', '73789.38', '88006.84', '117958.96', '407738.79'], '2016_q1_sales': ['337804.05', '21736.63', '306942.27', '9307.75', '58939.9', '332979.03', '302592.88', '55315.23', '42398.57', '56241.57', '3536.14', '137416.93', '158476.55', '125731.51', '21834.49', '39609.55', '204038.87', '15689.8', '48545.69', '16320.74', '53182.55', '30402.43', '47217.4', '6367.76', '329252.09', '332976.05', '234926.02', '151934.45', '314068.92', '61847.52', '203644.27', '123632.78', '1050711.75', '149791.31', '61560.7', '260966.41', '160278.07', '78108.56', '253886.88', '173395.13', '91524.76', '247802.36', '115591.86', '233350.35', '84940.69', '115993.15', '125207.35', '161300.14', '164939.85', '147632.84', '168603.49', '342772.02', '102829.3', '216415.08', '174492.82', '41213.34', '91213.98', '256707.1', '75866.02', '50434.54', '141712.31', '91203.94', '156730.02', '24206.67', '218335.21', '325127.17', '94229.41', '117784.92', '166847.54', '73117.79', '100545.74', '68966.63', '96059.65', '211600.93', '234774.87', '268226.7', '58122.85', '106662.37', '147781.41', '67514.95', '170466.11', '94158.15', '139676.38', '167692.5', '180371.58', '298889.32', '133868.58', '39138.81', '343447.68', '191091.14', '184729.87', '37179.58', '40160.26', '40970.96', '135871.39', '92843.04', '295483.47', '130377.49', '192747.83', '103958.63', '106452.13', '12380.85', '94010.77', '51773.27', '242057.36', '392226.28', '369424.59', '73141.35', '292690.86', '172189.31', '211032.4', '171119.44', '201433.01', '200599.95', '476439.16', '215106.77', '204808.54', '37858.67', '382790.84', '58332.41', '2443253.18', '209066.17', '137697.56', '129489.89', '72810.95', '158368.58', '153305.98', '289199.43', '241311.79', '343060.63', '128500.31', '79554.12', '191155.87', '111207.97', '83399.61', '25290.33', '20962.08', '15307.3', '10464.02', '52246.46', '75566.37', '103585.1', '276396.86', '305851.47', '260453.94', '30441.76', '10564.6', '19357.49', '613414.14', '63793.71', '78706.47', '22675.76', '4566.74', '14050.14', '11883.3', '11597.18', '23801.1', '22824.14', '18957.2', '19832.94', '4258.14', '17116.11', '7499.47', '11367.4', '8041.09', '11966.94', '12707.72', '22130.31', '20817.32', '16786.98', '19203.82', '25302.92', '18650.96', '14626.29', '46323.73', '535381.86', '812011.78', '206247.57', '6402.78', '100765.67', '20098.61', '68230.36', '596063.0', '87471.43', '431990.7', '136717.57', '30008.81', '19420.34', '274752.6', '10315.35', '45252.42', '433832.28', '74215.43', '78137.29', '83915.72', '372240.72', '148021.9', '31184.18', '33324.36', '32079.13'], '2015_margin': ['93.8022814583', '21.0824246877', '93.6124943024', '16.8247038328', '35.0114570034', '31.8774372328', '76.5189730216', '19.3373453608', '26.142470349', '22.4270237673', '41.892132964', '66.4030492424', '47.8693242069', '86.7601504011', '22.9481762576', '18.7060545353', '44.0411766297', '62.1990040107', '14.2518095238', '16.0432686391', '25.1911714286', '31.7530658307', '23.1614514016', '48.8207407407', '73.2315044248', '23.4503335716', '14.1447926267', '36.4085284899', '36.1718619066', '59.8934779211', '37.1085548647', '52.4055916932', '30.681099171', '48.1333683392', '47.7406803594', '97.2243398739', '31.2923926822', '35.27017991', '31.9091555963', '29.1482532051', '32.6235916667', '47.9893704508', '55.5221865049', '31.941637952', '49.3420628537', '42.8693852124', '53.1849073341', '25.4050062762', '43.9390962494', '44.5348318408', '39.530065189', '31.5106033203', '50.1331972789', '28.7115801384', '52.4235630748', '24.0028010033', '47.3184344342', '49.1944300868', '19.9170827068', '32.71362436', '36.2773863187', '35.2223614865', '40.0093030623', '45.5890982815', '28.2592197712', '55.1925378788', '29.106349454', '39.1116473887', '46.7565544652', '57.2886237318', '46.2992604502', '25.5950192707', '36.7157591463', '20.0006463242', '29.0692439863', '22.7097016091', '58.9404393782', '25.7666753731', '49.1620319871', '33.9956304868', '24.4766550941', '52.3184580805', '30.5095964967', '36.2843302577', '30.1712350678', '41.0171194552', '20.0513661569', '45.5986531395', '35.8870443261', '37.9469113386', '29.3906663471', '64.7439062131', '35.0125088725', '52.2156089683', '22.4304264392', '49.1816335142', '25.0323193916', '45.8722118604', '34.7604217536', '39.9868794518', '34.6033448222', '41.6839918687', '40.8443061122', '44.7063598652', '21.1562997658', '41.547077748', '24.6607061069', '40.1006629307', '50.3450808487', '51.4318879891', '32.3875251705', '53.4813935145', '24.5552275389', '43.8089558984', '43.4441935484', '33.4178523235', '44.3454176281', '65.5244131168', '39.7529703822', '46.2175087515', '32.7325389894', '49.786953612', '44.9972396166', '168.245861698', '49.313768542', '56.569739846', '47.8089690393', '30.9038417358', '37.2649829658', '35.850617415', '68.1836227448', '34.2461556133', '46.0017090264', '26.9316699155', '48.3078228694', '36.216199621', '37.1569323092', '33.5548344371', '33.223011236', '20.8197112367', '15.8903673469', '35.4949411765', '26.0287842988', '37.0314331551', '29.1820981087', '72.4642572637', '52.1879774011', '40.9748793849', '23.8230645161', '24.1203833866', '26.6531361502', '65.067257829', '51.4916463712', '36.8113087607', '35.087834525', '17.9033416459', '38.2349306626', '41.9222384937', '50.8491202346', '36.282033195', '39.7768253968', '16.4770435274', '35.4246202532', '48.8443718593', '20.2525251678', '24.2994315245', '34.2229535865', '25.2890353698', '26.0607419899', '33.6023140496', '37.4071121352', '20.6980424984', '22.8882189239', '27.5077292965', '21.8116586538', '25.4285205993', '19.5683173077', '26.9353354257', '235.730677515', '268.869600245', '56.4333884407', '26.4409174312', '45.2938472174', '15.5945516903', '27.2886517761', '367.225653291', '42.0311992005', '45.8914772727', '255.153235294', '27.5122713672', '26.1030967742', '23.7416031028', '218.008349974', '21.5487817259', '46.7007087087', '233.533188694', '11.9961176992', '38.8099733155', '105.035207006', '225.055138499']}","tags":"python","loc":"http://www.instantinate.com/python/dictionary_comprehension_for_list_of_lists.html","title":"Dictionary Comprehension for List of Lists"},{"url":"http://www.instantinate.com/how_to/load_a_dataset_from_a_url.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. We'll bring in data with urllib and then load it into our dataset as a csv. import urllib import pandas as pd 1. Create a variable called \"data_url\" and paste in the URL as a string data_url = \"http://instantinate.com/data/sales_data.html\" 2. Call the urllib function with the urlretrieve method The first argument we pass is the variable with the link to the dataset. And the second argument is the name of the dataset. urllib . urlretrieve ( data_url , \"sales.data\" ) 3. Create the \"data\" variable and use the pandas.read_csv method Pass the 'sales.data' argument to the read_csv method. Specify the delimiter for the dataset which in this case is spaces. data = pd . read_csv ( 'sales.data' , delimiter = '\\s+' ) 4. Create a dataframe using the the .dataframe method Pass the data variable which read all the information from the url. df = pd . DataFrame ( data ) 5. Print out the .head( ) to confirm the dataset was loaded correctly. df . head () Here is the full code. data_url = \"http://instantinate.com/data/sales_data.html\" urllib . urlretrieve ( data_url , \"sales.data\" ) data = pd . read_csv ( 'sales.data' , delimiter = '\\s+' ) df = pd . DataFrame ( data ) df . head () volume_sold 2015_margin 2015_q1_sales 2016_q1_sales 0 18.420760 93.802281 337166.53 337804.05 1 4.776510 21.082425 22351.86 21736.63 2 16.602401 93.612494 277764.46 306942.27 3 4.296111 16.824704 16805.11 9307.75 4 8.156023 35.011457 54411.42 58939.90","tags":"how_to","loc":"http://www.instantinate.com/how_to/load_a_dataset_from_a_url.html","title":"How To: Load A Dataset from A URL"},{"url":"http://www.instantinate.com/python/load_dataset_from_url.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. We'll bring in data with urllib and then load it into our dataset as a csv. import urllib import pandas as pd 1. Create a variable called \"data_url\" and paste in the URL as a string data_url = \"http://instantinate.com/data/sales_data.html\" 2. Call the urllib function with the urlretrieve method The first argument we pass is the variable with the link to the dataset. And the second argument is the name of the dataset. urllib . urlretrieve ( data_url , \"sales.data\" ) 3. Create the \"data\" variable and use the pandas.read_csv method Pass the 'sales.data' argument to the read_csv method. Specify the delimiter for the dataset which in this case is spaces. data = pd . read_csv ( 'sales.data' , delimiter = '\\s+' ) 4. Create a dataframe using the the .dataframe method Pass the data variable which read all the information from the url. df = pd . DataFrame ( data ) 5. Print out the .head( ) to confirm the dataset was loaded correctly. df . head () Here is the full code. data_url = \"http://instantinate.com/data/sales_data.html\" urllib . urlretrieve ( data_url , \"sales.data\" ) data = pd . read_csv ( 'sales.data' , delimiter = '\\s+' ) df = pd . DataFrame ( data ) df . head () volume_sold 2015_margin 2015_q1_sales 2016_q1_sales 0 18.420760 93.802281 337166.53 337804.05 1 4.776510 21.082425 22351.86 21736.63 2 16.602401 93.612494 277764.46 306942.27 3 4.296111 16.824704 16805.11 9307.75 4 8.156023 35.011457 54411.42 58939.90","tags":"python","loc":"http://www.instantinate.com/python/load_dataset_from_url.html","title":"Load Dataset from URL"},{"url":"http://www.instantinate.com/how_to/write_dictionary_comprehensions.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. Turn a list of lists into a dictionary with one line of code... the magical comprehension. Click here for a refresher on how to read in data with the csv.reader import csv with open ( '../data/sales_data.csv' , 'rU' ) as f : data = [] reader = csv . reader ( f ) for row in reader : data . append ( row ) print data [['volume_sold', '2015_margin', '2015_q1_sales', '2016_q1_sales'], ['18.4207604861', '93.8022814583', '337166.53', '337804.05'], ['4.77650991918', '21.0824246877', '22351.86', '21736.63'], ['16.6024006077', '93.6124943024', '277764.46', '306942.27'], ['4.29611149826', '16.8247038328', '16805.11', '9307.75'], ['8.15602328201', '35.0114570034', '54411.42', '58939.9'], ['5.00512242518', '31.8774372328', '255939.81', '332979.03'], ['14.60675', '76.5189730216', '319020.69', '302592.88'], ['4.45646649485', '19.3373453608', '45340.33', '55315.23'], ['5.04752965097', '26.142470349', '57849.23', '42398.57'], ['5.38807023767', '22.4270237673', '51031.04', '56241.57'], ['9.34734863474', '41.892132964', '68657.91', '3536.14'], ['10.9303977273', '66.4030492424', '4151.93', '137416.93'], ['6.27020860495', '47.8693242069', '121837.56', '158476.55'], ['12.3959191176', '86.7601504011', '146725.31', '125731.51'], ['4.55771189614', '22.9481762576', '119287.76', '21834.49'], ['4.20012242627', '18.7060545353', '20335.03', '39609.55'], ['10.2528698945', '44.0411766297', '110552.94', '204038.87'], ['12.0767847594', '62.1990040107', '204237.78', '15689.8'], ['3.7250952381', '14.2518095238', '16939.15', '48545.69'], ['3.21072662722', '16.0432686391', '55498.12', '16320.74'], ['6.29097142857', '25.1911714286', '15983.98', '53182.55'], ['7.43482131661', '31.7530658307', '71758.66', '30402.43'], ['4.37622478386', '23.1614514016', '62988.17', '47217.4'], ['12.9889127838', '48.8207407407', '29637.75', '6367.76'], ['11.6974557522', '73.2315044248', '48759.71', '329252.09'], ['5.96517512509', '23.4503335716', '89736.7', '332976.05'], ['3.94522273425', '14.1447926267', '5577.61', '234926.02'], ['7.36958530901', '36.4085284899', '310035.66', '151934.45'], ['7.34350882699', '36.1718619066', '310718.21', '314068.92'], ['12.3500273544', '59.8934779211', '258284.84', '61847.52'], ['8.41791967737', '37.1085548647', '150049.71', '203644.27'], ['10.2608361718', '52.4055916932', '309568.16', '123632.78'], ['7.82435369972', '30.681099171', '66468.32', '1050711.75'], ['10.3314300532', '48.1333683392', '321983.24', '149791.31'], ['12.5284878049', '47.7406803594', '115531.13', '61560.7'], ['18.7447505256', '97.2243398739', '926706.76', '260966.41'], ['6.65773264189', '31.2923926822', '157981.2', '160278.07'], ['10.6321289355', '35.27017991', '51078.36', '78108.56'], ['6.92770422965', '31.9091555963', '272703.74', '253886.88'], ['6.61817422161', '29.1482532051', '180760.93', '173395.13'], ['7.12444444444', '32.6235916667', '72185.66', '91524.76'], ['9.84966032435', '47.9893704508', '263161.4', '247802.36'], ['11.5058377559', '55.5221865049', '164809.62', '115591.86'], ['6.30981315215', '31.941637952', '60986.22', '233350.35'], ['10.1866219839', '49.3420628537', '210690.45', '84940.69'], ['10.1221793301', '42.8693852124', '139068.33', '115993.15'], ['10.8003469032', '53.1849073341', '209403.19', '125207.35'], ['7.26782845188', '25.4050062762', '75110.03', '161300.14'], ['10.6737166742', '43.9390962494', '123291.57', '164939.85'], ['9.15026865672', '44.5348318408', '157591.18', '147632.84'], ['8.12418187744', '39.530065189', '163684.93', '168603.49'], ['6.27579970306', '31.5106033203', '146850.63', '342772.02'], ['10.6772953319', '50.1331972789', '143950.01', '102829.3'], ['5.88898828541', '28.7115801384', '136167.32', '216415.08'], ['10.6401714545', '52.4235630748', '327172.32', '174492.82'], ['4.75559643255', '24.0028010033', '105869.94', '41213.34'], ['10.246884068', '47.3184344342', '176266.68', '91213.98'], ['10.29268081', '49.1944300868', '176720.07', '256707.1'], ['4.41819548872', '19.9170827068', '44836.7', '75866.02'], ['7.10134734573', '32.71362436', '65830.25', '50434.54'], ['8.00611901938', '36.2773863187', '228680.53', '141712.31'], ['7.79050337838', '35.2223614865', '73810.68', '91203.94'], ['11.1293822598', '40.0093030623', '54655.98', '156730.02'], ['9.34847653987', '45.5890982815', '156056.28', '24206.67'], ['6.31088643791', '28.2592197712', '80923.05', '218335.21'], ['11.6256060606', '55.1925378788', '167606.75', '325127.17'], ['6.65440717629', '29.106349454', '25575.88', '94229.41'], ['7.93041476808', '39.1116473887', '211902.95', '117784.92'], ['9.003562316', '46.7565544652', '312502.67', '166847.54'], ['14.4394353772', '57.2886237318', '85217.68', '73117.79'], ['11.0115404852', '46.2992604502', '89525.66', '100545.74'], ['5.72389564186', '25.5950192707', '176252.03', '68966.63'], ['7.77732012195', '36.7157591463', '68258.49', '96059.65'], ['4.75918372602', '20.0006463242', '88008.31', '211600.93'], ['7.78586691659', '29.0692439863', '67219.54', '234774.87'], ['5.03499140759', '22.7097016091', '100722.77', '268226.7'], ['11.6845098446', '58.9404393782', '147242.83', '58122.85'], ['5.14772910448', '25.7666753731', '227702.34', '106662.37'], ['10.0860303763', '49.1620319871', '286763.37', '147781.41'], ['7.94465682362', '33.9956304868', '59531.82', '67514.95'], ['5.29439978635', '24.4766550941', '122811.86', '170466.11'], ['11.8265363003', '52.3184580805', '156513.07', '94158.15'], ['6.300925868', '30.5095964967', '55538.11', '139676.38'], ['8.64269487751', '36.2843302577', '143498.37', '167692.5'], ['6.04822838631', '30.1712350678', '84998.58', '180371.58'], ['9.47492913676', '41.0171194552', '138010.02', '298889.32'], ['3.99185767098', '20.0513661569', '146424.29', '133868.58'], ['8.59207381371', '45.5986531395', '175475.49', '39138.81'], ['7.21148957299', '35.8870443261', '247716.78', '343447.68'], ['8.19108557079', '37.9469113386', '130512.99', '191091.14'], ['7.69531160115', '29.3906663471', '52054.1', '184729.87'], ['13.6351500118', '64.7439062131', '332692.67', '37179.58'], ['6.96681681682', '35.0125088725', '135418.28', '40160.26'], ['11.2323237697', '52.2156089683', '163104.8', '40970.96'], ['5.09546375267', '22.4304264392', '45612.45', '135871.39'], ['11.9368836649', '49.1816335142', '56995.82', '92843.04'], ['5.90627376426', '25.0323193916', '39867.38', '295483.47'], ['9.12709469154', '45.8722118604', '133081.37', '130377.49'], ['7.7544572697', '34.7604217536', '108362.38', '192747.83'], ['7.58599675412', '39.9868794518', '271021.94', '103958.63'], ['7.43725207583', '34.6033448222', '134589.33', '106452.13'], ['9.8798900768', '41.6839918687', '216171.01', '12380.85'], ['9.30319388778', '40.8443061122', '104354.49', '94010.77'], ['9.21365050557', '44.7063598652', '105046.24', '51773.27'], ['5.18177205308', '21.1562997658', '13677.13', '242057.36'], ['8.55507774799', '41.547077748', '89053.08', '392226.28'], ['5.78126590331', '24.6607061069', '51427.05', '369424.59'], ['8.0710230118', '40.1006629307', '264408.55', '73141.35'], ['10.1250032713', '50.3450808487', '294990.21', '292690.86'], ['11.0196516733', '51.4318879891', '358098.75', '172189.31'], ['8.17666774927', '32.3875251705', '68077.3', '211032.4'], ['9.42171292239', '53.4813935145', '313345.86', '171119.44'], ['4.85870921867', '24.5552275389', '163324.44', '201433.01'], ['9.31378525791', '43.8089558984', '203099.8', '200599.95'], ['8.30018036767', '43.4441935484', '104044.33', '476439.16'], ['6.50688776965', '33.4178523235', '202835.96', '215106.77'], ['9.5852099677', '44.3454176281', '192934.41', '204808.54'], ['12.45572275', '65.5244131168', '447305.5', '37858.67'], ['8.09288312646', '39.7529703822', '205170.43', '382790.84'], ['8.68651837806', '46.2175087515', '201282.29', '58332.41'], ['8.34731752963', '32.7325389894', '36855.39', '2443253.18'], ['9.29224055739', '49.786953612', '381036.91', '209066.17'], ['9.77711182109', '44.9972396166', '46509.65', '137697.56'], ['29.878030039', '168.245861698', '2337324.42', '129489.89'], ['8.78393692369', '49.313768542', '208389.88', '72810.95'], ['11.9757685161', '56.569739846', '145742.16', '158368.58'], ['11.1401021385', '47.8089690393', '187407.93', '153305.98'], ['7.5605488194', '30.9038417358', '62335.59', '289199.43'], ['7.39098798637', '37.2649829658', '138878.63', '241311.79'], ['6.43360588592', '35.850617415', '169131.81', '343060.63'], ['13.7999774485', '68.1836227448', '280506.28', '128500.31'], ['6.44703955254', '34.2461556133', '223641.14', '79554.12'], ['8.01794477751', '46.0017090264', '361865.13', '191155.87'], ['6.2553554256', '26.9316699155', '134325.62', '111207.97'], ['9.69742181905', '48.3078228694', '145361.36', '83399.61'], ['7.77268351232', '36.216199621', '185580.5', '25290.33'], ['8.75192030725', '37.1569323092', '93901.53', '20962.08'], ['6.79288937945', '33.5548344371', '68438.51', '15307.3'], ['7.68249438202', '33.223011236', '32698.19', '10464.02'], ['4.38545511613', '20.8197112367', '22829.66', '52246.46'], ['3.60671020408', '15.8903673469', '13141.2', '75566.37'], ['8.45364705882', '35.4949411765', '8738.45', '103585.1'], ['5.21488185976', '26.0287842988', '48441.46', '276396.86'], ['8.40056149733', '37.0314331551', '87115.74', '305851.47'], ['6.84136327817', '29.1820981087', '102794.01', '260453.94'], ['12.5099672623', '72.4642572637', '306947.29', '30441.76'], ['9.0148700565', '52.1879774011', '225615.38', '10564.6'], ['7.20036424796', '40.9748793849', '327334.28', '19357.49'], ['5.77809677419', '23.8230645161', '27114.29', '613414.14'], ['4.94129392971', '24.1203833866', '7539.14', '63793.71'], ['6.00045070423', '26.6531361502', '20378.73', '78706.47'], ['11.9971174753', '65.067257829', '501953.12', '22675.76'], ['10.6377691184', '51.4916463712', '66097.53', '4566.74'], ['8.56422809829', '36.8113087607', '89327.88', '14050.14'], ['8.62268641471', '35.087834525', '20534.72', '11883.3'], ['4.83114713217', '17.9033416459', '5742.23', '11597.18'], ['10.2701848998', '38.2349306626', '15987.52', '23801.1'], ['12.5816945607', '41.9222384937', '13518.07', '22824.14'], ['16.0599706745', '50.8491202346', '17574.68', '18957.2'], ['11.8677385892', '36.282033195', '24357.22', '19832.94'], ['10.2945011338', '39.7768253968', '29611.32', '4258.14'], ['4.17606557377', '16.4770435274', '18571.65', '17116.11'], ['9.36189873418', '35.4246202532', '18712.28', '7499.47'], ['11.0917085427', '48.8443718593', '8458.2', '11367.4'], ['5.3244966443', '20.2525251678', '19089.74', '8041.09'], ['6.63090439276', '24.2994315245', '7305.46', '11966.94'], ['8.58392405063', '34.2229535865', '14796.7', '12707.72'], ['5.53106109325', '25.2890353698', '8200.55', '22130.31'], ['6.13912310287', '26.0607419899', '11412.54', '20817.32'], ['8.47737603306', '33.6023140496', '11228.97', '16786.98'], ['8.44393241167', '37.4071121352', '17232.45', '19203.82'], ['5.15196394076', '20.6980424984', '21340.78', '25302.92'], ['6.53706864564', '22.8882189239', '16617.85', '18650.96'], ['8.50044523598', '27.5077292965', '21711.71', '14626.29'], ['3.93154326923', '21.8116586538', '28128.95', '46323.73'], ['6.16368913858', '25.4285205993', '21803.2', '535381.86'], ['4.90444711538', '19.5683173077', '12517.29', '812011.78'], ['7.40241271087', '26.9353354257', '53988.92', '206247.57'], ['47.5032692308', '235.730677515', '555707.4', '6402.78'], ['55.7391800938', '268.869600245', '1082136.01', '100765.67'], ['11.8407803201', '56.4333884407', '192089.46', '20098.61'], ['7.00229357798', '26.4409174312', '5574.99', '68230.36'], ['8.75314206706', '45.2938472174', '80241.27', '596063.0'], ['3.14774130328', '15.5945516903', '27043.54', '87471.43'], ['7.1967787944', '27.2886517761', '61977.54', '431990.7'], ['76.2036918138', '367.225653291', '977772.62', '136717.57'], ['10.8043371086', '42.0311992005', '41905.18', '30008.81'], ['10.705327051', '45.8914772727', '87839.45', '19420.34'], ['51.8006862745', '255.153235294', '445058.32', '274752.6'], ['5.88277871731', '27.5122713672', '127495.18', '10315.35'], ['6.68640645161', '26.1030967742', '23874.67', '45252.42'], ['5.83335488041', '23.7416031028', '21535.87', '433832.28'], ['45.5560956385', '218.008349974', '276096.18', '74215.43'], ['5.17260575296', '21.5487817259', '8506.79', '78137.29'], ['10.118018018', '46.7007087087', '49163.01', '83915.72'], ['51.6755374204', '233.533188694', '434110.57', '372240.72'], ['2.79463149728', '11.9961176992', '73789.38', '148021.9'], ['7.61169779853', '38.8099733155', '88006.84', '31184.18'], ['15.6976512739', '105.035207006', '117958.96', '33324.36'], ['50.2758932156', '225.055138499', '407738.79', '32079.13']] Every dictionary needs some keys. Below we're \"slicing\" the data from the top row of the dataset (which is our headers) and assigning it to the \"header\" variable. Can you see where this is going? header = data [ 0 ][:] Next we're going back to the list of lists to remove that first row so we don't have any conflicts. At the end we'll end up with two lists one that can be used as \"Keys\" (the header row), and one that can be used as \"Values\" (everything else). data . pop ( 0 ) header = data [ 0 ][:] data . pop ( 0 ) ['volume_sold', '2015_margin', '2015_q1_sales', '2016_q1_sales'] Now for that scary comprehension thingy. So we've got two lists, one that has only 4 items and one that has lots and lots of items and each of those items has 4 items embedded(list of lists). Conceptually, we want to take the 1st entry from each of those embedded lists and map it to the first key value. Then we want to get the second and map it to the second value. etc. etc. Here's a refresher on how dictionary comprehensions are laid out. { key_extractor ( obj ): value_extractor ( obj ) for i , obj in enumerate ( objects )} Values to be returned. Remember that comprehensions are shorthand for other types of functions. So it starts with the values that are going to be returned, in this case we're going to extract a key, and a value for each list in the big list. { column_name : [ row [ i ] for row in data ] Values to iterate through. Then we define the variable \"i\" to iterate through each list. Since we are going through two things at a time (key and value) we need to specify the second item to pull when we iterrate through which will be the \"column name\". That will act as our \"key\" in the dictionary. { column_name : [ row [ i ] for row in data ] for i , column_name Enumerate is the key. Next we need to get two values to iterate through. The \"enumerate\" function is the magical key for this. It will return both the index number and the value in a list that can then be passed to the variables to iterate through.... phew that was a mouthful... so if that was difficult to follow unpack each statement and trace it to the full function until you see what is happening. { column_name : [ row [ i ] for row in data ] for i , column_name in enumerate ( header )} Did you notice that other thing in here? Yeah I did see that mean thing in there next to the first \"column name\". Because we're iterating over a list of lists we need to define all of the items to be returned as values with that key. So we put a second simple List Comprehension to go through all the list of lists and return the value that is at each index point that matches the \"i\" variable. [ row [ i ] for row in data ] Now to see how it all goes together and returns a dictionary of values called \"Sales_Data\" Notice that each of the \"Column Headers\" (Dictionary Keys), has a list of \"Dictionary Values\" that is the corresponding list item for each position (The first item in the list goes to the first column, the second item goes to the second column etc.). sales_data = { column_name : [ row [ i ] for row in data ] for i , column_name in enumerate ( header )} print sales_data {'volume_sold': ['18.4207604861', '4.77650991918', '16.6024006077', '4.29611149826', '8.15602328201', '5.00512242518', '14.60675', '4.45646649485', '5.04752965097', '5.38807023767', '9.34734863474', '10.9303977273', '6.27020860495', '12.3959191176', '4.55771189614', '4.20012242627', '10.2528698945', '12.0767847594', '3.7250952381', '3.21072662722', '6.29097142857', '7.43482131661', '4.37622478386', '12.9889127838', '11.6974557522', '5.96517512509', '3.94522273425', '7.36958530901', '7.34350882699', '12.3500273544', '8.41791967737', '10.2608361718', '7.82435369972', '10.3314300532', '12.5284878049', '18.7447505256', '6.65773264189', '10.6321289355', '6.92770422965', '6.61817422161', '7.12444444444', '9.84966032435', '11.5058377559', '6.30981315215', '10.1866219839', '10.1221793301', '10.8003469032', '7.26782845188', '10.6737166742', '9.15026865672', '8.12418187744', '6.27579970306', '10.6772953319', '5.88898828541', '10.6401714545', '4.75559643255', '10.246884068', '10.29268081', '4.41819548872', '7.10134734573', '8.00611901938', '7.79050337838', '11.1293822598', '9.34847653987', '6.31088643791', '11.6256060606', '6.65440717629', '7.93041476808', '9.003562316', '14.4394353772', '11.0115404852', '5.72389564186', '7.77732012195', '4.75918372602', '7.78586691659', '5.03499140759', '11.6845098446', '5.14772910448', '10.0860303763', '7.94465682362', '5.29439978635', '11.8265363003', '6.300925868', '8.64269487751', '6.04822838631', '9.47492913676', '3.99185767098', '8.59207381371', '7.21148957299', '8.19108557079', '7.69531160115', '13.6351500118', '6.96681681682', '11.2323237697', '5.09546375267', '11.9368836649', '5.90627376426', '9.12709469154', '7.7544572697', '7.58599675412', '7.43725207583', '9.8798900768', '9.30319388778', '9.21365050557', '5.18177205308', '8.55507774799', '5.78126590331', '8.0710230118', '10.1250032713', '11.0196516733', '8.17666774927', '9.42171292239', '4.85870921867', '9.31378525791', '8.30018036767', '6.50688776965', '9.5852099677', '12.45572275', '8.09288312646', '8.68651837806', '8.34731752963', '9.29224055739', '9.77711182109', '29.878030039', '8.78393692369', '11.9757685161', '11.1401021385', '7.5605488194', '7.39098798637', '6.43360588592', '13.7999774485', '6.44703955254', '8.01794477751', '6.2553554256', '9.69742181905', '7.77268351232', '8.75192030725', '6.79288937945', '7.68249438202', '4.38545511613', '3.60671020408', '8.45364705882', '5.21488185976', '8.40056149733', '6.84136327817', '12.5099672623', '9.0148700565', '7.20036424796', '5.77809677419', '4.94129392971', '6.00045070423', '11.9971174753', '10.6377691184', '8.56422809829', '8.62268641471', '4.83114713217', '10.2701848998', '12.5816945607', '16.0599706745', '11.8677385892', '10.2945011338', '4.17606557377', '9.36189873418', '11.0917085427', '5.3244966443', '6.63090439276', '8.58392405063', '5.53106109325', '6.13912310287', '8.47737603306', '8.44393241167', '5.15196394076', '6.53706864564', '8.50044523598', '3.93154326923', '6.16368913858', '4.90444711538', '7.40241271087', '47.5032692308', '55.7391800938', '11.8407803201', '7.00229357798', '8.75314206706', '3.14774130328', '7.1967787944', '76.2036918138', '10.8043371086', '10.705327051', '51.8006862745', '5.88277871731', '6.68640645161', '5.83335488041', '45.5560956385', '5.17260575296', '10.118018018', '51.6755374204', '2.79463149728', '7.61169779853', '15.6976512739', '50.2758932156'], '2015_q1_sales': ['337166.53', '22351.86', '277764.46', '16805.11', '54411.42', '255939.81', '319020.69', '45340.33', '57849.23', '51031.04', '68657.91', '4151.93', '121837.56', '146725.31', '119287.76', '20335.03', '110552.94', '204237.78', '16939.15', '55498.12', '15983.98', '71758.66', '62988.17', '29637.75', '48759.71', '89736.7', '5577.61', '310035.66', '310718.21', '258284.84', '150049.71', '309568.16', '66468.32', '321983.24', '115531.13', '926706.76', '157981.2', '51078.36', '272703.74', '180760.93', '72185.66', '263161.4', '164809.62', '60986.22', '210690.45', '139068.33', '209403.19', '75110.03', '123291.57', '157591.18', '163684.93', '146850.63', '143950.01', '136167.32', '327172.32', '105869.94', '176266.68', '176720.07', '44836.7', '65830.25', '228680.53', '73810.68', '54655.98', '156056.28', '80923.05', '167606.75', '25575.88', '211902.95', '312502.67', '85217.68', '89525.66', '176252.03', '68258.49', '88008.31', '67219.54', '100722.77', '147242.83', '227702.34', '286763.37', '59531.82', '122811.86', '156513.07', '55538.11', '143498.37', '84998.58', '138010.02', '146424.29', '175475.49', '247716.78', '130512.99', '52054.1', '332692.67', '135418.28', '163104.8', '45612.45', '56995.82', '39867.38', '133081.37', '108362.38', '271021.94', '134589.33', '216171.01', '104354.49', '105046.24', '13677.13', '89053.08', '51427.05', '264408.55', '294990.21', '358098.75', '68077.3', '313345.86', '163324.44', '203099.8', '104044.33', '202835.96', '192934.41', '447305.5', '205170.43', '201282.29', '36855.39', '381036.91', '46509.65', '2337324.42', '208389.88', '145742.16', '187407.93', '62335.59', '138878.63', '169131.81', '280506.28', '223641.14', '361865.13', '134325.62', '145361.36', '185580.5', '93901.53', '68438.51', '32698.19', '22829.66', '13141.2', '8738.45', '48441.46', '87115.74', '102794.01', '306947.29', '225615.38', '327334.28', '27114.29', '7539.14', '20378.73', '501953.12', '66097.53', '89327.88', '20534.72', '5742.23', '15987.52', '13518.07', '17574.68', '24357.22', '29611.32', '18571.65', '18712.28', '8458.2', '19089.74', '7305.46', '14796.7', '8200.55', '11412.54', '11228.97', '17232.45', '21340.78', '16617.85', '21711.71', '28128.95', '21803.2', '12517.29', '53988.92', '555707.4', '1082136.01', '192089.46', '5574.99', '80241.27', '27043.54', '61977.54', '977772.62', '41905.18', '87839.45', '445058.32', '127495.18', '23874.67', '21535.87', '276096.18', '8506.79', '49163.01', '434110.57', '73789.38', '88006.84', '117958.96', '407738.79'], '2016_q1_sales': ['337804.05', '21736.63', '306942.27', '9307.75', '58939.9', '332979.03', '302592.88', '55315.23', '42398.57', '56241.57', '3536.14', '137416.93', '158476.55', '125731.51', '21834.49', '39609.55', '204038.87', '15689.8', '48545.69', '16320.74', '53182.55', '30402.43', '47217.4', '6367.76', '329252.09', '332976.05', '234926.02', '151934.45', '314068.92', '61847.52', '203644.27', '123632.78', '1050711.75', '149791.31', '61560.7', '260966.41', '160278.07', '78108.56', '253886.88', '173395.13', '91524.76', '247802.36', '115591.86', '233350.35', '84940.69', '115993.15', '125207.35', '161300.14', '164939.85', '147632.84', '168603.49', '342772.02', '102829.3', '216415.08', '174492.82', '41213.34', '91213.98', '256707.1', '75866.02', '50434.54', '141712.31', '91203.94', '156730.02', '24206.67', '218335.21', '325127.17', '94229.41', '117784.92', '166847.54', '73117.79', '100545.74', '68966.63', '96059.65', '211600.93', '234774.87', '268226.7', '58122.85', '106662.37', '147781.41', '67514.95', '170466.11', '94158.15', '139676.38', '167692.5', '180371.58', '298889.32', '133868.58', '39138.81', '343447.68', '191091.14', '184729.87', '37179.58', '40160.26', '40970.96', '135871.39', '92843.04', '295483.47', '130377.49', '192747.83', '103958.63', '106452.13', '12380.85', '94010.77', '51773.27', '242057.36', '392226.28', '369424.59', '73141.35', '292690.86', '172189.31', '211032.4', '171119.44', '201433.01', '200599.95', '476439.16', '215106.77', '204808.54', '37858.67', '382790.84', '58332.41', '2443253.18', '209066.17', '137697.56', '129489.89', '72810.95', '158368.58', '153305.98', '289199.43', '241311.79', '343060.63', '128500.31', '79554.12', '191155.87', '111207.97', '83399.61', '25290.33', '20962.08', '15307.3', '10464.02', '52246.46', '75566.37', '103585.1', '276396.86', '305851.47', '260453.94', '30441.76', '10564.6', '19357.49', '613414.14', '63793.71', '78706.47', '22675.76', '4566.74', '14050.14', '11883.3', '11597.18', '23801.1', '22824.14', '18957.2', '19832.94', '4258.14', '17116.11', '7499.47', '11367.4', '8041.09', '11966.94', '12707.72', '22130.31', '20817.32', '16786.98', '19203.82', '25302.92', '18650.96', '14626.29', '46323.73', '535381.86', '812011.78', '206247.57', '6402.78', '100765.67', '20098.61', '68230.36', '596063.0', '87471.43', '431990.7', '136717.57', '30008.81', '19420.34', '274752.6', '10315.35', '45252.42', '433832.28', '74215.43', '78137.29', '83915.72', '372240.72', '148021.9', '31184.18', '33324.36', '32079.13'], '2015_margin': ['93.8022814583', '21.0824246877', '93.6124943024', '16.8247038328', '35.0114570034', '31.8774372328', '76.5189730216', '19.3373453608', '26.142470349', '22.4270237673', '41.892132964', '66.4030492424', '47.8693242069', '86.7601504011', '22.9481762576', '18.7060545353', '44.0411766297', '62.1990040107', '14.2518095238', '16.0432686391', '25.1911714286', '31.7530658307', '23.1614514016', '48.8207407407', '73.2315044248', '23.4503335716', '14.1447926267', '36.4085284899', '36.1718619066', '59.8934779211', '37.1085548647', '52.4055916932', '30.681099171', '48.1333683392', '47.7406803594', '97.2243398739', '31.2923926822', '35.27017991', '31.9091555963', '29.1482532051', '32.6235916667', '47.9893704508', '55.5221865049', '31.941637952', '49.3420628537', '42.8693852124', '53.1849073341', '25.4050062762', '43.9390962494', '44.5348318408', '39.530065189', '31.5106033203', '50.1331972789', '28.7115801384', '52.4235630748', '24.0028010033', '47.3184344342', '49.1944300868', '19.9170827068', '32.71362436', '36.2773863187', '35.2223614865', '40.0093030623', '45.5890982815', '28.2592197712', '55.1925378788', '29.106349454', '39.1116473887', '46.7565544652', '57.2886237318', '46.2992604502', '25.5950192707', '36.7157591463', '20.0006463242', '29.0692439863', '22.7097016091', '58.9404393782', '25.7666753731', '49.1620319871', '33.9956304868', '24.4766550941', '52.3184580805', '30.5095964967', '36.2843302577', '30.1712350678', '41.0171194552', '20.0513661569', '45.5986531395', '35.8870443261', '37.9469113386', '29.3906663471', '64.7439062131', '35.0125088725', '52.2156089683', '22.4304264392', '49.1816335142', '25.0323193916', '45.8722118604', '34.7604217536', '39.9868794518', '34.6033448222', '41.6839918687', '40.8443061122', '44.7063598652', '21.1562997658', '41.547077748', '24.6607061069', '40.1006629307', '50.3450808487', '51.4318879891', '32.3875251705', '53.4813935145', '24.5552275389', '43.8089558984', '43.4441935484', '33.4178523235', '44.3454176281', '65.5244131168', '39.7529703822', '46.2175087515', '32.7325389894', '49.786953612', '44.9972396166', '168.245861698', '49.313768542', '56.569739846', '47.8089690393', '30.9038417358', '37.2649829658', '35.850617415', '68.1836227448', '34.2461556133', '46.0017090264', '26.9316699155', '48.3078228694', '36.216199621', '37.1569323092', '33.5548344371', '33.223011236', '20.8197112367', '15.8903673469', '35.4949411765', '26.0287842988', '37.0314331551', '29.1820981087', '72.4642572637', '52.1879774011', '40.9748793849', '23.8230645161', '24.1203833866', '26.6531361502', '65.067257829', '51.4916463712', '36.8113087607', '35.087834525', '17.9033416459', '38.2349306626', '41.9222384937', '50.8491202346', '36.282033195', '39.7768253968', '16.4770435274', '35.4246202532', '48.8443718593', '20.2525251678', '24.2994315245', '34.2229535865', '25.2890353698', '26.0607419899', '33.6023140496', '37.4071121352', '20.6980424984', '22.8882189239', '27.5077292965', '21.8116586538', '25.4285205993', '19.5683173077', '26.9353354257', '235.730677515', '268.869600245', '56.4333884407', '26.4409174312', '45.2938472174', '15.5945516903', '27.2886517761', '367.225653291', '42.0311992005', '45.8914772727', '255.153235294', '27.5122713672', '26.1030967742', '23.7416031028', '218.008349974', '21.5487817259', '46.7007087087', '233.533188694', '11.9961176992', '38.8099733155', '105.035207006', '225.055138499']}","tags":"how_to","loc":"http://www.instantinate.com/how_to/write_dictionary_comprehensions.html","title":"How To: Write Dictionary Comprehensions"},{"url":"http://www.instantinate.com/python/a_basic_python_function.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. What do dictionaries and baking cakes have in common? Read below to find out. First we'll load in some data below. This is a list of dictionaries that contains information about different movies. #Loading in a movie ratings data set. movies = [ { \"name\" : \"Usual Suspects\" , \"imdb\" : 7.0 , \"category\" : \"Thriller\" }, { \"name\" : \"Hitman\" , \"imdb\" : 6.3 , \"category\" : \"Action\" }, { \"name\" : \"Dark Knight\" , \"imdb\" : 9.0 , \"category\" : \"Adventure\" }, { \"name\" : \"The Help\" , \"imdb\" : 8.0 , \"category\" : \"Drama\" }, { \"name\" : \"The Choice\" , \"imdb\" : 6.2 , \"category\" : \"Romance\" }, { \"name\" : \"Colonia\" , \"imdb\" : 7.4 , \"category\" : \"Romance\" }, { \"name\" : \"Love\" , \"imdb\" : 6.0 , \"category\" : \"Romance\" }, { \"name\" : \"Bride Wars\" , \"imdb\" : 5.4 , \"category\" : \"Romance\" }, { \"name\" : \"AlphaJet\" , \"imdb\" : 3.2 , \"category\" : \"War\" }, { \"name\" : \"Ringing Crime\" , \"imdb\" : 4.0 , \"category\" : \"Crime\" }, { \"name\" : \"Joking muck\" , \"imdb\" : 7.2 , \"category\" : \"Comedy\" }, { \"name\" : \"What is the name\" , \"imdb\" : 9.2 , \"category\" : \"Suspense\" }, { \"name\" : \"Detective\" , \"imdb\" : 7.0 , \"category\" : \"Suspense\" }, { \"name\" : \"Exam\" , \"imdb\" : 4.2 , \"category\" : \"Thriller\" }, { \"name\" : \"We Two\" , \"imdb\" : 7.2 , \"category\" : \"Romance\" } ] It starts with a def Every function needs a definition. Think of it like a normal dictionary. If you were to go look up a certain word you would need to know the first few letters to find it. But there also would only be one possible definition for that word in the dictionary. Python functions are the same in this regard, they need a unique name so that you can find it later. We're going to definie our function below: def genre_avg ( cat ): Wait a minute?! What's that \"cat\" thing in there?!?! (Your ingredients) That is a scary thing called a \"parameter\". It is basically the ingredients to your recipe. If you want to bake a cake you put ingredients into it and out comes a delicious cake. For parameters you \"pass\" these ingredients and then tell the function what to do with them and hopefully [crossesfingers] we get cake. Create an empty list. (Get out a mixing bowl) The genre_avg list will hold all the results of our function. def genre_avg ( cat ): genre_avg = [] For Loop or not to For Loop (Measure the ingredients) We have the ingredients and the mixing bowl... so now we need to measure the ingredients. The for loop in this function goes through each statement in the dictionary above and measures it against the \"parameter\" we passed it. In this case we're interested only in movies that fall in the Action genre. So we measure out all the action movies from our dataset and then what? Can you see what happens next? def genre_avg ( cat ): genre_avg = [] for x in movies : if cat == x [ \"category\" ]: Append (Mix the ingredients) This takes all the ingredients (parameters) that you just measured and mixes them together. And where do you mix things? In the mixing bowl of course! So now all our ingredients have been measured and are back in our mixing bowl all neat and unbaked... hmm... I wonder what the next step is. def genre_avg ( cat ): genre_avg = [] for x in movies : if cat == x [ \"category\" ]: genre_avg . append ( x [ 'imdb' ]) Return (Bake it!!!!) Now we bake all those nice ingredients to get the most perfect cake we ever could ask for. In this case, a perfect average of score of a particular genre in our movie dataset. def genre_avg ( cat ): genre_avg = [] for x in movies : if cat == x [ \"category\" ]: genre_avg . append ( x [ 'imdb' ]) return sum ( genre_avg ) / len ( genre_avg ) Now for the magic. This is not just any cake baking exercise. This is a magical cake machine that can give you any kind of cake you want so long as you give it the ingredients with the right \"parameter\". That's what is going on in the last statement after the function is defined. We reference that function and give it some ingredients and instantly we have some magic cake! How about that. genre_avg ( \"Action\" ) Here's the whole code. def genre_avg ( cat ): genre_avg = [] for x in movies : if cat == x [ \"category\" ]: genre_avg . append ( x [ 'imdb' ]) return sum ( genre_avg ) / len ( genre_avg ) genre_avg ( \"Action\" ) 6.3","tags":"python","loc":"http://www.instantinate.com/python/a_basic_python_function.html","title":"A Basic Python Function"},{"url":"http://www.instantinate.com/articles/billboard_analysis.html","text":"Here is some open ended analysis using pandas to show the power of this library for handling literally anything that can be thrown at it. import pandas as pd import numpy as np import matplotlib.pyplot as plt % matplotlib inline Pandas to read in data data = pd . read_csv ( '../data/billboard.csv' ) df = pd . DataFrame ( data ) df . head () year artist.inverted track time genre date.entered date.peaked x1st.week x2nd.week x3rd.week ... x67th.week x68th.week x69th.week x70th.week x71st.week x72nd.week x73rd.week x74th.week x75th.week x76th.week 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 2000 Santana Maria, Maria 4:18 Rock 2000-02-12 2000-04-08 15 8.0 6.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2 2000 Savage Garden I Knew I Loved You 4:07 Rock 1999-10-23 2000-01-29 71 48.0 43.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 2000 Madonna Music 3:45 Rock 2000-08-12 2000-09-16 41 23.0 18.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 2000 Aguilera, Christina Come On Over Baby (All I Want Is You) 3:38 Rock 2000-08-05 2000-10-14 57 47.0 45.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 5 rows × 83 columns Renaming column names df . columns = [ col . replace ( '.week' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'x' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'st' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'nd' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'rd' , '' ) for col in df . columns ] df . columns = [ col . replace ( 'th' , '' ) for col in df . columns ] #df.columns = ufo.columns.str.replace(' ', '_') Describe your data: check the value counts + descrisptive stats #We will first drop 'year' since all the songs are from 2000 df . drop ( 'year' , axis = 1 , inplace = True ) #basic describe df . describe () 1 2 3 4 5 6 7 8 9 10 ... 67 68 69 70 71 72 73 74 75 76 count 317.000000 312.000000 307.000000 300.000000 292.000000 280.000000 269.000000 260.000000 253.000000 244.000000 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 mean 79.958991 71.173077 65.045603 59.763333 56.339041 52.360714 49.219331 47.119231 46.343874 45.786885 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN std 14.686865 18.200443 20.752302 22.324619 23.780022 24.473273 25.654279 26.370782 27.136419 28.152357 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN min 15.000000 8.000000 6.000000 5.000000 2.000000 1.000000 1.000000 1.000000 1.000000 1.000000 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 25% 74.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 50% 81.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 75% 91.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN max 100.000000 100.000000 100.000000 100.000000 100.000000 99.000000 100.000000 99.000000 100.000000 100.000000 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 8 rows × 76 columns df2 = pd . melt ( df , id_vars = [ 'arti.inverted' , 'track' , 'time' , 'genre' , 'date.entered' , 'date.peaked' ], var_name = 'week' , value_name = 'rank' ) #checking function that was run print df2 . shape print df2 . columns (24092, 8) Index([u'arti.inverted', u'track', u'time', u'genre', u'date.entered', u'date.peaked', u'week', u'rank'], dtype='object') #We now the data formated to plot ranking over time for each track. #We will use other methods to determine which tracks to plot. df3 = df2 [ df2 [ 'track' ] == 'I Wanna Know' ] plt . plot ( df3 [ 'rank' ]) plt . ylabel ( 'Weekly Rank' ) plt . axis ([ 0 , 18000 , 100 , 0 ]) plt . show () Future Exploratory options... the world is yours with pandas. Look at time it takes to get to the top. (time entered, to time peak) Also add a column called time in top 100. Compare the three columns. That was interesting... now lets wrangle this dataset for some cool correlations. Data wrangling for the most correlated genres #create a new dataframe for manipulation. df7 = df2 Group the data df7 = df7 . groupby ([ 'week' , 'genre' ], as_index = False ) . mean () #check that the group by function worked. df7 . head () week genre rank 0 1 Country 82.405405 1 1 Electronica 84.500000 2 1 Gospel 76.000000 3 1 Jazz 89.000000 4 1 Latin 73.222222 Create a pivot table #Use the pivot table function to get to something that can be correlated. df7 = df7 . pivot ( index = 'week' , columns = 'genre' , values = 'rank' ) #Moving the week column from the index back into a column position on the data table. df7 . reset_index ( inplace = True ) df7 . head ( 5 ) genre week Country Electronica Gospel Jazz Latin Pop R&B Rap Reggae Rock 0 1 82.405405 84.500000 76.0 89.0 73.222222 79.222222 84.086957 85.172414 72.0 76.116788 1 10 52.377049 55.750000 59.0 NaN 43.250000 43.571429 63.866667 53.380952 75.0 35.895238 2 11 51.016949 53.250000 66.0 NaN 49.625000 50.142857 62.538462 52.538462 84.0 36.048077 3 12 50.714286 59.750000 68.0 NaN 35.285714 58.250000 67.000000 50.000000 92.0 33.734694 4 13 52.301887 49.333333 61.0 NaN 39.285714 58.333333 59.666667 53.235294 85.0 34.125000 #Sorting the data inside week after it is converted to a string. df7 [ 'week' ] = df7 . week . astype ( int ) df7 . sort ( 'week' ) df7 . head () /anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:3: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....) app.launch_new_instance() genre week Country Electronica Gospel Jazz Latin Pop R&B Rap Reggae Rock 0 1 82.405405 84.500000 76.0 89.0 73.222222 79.222222 84.086957 85.172414 72.0 76.116788 1 10 52.377049 55.750000 59.0 NaN 43.250000 43.571429 63.866667 53.380952 75.0 35.895238 2 11 51.016949 53.250000 66.0 NaN 49.625000 50.142857 62.538462 52.538462 84.0 36.048077 3 12 50.714286 59.750000 68.0 NaN 35.285714 58.250000 67.000000 50.000000 92.0 33.734694 4 13 52.301887 49.333333 61.0 NaN 39.285714 58.333333 59.666667 53.235294 85.0 34.125000 #Remove the pesky column and index names that will mess up the correlation formula later. df7 . index . name = None df7 . columns . name = None #Remove the week column since we sorted by it already. df7 . drop ( 'week' , inplace = True , axis = 1 ) df7 . head () Country Electronica Gospel Jazz Latin Pop R&B Rap Reggae Rock 0 82.405405 84.500000 76.0 89.0 73.222222 79.222222 84.086957 85.172414 72.0 76.116788 1 52.377049 55.750000 59.0 NaN 43.250000 43.571429 63.866667 53.380952 75.0 35.895238 2 51.016949 53.250000 66.0 NaN 49.625000 50.142857 62.538462 52.538462 84.0 36.048077 3 50.714286 59.750000 68.0 NaN 35.285714 58.250000 67.000000 50.000000 92.0 33.734694 4 52.301887 49.333333 61.0 NaN 39.285714 58.333333 59.666667 53.235294 85.0 34.125000 Data wrangling for the most correlated artist rankings df8 = df2 Group the data df8 = df8 . groupby ([ 'week' , 'arti.inverted' ], as_index = False ) . mean () df8 . shape (17328, 3) df8 = df8 [ np . isfinite ( df8 [ 'rank' ])] df8 . shape (3989, 3) counts = df8 [ 'arti.inverted' ] . value_counts () ##Removing any artists that have less than 15 datapoints on the rankings. df8 = df8 [ df8 [ 'arti.inverted' ] . isin ( counts [ counts > 30 ] . index )] Create a pivot table df8 = df8 . pivot ( index = 'week' , columns = 'arti.inverted' , values = 'rank' ) #Moving the week column from the index back into a column position on the data table. df8 . reset_index ( inplace = True ) #Sorting the data inside week after it is converted to a string. df8 [ 'week' ] = df8 . week . astype ( int ) df8 . sort ( 'week' ) df8 . head () /anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:3: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....) app.launch_new_instance() arti.inverted week 3 Doors Down Aaliyah Anthony, Marc BBMak Braxton, Toni Creed Destiny's Child Hill, Faith Joe Jordan, Montell Lonestar Nelly Pink Savage Garden Vertical Horizon matchbox twenty 0 1 78.5 71.5 79.5 99.0 79.0 82.5 78.333333 82.0 85.5 92.0 82.666667 100.0 55.0 73.0 67.0 60.0 1 10 56.0 23.0 44.5 18.0 21.0 65.5 12.333333 49.0 53.0 24.0 33.333333 36.0 11.5 16.5 22.5 12.0 2 11 56.0 23.0 43.0 19.0 27.0 63.5 10.333333 42.0 52.5 24.0 33.000000 37.0 10.0 19.0 20.0 8.0 3 12 55.0 22.5 43.5 15.0 29.0 68.0 7.666667 46.0 54.5 20.0 31.666667 30.0 11.0 22.5 19.0 6.0 4 13 54.0 22.0 50.0 18.0 32.0 70.5 2.666667 55.5 54.0 19.0 35.000000 23.0 12.0 25.0 19.0 1.0 #Remove the pesky column and index names that will mess up the correlation formula later. df8 . index . name = None df8 . columns . name = None #Remove the week column since we sorted by it already. df8 . drop ( 'week' , inplace = True , axis = 1 ) Data wrangling for the most correlated song rankings How would you do this part? #df9 = df2 Run a correlation function on the dataframes from the wrangling steps. This is a simple formula that I have been working to improve to work on any data set. It is designed to be a useful alternative to the spray and pray sns.pairplot or scatter matrix methods. sns.pairplot (on df7) = 14s corrr_pairs function (on df7) =384ms def corr_pairs ( df_input , coef_percentile ): #,mse_percentile #from sklearn.metrics import mean_squared_error #Get top correlated pairs using Pearson coefficient c = df_input . corr () s = c . unstack () so = s . sort_values ( kind = \"quicksort\" ) df_output = pd . DataFrame ( so . abs (), columns = [ 'coef' ]) df_output = df_output . reset_index () df_output . drop_duplicates ( 'coef' , inplace = True ) df_output . dropna ( inplace = True ) #df_input = df_input.fillna(0.0) #Get mean squared error for better accuracy #mse_l = [] #for i in range(len(df_output.iloc[:,0:2])): #mse_var = mean_squared_error(df_input[df_output.iloc[i,0]], df_input[df_output.iloc[i,1]]) #mse_l.append(mse_var) #df_output['mse'] = mse_l #Filter the results by both Coefficient and MSE for best pairs. df_output = df_output [( df_output [ 'coef' ] < 1 ) & ( df_output . coef > np . percentile ( df_output [ 'coef' ], coef_percentile ))] #& (df_output.mse < np.percentile(df_output['mse'],mse_percentile))] #Plot the best pairs. for i in range ( len ( df_output . iloc [:, 0 : 2 ])): colors = [ 'r' , 'b' ] plt . scatter ( df_output . iloc [ i , 0 ], df_output . iloc [ i , 1 ], data = df_input , c = colors ) plt . xlabel ( df_output . iloc [ i , 0 ]) plt . ylabel ( df_output . iloc [ i , 1 ]) plt . legend () plt . show () return df_output Showing the most correlated genres in the rankings corr_pairs ( df7 , 95 ) level_0 level_1 coef 86 Electronica Country 0.848179 88 Electronica R&B 0.913870 Showing the most correlated artists in the rankings corr_pairs ( df8 , 95 ) level_0 level_1 coef 230 Creed Joe 0.909992 232 Savage Garden Aaliyah 0.949246 234 BBMak matchbox twenty 0.954013 236 matchbox twenty Destiny's Child 0.965130 238 Destiny's Child BBMak 0.982112 Showing the most correlated songs in the rankings How would you do this part?","tags":"Articles","loc":"http://www.instantinate.com/articles/billboard_analysis.html","title":"Billboard Hot 100 Analysis"},{"url":"http://www.instantinate.com/how_to/build_a_basic_python_function.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. What do dictionaries and baking cakes have in common? Read below to find out. First we'll load in some data below. This is a list of dictionaries that contains information about different movies. #Loading in a movie ratings data set. movies = [ { \"name\" : \"Usual Suspects\" , \"imdb\" : 7.0 , \"category\" : \"Thriller\" }, { \"name\" : \"Hitman\" , \"imdb\" : 6.3 , \"category\" : \"Action\" }, { \"name\" : \"Dark Knight\" , \"imdb\" : 9.0 , \"category\" : \"Adventure\" }, { \"name\" : \"The Help\" , \"imdb\" : 8.0 , \"category\" : \"Drama\" }, { \"name\" : \"The Choice\" , \"imdb\" : 6.2 , \"category\" : \"Romance\" }, { \"name\" : \"Colonia\" , \"imdb\" : 7.4 , \"category\" : \"Romance\" }, { \"name\" : \"Love\" , \"imdb\" : 6.0 , \"category\" : \"Romance\" }, { \"name\" : \"Bride Wars\" , \"imdb\" : 5.4 , \"category\" : \"Romance\" }, { \"name\" : \"AlphaJet\" , \"imdb\" : 3.2 , \"category\" : \"War\" }, { \"name\" : \"Ringing Crime\" , \"imdb\" : 4.0 , \"category\" : \"Crime\" }, { \"name\" : \"Joking muck\" , \"imdb\" : 7.2 , \"category\" : \"Comedy\" }, { \"name\" : \"What is the name\" , \"imdb\" : 9.2 , \"category\" : \"Suspense\" }, { \"name\" : \"Detective\" , \"imdb\" : 7.0 , \"category\" : \"Suspense\" }, { \"name\" : \"Exam\" , \"imdb\" : 4.2 , \"category\" : \"Thriller\" }, { \"name\" : \"We Two\" , \"imdb\" : 7.2 , \"category\" : \"Romance\" } ] It starts with a def Every function needs a definition. Think of it like a normal dictionary. If you were to go look up a certain word you would need to know the first few letters to find it. But there also would only be one possible definition for that word in the dictionary. Python functions are the same in this regard, they need a unique name so that you can find it later. We're going to definie our function below: def genre_avg ( cat ): Wait a minute?! What's that \"cat\" thing in there?!?! (Your ingredients) That is a scary thing called a \"parameter\". It is basically the ingredients to your recipe. If you want to bake a cake you put ingredients into it and out comes a delicious cake. For parameters you \"pass\" these ingredients and then tell the function what to do with them and hopefully [crossesfingers] we get cake. Create an empty list. (Get out a mixing bowl) The genre_avg list will hold all the results of our function. def genre_avg ( cat ): genre_avg = [] For Loop or not to For Loop (Measure the ingredients) We have the ingredients and the mixing bowl... so now we need to measure the ingredients. The for loop in this function goes through each statement in the dictionary above and measures it against the \"parameter\" we passed it. In this case we're interested only in movies that fall in the Action genre. So we measure out all the action movies from our dataset and then what? Can you see what happens next? def genre_avg ( cat ): genre_avg = [] for x in movies : if cat == x [ \"category\" ]: Append (Mix the ingredients) This takes all the ingredients (parameters) that you just measured and mixes them together. And where do you mix things? In the mixing bowl of course! So now all our ingredients have been measured and are back in our mixing bowl all neat and unbaked... hmm... I wonder what the next step is. def genre_avg ( cat ): genre_avg = [] for x in movies : if cat == x [ \"category\" ]: genre_avg . append ( x [ 'imdb' ]) Return (Bake it!!!!) Now we bake all those nice ingredients to get the most perfect cake we ever could ask for. In this case, a perfect average of score of a particular genre in our movie dataset. def genre_avg ( cat ): genre_avg = [] for x in movies : if cat == x [ \"category\" ]: genre_avg . append ( x [ 'imdb' ]) return sum ( genre_avg ) / len ( genre_avg ) Now for the magic. This is not just any cake baking exercise. This is a magical cake machine that can give you any kind of cake you want so long as you give it the ingredients with the right \"parameter\". That's what is going on in the last statement after the function is defined. We reference that function and give it some ingredients and instantly we have some magic cake! How about that. genre_avg ( \"Action\" ) Here's the whole code. def genre_avg ( cat ): genre_avg = [] for x in movies : if cat == x [ \"category\" ]: genre_avg . append ( x [ 'imdb' ]) return sum ( genre_avg ) / len ( genre_avg ) genre_avg ( \"Action\" ) 6.3","tags":"how_to","loc":"http://www.instantinate.com/how_to/build_a_basic_python_function.html","title":"How To: Build A Basic Python Function"},{"url":"http://www.instantinate.com/python/load_dataset_with_csv_reader.html","text":"Link to the full jupyter notebook. Link to the code only jupyter notebook. Quick reference for using built in python functionality to import a csv dataset. import csv Brings in the csv library that is core to python. with open('../data/sales/data.csv', 'rU') as f: Declare open to be used. Specify the file location. Select a dialect to use. Assign the results to the variable f. data = [ ] Create an empty list to receive the data from the f variable. reader = csv.reader(f) Create a reader variable for the csv reader to be applied to the f variable. for row in reader: Create a for loop to iterate over each row of the f variable(csv) using the reader variable. data.append(row) Append rows to the data list as they are read with the reader variable print data Print your data set as a list of lists with the first list being the header row of the csv file. import csv with open ( '../data/sales_data.csv' , 'rU' ) as f : data = [] reader = csv . reader ( f ) for row in reader : data . append ( row ) print data #f.close() -- each with statement has a close statement built into the block so this is not needed. [['volume_sold', '2015_margin', '2015_q1_sales', '2016_q1_sales'], ['18.4207604861', '93.8022814583', '337166.53', '337804.05'], ['4.77650991918', '21.0824246877', '22351.86', '21736.63'], ['16.6024006077', '93.6124943024', '277764.46', '306942.27'], ['4.29611149826', '16.8247038328', '16805.11', '9307.75'], ['8.15602328201', '35.0114570034', '54411.42', '58939.9'], ['5.00512242518', '31.8774372328', '255939.81', '332979.03'], ['14.60675', '76.5189730216', '319020.69', '302592.88'], ['4.45646649485', '19.3373453608', '45340.33', '55315.23'], ['5.04752965097', '26.142470349', '57849.23', '42398.57'], ['5.38807023767', '22.4270237673', '51031.04', '56241.57'], ['9.34734863474', '41.892132964', '68657.91', '3536.14'], ['10.9303977273', '66.4030492424', '4151.93', '137416.93'], ['6.27020860495', '47.8693242069', '121837.56', '158476.55'], ['12.3959191176', '86.7601504011', '146725.31', '125731.51'], ['4.55771189614', '22.9481762576', '119287.76', '21834.49'], ['4.20012242627', '18.7060545353', '20335.03', '39609.55'], ['10.2528698945', '44.0411766297', '110552.94', '204038.87'], ['12.0767847594', '62.1990040107', '204237.78', '15689.8'], ['3.7250952381', '14.2518095238', '16939.15', '48545.69'], ['3.21072662722', '16.0432686391', '55498.12', '16320.74'], ['6.29097142857', '25.1911714286', '15983.98', '53182.55'], ['7.43482131661', '31.7530658307', '71758.66', '30402.43'], ['4.37622478386', '23.1614514016', '62988.17', '47217.4'], ['12.9889127838', '48.8207407407', '29637.75', '6367.76'], ['11.6974557522', '73.2315044248', '48759.71', '329252.09'], ['5.96517512509', '23.4503335716', '89736.7', '332976.05'], ['3.94522273425', '14.1447926267', '5577.61', '234926.02'], ['7.36958530901', '36.4085284899', '310035.66', '151934.45'], ['7.34350882699', '36.1718619066', '310718.21', '314068.92'], ['12.3500273544', '59.8934779211', '258284.84', '61847.52'], ['8.41791967737', '37.1085548647', '150049.71', '203644.27'], ['10.2608361718', '52.4055916932', '309568.16', '123632.78'], ['7.82435369972', '30.681099171', '66468.32', '1050711.75'], ['10.3314300532', '48.1333683392', '321983.24', '149791.31'], ['12.5284878049', '47.7406803594', '115531.13', '61560.7'], ['18.7447505256', '97.2243398739', '926706.76', '260966.41'], ['6.65773264189', '31.2923926822', '157981.2', '160278.07'], ['10.6321289355', '35.27017991', '51078.36', '78108.56'], ['6.92770422965', '31.9091555963', '272703.74', '253886.88'], ['6.61817422161', '29.1482532051', '180760.93', '173395.13'], ['7.12444444444', '32.6235916667', '72185.66', '91524.76'], ['9.84966032435', '47.9893704508', '263161.4', '247802.36'], ['11.5058377559', '55.5221865049', '164809.62', '115591.86'], ['6.30981315215', '31.941637952', '60986.22', '233350.35'], ['10.1866219839', '49.3420628537', '210690.45', '84940.69'], ['10.1221793301', '42.8693852124', '139068.33', '115993.15'], ['10.8003469032', '53.1849073341', '209403.19', '125207.35'], ['7.26782845188', '25.4050062762', '75110.03', '161300.14'], ['10.6737166742', '43.9390962494', '123291.57', '164939.85'], ['9.15026865672', '44.5348318408', '157591.18', '147632.84'], ['8.12418187744', '39.530065189', '163684.93', '168603.49'], ['6.27579970306', '31.5106033203', '146850.63', '342772.02'], ['10.6772953319', '50.1331972789', '143950.01', '102829.3'], ['5.88898828541', '28.7115801384', '136167.32', '216415.08'], ['10.6401714545', '52.4235630748', '327172.32', '174492.82'], ['4.75559643255', '24.0028010033', '105869.94', '41213.34'], ['10.246884068', '47.3184344342', '176266.68', '91213.98'], ['10.29268081', '49.1944300868', '176720.07', '256707.1'], ['4.41819548872', '19.9170827068', '44836.7', '75866.02'], ['7.10134734573', '32.71362436', '65830.25', '50434.54'], ['8.00611901938', '36.2773863187', '228680.53', '141712.31'], ['7.79050337838', '35.2223614865', '73810.68', '91203.94'], ['11.1293822598', '40.0093030623', '54655.98', '156730.02'], ['9.34847653987', '45.5890982815', '156056.28', '24206.67'], ['6.31088643791', '28.2592197712', '80923.05', '218335.21'], ['11.6256060606', '55.1925378788', '167606.75', '325127.17'], ['6.65440717629', '29.106349454', '25575.88', '94229.41'], ['7.93041476808', '39.1116473887', '211902.95', '117784.92'], ['9.003562316', '46.7565544652', '312502.67', '166847.54'], ['14.4394353772', '57.2886237318', '85217.68', '73117.79'], ['11.0115404852', '46.2992604502', '89525.66', '100545.74'], ['5.72389564186', '25.5950192707', '176252.03', '68966.63'], ['7.77732012195', '36.7157591463', '68258.49', '96059.65'], ['4.75918372602', '20.0006463242', '88008.31', '211600.93'], ['7.78586691659', '29.0692439863', '67219.54', '234774.87'], ['5.03499140759', '22.7097016091', '100722.77', '268226.7'], ['11.6845098446', '58.9404393782', '147242.83', '58122.85'], ['5.14772910448', '25.7666753731', '227702.34', '106662.37'], ['10.0860303763', '49.1620319871', '286763.37', '147781.41'], ['7.94465682362', '33.9956304868', '59531.82', '67514.95'], ['5.29439978635', '24.4766550941', '122811.86', '170466.11'], ['11.8265363003', '52.3184580805', '156513.07', '94158.15'], ['6.300925868', '30.5095964967', '55538.11', '139676.38'], ['8.64269487751', '36.2843302577', '143498.37', '167692.5'], ['6.04822838631', '30.1712350678', '84998.58', '180371.58'], ['9.47492913676', '41.0171194552', '138010.02', '298889.32'], ['3.99185767098', '20.0513661569', '146424.29', '133868.58'], ['8.59207381371', '45.5986531395', '175475.49', '39138.81'], ['7.21148957299', '35.8870443261', '247716.78', '343447.68'], ['8.19108557079', '37.9469113386', '130512.99', '191091.14'], ['7.69531160115', '29.3906663471', '52054.1', '184729.87'], ['13.6351500118', '64.7439062131', '332692.67', '37179.58'], ['6.96681681682', '35.0125088725', '135418.28', '40160.26'], ['11.2323237697', '52.2156089683', '163104.8', '40970.96'], ['5.09546375267', '22.4304264392', '45612.45', '135871.39'], ['11.9368836649', '49.1816335142', '56995.82', '92843.04'], ['5.90627376426', '25.0323193916', '39867.38', '295483.47'], ['9.12709469154', '45.8722118604', '133081.37', '130377.49'], ['7.7544572697', '34.7604217536', '108362.38', '192747.83'], ['7.58599675412', '39.9868794518', '271021.94', '103958.63'], ['7.43725207583', '34.6033448222', '134589.33', '106452.13'], ['9.8798900768', '41.6839918687', '216171.01', '12380.85'], ['9.30319388778', '40.8443061122', '104354.49', '94010.77'], ['9.21365050557', '44.7063598652', '105046.24', '51773.27'], ['5.18177205308', '21.1562997658', '13677.13', '242057.36'], ['8.55507774799', '41.547077748', '89053.08', '392226.28'], ['5.78126590331', '24.6607061069', '51427.05', '369424.59'], ['8.0710230118', '40.1006629307', '264408.55', '73141.35'], ['10.1250032713', '50.3450808487', '294990.21', '292690.86'], ['11.0196516733', '51.4318879891', '358098.75', '172189.31'], ['8.17666774927', '32.3875251705', '68077.3', '211032.4'], ['9.42171292239', '53.4813935145', '313345.86', '171119.44'], ['4.85870921867', '24.5552275389', '163324.44', '201433.01'], ['9.31378525791', '43.8089558984', '203099.8', '200599.95'], ['8.30018036767', '43.4441935484', '104044.33', '476439.16'], ['6.50688776965', '33.4178523235', '202835.96', '215106.77'], ['9.5852099677', '44.3454176281', '192934.41', '204808.54'], ['12.45572275', '65.5244131168', '447305.5', '37858.67'], ['8.09288312646', '39.7529703822', '205170.43', '382790.84'], ['8.68651837806', '46.2175087515', '201282.29', '58332.41'], ['8.34731752963', '32.7325389894', '36855.39', '2443253.18'], ['9.29224055739', '49.786953612', '381036.91', '209066.17'], ['9.77711182109', '44.9972396166', '46509.65', '137697.56'], ['29.878030039', '168.245861698', '2337324.42', '129489.89'], ['8.78393692369', '49.313768542', '208389.88', '72810.95'], ['11.9757685161', '56.569739846', '145742.16', '158368.58'], ['11.1401021385', '47.8089690393', '187407.93', '153305.98'], ['7.5605488194', '30.9038417358', '62335.59', '289199.43'], ['7.39098798637', '37.2649829658', '138878.63', '241311.79'], ['6.43360588592', '35.850617415', '169131.81', '343060.63'], ['13.7999774485', '68.1836227448', '280506.28', '128500.31'], ['6.44703955254', '34.2461556133', '223641.14', '79554.12'], ['8.01794477751', '46.0017090264', '361865.13', '191155.87'], ['6.2553554256', '26.9316699155', '134325.62', '111207.97'], ['9.69742181905', '48.3078228694', '145361.36', '83399.61'], ['7.77268351232', '36.216199621', '185580.5', '25290.33'], ['8.75192030725', '37.1569323092', '93901.53', '20962.08'], ['6.79288937945', '33.5548344371', '68438.51', '15307.3'], ['7.68249438202', '33.223011236', '32698.19', '10464.02'], ['4.38545511613', '20.8197112367', '22829.66', '52246.46'], ['3.60671020408', '15.8903673469', '13141.2', '75566.37'], ['8.45364705882', '35.4949411765', '8738.45', '103585.1'], ['5.21488185976', '26.0287842988', '48441.46', '276396.86'], ['8.40056149733', '37.0314331551', '87115.74', '305851.47'], ['6.84136327817', '29.1820981087', '102794.01', '260453.94'], ['12.5099672623', '72.4642572637', '306947.29', '30441.76'], ['9.0148700565', '52.1879774011', '225615.38', '10564.6'], ['7.20036424796', '40.9748793849', '327334.28', '19357.49'], ['5.77809677419', '23.8230645161', '27114.29', '613414.14'], ['4.94129392971', '24.1203833866', '7539.14', '63793.71'], ['6.00045070423', '26.6531361502', '20378.73', '78706.47'], ['11.9971174753', '65.067257829', '501953.12', '22675.76'], ['10.6377691184', '51.4916463712', '66097.53', '4566.74'], ['8.56422809829', '36.8113087607', '89327.88', '14050.14'], ['8.62268641471', '35.087834525', '20534.72', '11883.3'], ['4.83114713217', '17.9033416459', '5742.23', '11597.18'], ['10.2701848998', '38.2349306626', '15987.52', '23801.1'], ['12.5816945607', '41.9222384937', '13518.07', '22824.14'], ['16.0599706745', '50.8491202346', '17574.68', '18957.2'], ['11.8677385892', '36.282033195', '24357.22', '19832.94'], ['10.2945011338', '39.7768253968', '29611.32', '4258.14'], ['4.17606557377', '16.4770435274', '18571.65', '17116.11'], ['9.36189873418', '35.4246202532', '18712.28', '7499.47'], ['11.0917085427', '48.8443718593', '8458.2', '11367.4'], ['5.3244966443', '20.2525251678', '19089.74', '8041.09'], ['6.63090439276', '24.2994315245', '7305.46', '11966.94'], ['8.58392405063', '34.2229535865', '14796.7', '12707.72'], ['5.53106109325', '25.2890353698', '8200.55', '22130.31'], ['6.13912310287', '26.0607419899', '11412.54', '20817.32'], ['8.47737603306', '33.6023140496', '11228.97', '16786.98'], ['8.44393241167', '37.4071121352', '17232.45', '19203.82'], ['5.15196394076', '20.6980424984', '21340.78', '25302.92'], ['6.53706864564', '22.8882189239', '16617.85', '18650.96'], ['8.50044523598', '27.5077292965', '21711.71', '14626.29'], ['3.93154326923', '21.8116586538', '28128.95', '46323.73'], ['6.16368913858', '25.4285205993', '21803.2', '535381.86'], ['4.90444711538', '19.5683173077', '12517.29', '812011.78'], ['7.40241271087', '26.9353354257', '53988.92', '206247.57'], ['47.5032692308', '235.730677515', '555707.4', '6402.78'], ['55.7391800938', '268.869600245', '1082136.01', '100765.67'], ['11.8407803201', '56.4333884407', '192089.46', '20098.61'], ['7.00229357798', '26.4409174312', '5574.99', '68230.36'], ['8.75314206706', '45.2938472174', '80241.27', '596063.0'], ['3.14774130328', '15.5945516903', '27043.54', '87471.43'], ['7.1967787944', '27.2886517761', '61977.54', '431990.7'], ['76.2036918138', '367.225653291', '977772.62', '136717.57'], ['10.8043371086', '42.0311992005', '41905.18', '30008.81'], ['10.705327051', '45.8914772727', '87839.45', '19420.34'], ['51.8006862745', '255.153235294', '445058.32', '274752.6'], ['5.88277871731', '27.5122713672', '127495.18', '10315.35'], ['6.68640645161', '26.1030967742', '23874.67', '45252.42'], ['5.83335488041', '23.7416031028', '21535.87', '433832.28'], ['45.5560956385', '218.008349974', '276096.18', '74215.43'], ['5.17260575296', '21.5487817259', '8506.79', '78137.29'], ['10.118018018', '46.7007087087', '49163.01', '83915.72'], ['51.6755374204', '233.533188694', '434110.57', '372240.72'], ['2.79463149728', '11.9961176992', '73789.38', '148021.9'], ['7.61169779853', '38.8099733155', '88006.84', '31184.18'], ['15.6976512739', '105.035207006', '117958.96', '33324.36'], ['50.2758932156', '225.055138499', '407738.79', '32079.13']] NOTE: At first I had noted that an f.close() statement was needed at the end of this function. But it was rightly pointed out to me that this is a built in functionality for the with statement code block and is therefore not needed.","tags":"python","loc":"http://www.instantinate.com/python/load_dataset_with_csv_reader.html","title":"Load Dataset with CSV.reader"}]}